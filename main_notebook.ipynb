{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utility_functions\n",
    "import importlib\n",
    "importlib.reload(utility_functions)\n",
    "import clustering\n",
    "importlib.reload(clustering)\n",
    "import training\n",
    "importlib.reload(training)\n",
    "\n",
    "import shutil\n",
    "import json\n",
    "# import mxnet as mx\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from os.path import join\n",
    "from numpy.random import seed\n",
    "from numpy import unique, where, savez_compressed\n",
    "import random\n",
    "import torch\n",
    "import re\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils.extmath import softmax\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "# very important\n",
    "# ! git clone https://github.com/yinguobing/arcface.git\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn import metrics\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 20000\n",
    "n_clusters = 10\n",
    "\n",
    "methods = [\"full_random\", \"half_random\", \"kmeans\", \"sample_kmeans\"]\n",
    "if n_clusters == 1:\n",
    "    method = methods[2]\n",
    "else:\n",
    "    method = methods[2]\n",
    "# scenario_number = 'normal_' + str(n_classes) + '_' + method + str(n_clusters)\n",
    "scenario_number = 'new_fake_' + str(n_classes) + '_' + method + str(n_clusters)\n",
    "dataset_name = 'glint360k_224'\n",
    "# scenario_number = str(n_classes) + '_half_random'\n",
    "\n",
    "os.makedirs(join('..', dataset_name, 'data', scenario_number), exist_ok=True)\n",
    "os.makedirs(join('..', dataset_name, 'models', scenario_number), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_time():\n",
    "    return datetime.now().strftime('%y/%m/%d-%H:%M:%S')\n",
    "\n",
    "def pprint(tpl, dataset_name='glint360k_224', endline=False, doubleendline=False):\n",
    "    with open(join('..', dataset_name, 'data', str(scenario_number), 'results.txt'), 'a') as f:\n",
    "        # r = \"\"\n",
    "        # for t in tpl:\n",
    "        #     r += \" \" + str(t)\n",
    "        print(current_time(), tpl)\n",
    "        print(current_time(), tpl, file=f)\n",
    "        if endline:\n",
    "            print('-'*15)\n",
    "            print('-'*15, file=f)\n",
    "        if doubleendline:\n",
    "            print('-'*15)\n",
    "            print('-'*15)\n",
    "            print('*'*15, file=f)\n",
    "            print('*'*15, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# منظور از لیبل، شماره عددی است و منظور از\n",
    "# y\n",
    "# برچسب غیرعددی است\n",
    "\n",
    "train_embeddings_path = join('..', dataset_name, 'embeddings', 'common', 'all', 'train')\n",
    "# test_embeddings_path = join('..', dataset_name, 'embeddings', 'common', 'all', 'val')\n",
    "test_embeddings_path = join('F:\\\\test\\deepface\\\\glint360k_224\\\\img_data\\\\embeddings', 'test')\n",
    "\n",
    "trainx = []\n",
    "trainl = []\n",
    "trainy = []\n",
    "traincenterx = []\n",
    "traincentery = []\n",
    "traincenterl = []\n",
    "testx = []\n",
    "testl = []\n",
    "testy = []\n",
    "\n",
    "# if os.path.isfile(join('..', dataset_name, 'data', str(n_classes), 'trainx.npz')):\n",
    "if os.path.isfile(join('..', dataset_name, 'data', str(n_classes), 'new_testx.npz')):\n",
    "    trainx = np.load(join('..', dataset_name, 'data', str(n_classes), 'trainx.npz'))['res']\n",
    "    trainy = np.load(join('..', dataset_name, 'data', str(n_classes), 'trainy.npz'))['res']\n",
    "    trainl = np.load(join('..', dataset_name, 'data', str(n_classes), 'trainl.npz'))['res']\n",
    "\n",
    "    traincenterx = np.load(join('..', dataset_name, 'data', str(n_classes), 'traincenterx.npz'))['res']\n",
    "    traincentery = np.load(join('..', dataset_name, 'data', str(n_classes), 'traincentery.npz'))['res']\n",
    "    traincenterl = np.load(join('..', dataset_name, 'data', str(n_classes), 'traincenterl.npz'))['res']\n",
    "\n",
    "    # testx = np.load(join('..', dataset_name, 'data', str(n_classes), 'testx.npz'))['res']\n",
    "    # testy = np.load(join('..', dataset_name, 'data', str(n_classes), 'testy.npz'))['res']\n",
    "    # testl = np.load(join('..', dataset_name, 'data', str(n_classes), 'testl.npz'))['res']\n",
    "\n",
    "    testx = np.load(join('..', dataset_name, 'data', str(n_classes), 'new_testx.npz'))['res']\n",
    "    testy = np.load(join('..', dataset_name, 'data', str(n_classes), 'new_testy.npz'))['res']\n",
    "    testl = np.load(join('..', dataset_name, 'data', str(n_classes), 'new_testl.npz'))['res']\n",
    "\n",
    "else:\n",
    "    all_id_files = dict()\n",
    "    with open(join('..', dataset_name, 'data', 'all_id_files.json')) as jsonfile:\n",
    "        all_id_files = json.load(jsonfile)\n",
    "\n",
    "    keys = list(all_id_files.keys())[:n_classes]\n",
    "    \n",
    "    os.makedirs(join('..', dataset_name, 'data', str(n_classes)), exist_ok=True)\n",
    "\n",
    "    idx = 0\n",
    "    for class_name in tqdm(keys):\n",
    "        tr_x = np.load(join(train_embeddings_path, class_name + '.npz'), allow_pickle=True)\n",
    "        tr_features = tr_x[tr_x.files[0]]    \n",
    "        tr_f = len(tr_features)\n",
    "\n",
    "        trainx.extend(tr_features)\n",
    "        trainl.extend([idx] * tr_f)\n",
    "        trainy.extend([class_name] * tr_f)\n",
    "\n",
    "        traincenterx.append(np.mean(tr_features, axis=0))\n",
    "        traincenterl.append(idx)\n",
    "        traincentery.append(class_name)\n",
    "\n",
    "        te_x = np.load(join(test_embeddings_path, class_name + '.npz'), allow_pickle=True)\n",
    "        te_features = te_x[te_x.files[0]]\n",
    "        te_f = len(te_features)\n",
    "\n",
    "        testx.extend(te_features)\n",
    "        testl.extend([idx] * te_f)\n",
    "        testy.extend([class_name] * te_f)\n",
    "\n",
    "        idx+=1\n",
    "\n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    trainl = np.array(trainl)\n",
    "    traincenterx = np.array(traincenterx)\n",
    "    traincentery = np.array(traincentery)\n",
    "    traincenterl = np.array(traincenterl)\n",
    "    testx = np.array(testx)\n",
    "    testy = np.array(testy)\n",
    "    testl = np.array(testl)\n",
    "\n",
    "    savez_compressed(join('..', dataset_name, 'data', str(n_classes), 'trainx.npz'), res=trainx)\n",
    "    savez_compressed(join('..', dataset_name, 'data', str(n_classes), 'trainy.npz'), res=trainy)\n",
    "    savez_compressed(join('..', dataset_name, 'data', str(n_classes), 'trainl.npz'), res=trainl)\n",
    "\n",
    "    savez_compressed(join('..', dataset_name, 'data', str(n_classes), 'traincenterx.npz'), res=traincenterx)\n",
    "    savez_compressed(join('..', dataset_name, 'data', str(n_classes), 'traincentery.npz'), res=traincentery)\n",
    "    savez_compressed(join('..', dataset_name, 'data', str(n_classes), 'traincenterl.npz'), res=traincenterl)\n",
    "\n",
    "    # savez_compressed(join('..', dataset_name, 'data', str(n_classes), 'testx.npz'), res=testx)\n",
    "    # savez_compressed(join('..', dataset_name, 'data', str(n_classes), 'testy.npz'), res=testy)\n",
    "    # savez_compressed(join('..', dataset_name, 'data', str(n_classes), 'testl.npz'), res=testl)\n",
    "\n",
    "    savez_compressed(join('..', dataset_name, 'data', str(n_classes), 'new_testx.npz'), res=testx)\n",
    "    savez_compressed(join('..', dataset_name, 'data', str(n_classes), 'new_testy.npz'), res=testy)\n",
    "    savez_compressed(join('..', dataset_name, 'data', str(n_classes), 'new_testl.npz'), res=testl)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, key in tqdm(enumerate(keys)):\n",
    "#     source_name = os.path.join('F:\\\\test\\deepface\\\\glint360k_224\\\\img_data\\\\embeddings', str(10 * (idx // 10) + 10) + '.npz')\n",
    "#     f_name = os.path.join('F:\\\\test\\deepface\\\\glint360k_224\\\\img_data\\\\embeddings', 'test', key + '.npz')\n",
    "#     ten_features = np.load(source_name)['res']\n",
    "#     np.savez_compressed(f_name, res=ten_features[5 * (idx % 10): 5 * ((idx % 10) + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_path = join('..', dataset_name, 'data', str(scenario_number))\n",
    "\n",
    "if False: #os.path.isfile(join(pre_path, 'weights.pt')):\n",
    "    weights = np.array(torch.load(join(pre_path, 'weights.pt')))\n",
    "else:\n",
    "    weights = []\n",
    "    for i in range(n_classes):\n",
    "        w = (len(np.where(testl == i)[0]))\n",
    "        for s in range(w):\n",
    "            weights.append(1/w)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    torch.save(torch.Tensor(weights), join(pre_path, 'weights.pt'))\n",
    "    weights = np.array(torch.load(join(pre_path, 'weights.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97661006"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# knn with cosine similarity\n",
    "\n",
    "cs = utility_functions.cos_sim(torch.Tensor(testx), torch.Tensor(traincenterx))\n",
    "max_cs_values = cs.max(1)[0]\n",
    "max_cs_args = cs.max(1)[1]\n",
    "r = (max_cs_args.numpy() == testl) * weights\n",
    "np.sum(r) / np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95212996"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# knn with euclidean similarity\n",
    "\n",
    "es = utility_functions.euc_sim(torch.Tensor(testx), torch.Tensor(traincenterx))\n",
    "max_es_values = es.max(1)[0]\n",
    "max_es_args = es.max(1)[1]\n",
    "r = (max_es_args.numpy() == testl) * weights\n",
    "np.sum(r) / np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_sample_size = 30\n",
    "# new_trainx = []\n",
    "# new_testx = []\n",
    "# new_trainl = []\n",
    "# new_trainy = []\n",
    "# new_testy = []\n",
    "# new_testl = []\n",
    "\n",
    "# for idx in tqdm(range(n_classes)):\n",
    "#     test_idxes = np.where(testl == idx)[0]\n",
    "#     train_idxes = np.where(trainl == idx)[0]\n",
    "#     test_features = testx[test_idxes]\n",
    "#     train_features = trainx[train_idxes]\n",
    "#     y = trainy[train_idxes[0]]\n",
    "#     l = trainl[train_idxes[0]]\n",
    "    \n",
    "#     # if len(train_features) >= (max_sample_size * 2) // 3:\n",
    "#     #     train_features = train_features[:max_sample_size // 3 * 2]\n",
    "#     # if len(test_features) >= (max_sample_size // 3):\n",
    "#     #     test_features = test_features[:max_sample_size // 3]\n",
    "\n",
    "#     features = np.array(list(train_features) + list(test_features))\n",
    "\n",
    "#     if len(features) >= max_sample_size:\n",
    "#         new_trainx.extend(list(train_features))\n",
    "#         # new_testx.extend(list(test_features))\n",
    "#         new_trainy.extend([y] * len(list(train_features)))\n",
    "#         # new_testy.extend([y] * len(list(test_features)))\n",
    "#         new_trainl.extend([l] * len(list(train_features)))\n",
    "#         # new_testl.extend([l] * len(list(test_features)))\n",
    "#         continue\n",
    "\n",
    "#     d = utility_functions.cos_sim(torch.Tensor(features), torch.Tensor(features))\n",
    "#     for i in range(len(d)):\n",
    "#         d[i,i] = 0\n",
    "#     m = np.sum(d.numpy(), 0) / (len(d) - 1)\n",
    "#     candidates = np.argsort(m)[::-1][:np.min([len(d), 8])]\n",
    "\n",
    "#     passed = []\n",
    "#     new_features = []\n",
    "#     ll = list(itertools.product(range(len(candidates)), range(len(candidates))))\n",
    "#     np.random.shuffle(ll)\n",
    "\n",
    "#     for i, j in ll:\n",
    "#         if i == j:\n",
    "#             continue\n",
    "#         if (j, i) in passed:\n",
    "#             continue\n",
    "#         passed.append((i, j))\n",
    "\n",
    "#         new_feature = np.mean([features[i], features[j]], 0)\n",
    "#         new_features.append(list(new_feature))\n",
    "#         if len(new_features) + len(features) >= max_sample_size:\n",
    "#             terminate = True\n",
    "\n",
    "#     if len(features) > 7:\n",
    "#         x = max_sample_size // 3 * 2 - len(train_features)\n",
    "#         new_trainx.extend(list(train_features) + list(new_features[:x]))\n",
    "#         # new_testx.extend(list(test_features) + list(new_features[x : x + max_sample_size // 3 - len(test_features)]))\n",
    "#         new_trainy.extend([y] * (max_sample_size // 3 * 2))\n",
    "#         # new_testy.extend([y] * (max_sample_size // 3))\n",
    "#         new_trainl.extend([l] * (max_sample_size // 3 * 2))\n",
    "#         # new_testl.extend([l] * (max_sample_size // 3))\n",
    "#     else:\n",
    "#         x = int(len(train_features) / len(features) * len(new_features))\n",
    "#         new_trainx.extend(list(train_features) + list(new_features[:x]))\n",
    "#         # new_testx.extend(list(test_features) + list(new_features[x:]))\n",
    "#         new_trainy.extend([y] * len(list(train_features) + list(new_features[:x])))\n",
    "#         # new_testy.extend([y] * len(list(test_features) + list(new_features[x:])))\n",
    "#         new_trainl.extend([l] * len(list(train_features) + list(new_features[:x])))\n",
    "#         # new_testl.extend([l] * len(list(test_features) + list(new_features[x:])))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "  \n",
    "# pca = PCA(n_components = 2)\n",
    "  \n",
    "# X_train = pca.fit_transform(trainx[:6])\n",
    "# X_test = pca.transform(trainx[6:])\n",
    "  \n",
    "# explained_variance = pca.explained_variance_ratio_\n",
    "# explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression  \n",
    "  \n",
    "# classifier = LogisticRegression(random_state = 0)\n",
    "# classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o_test_features = testx[np.where(testl == 2)[0]]\n",
    "# o_train_features = trainx[np.where(trainl == 2)[0]]\n",
    "# o_features = np.array(list(o_train_features) + list(o_test_features))\n",
    "# o_d = utility_functions.cos_sim(torch.Tensor(o_features), torch.Tensor(o_features))\n",
    "# np.mean(o_d.numpy(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_o_d = utility_functions.cos_sim(torch.Tensor(z_features), torch.Tensor(o_features))\n",
    "# np.mean(z_o_d.numpy(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_testx = []\n",
    "# temp_testy = []\n",
    "# temp_testl = []\n",
    "\n",
    "# for i in range(n_classes):\n",
    "#     idx = np.where(testl == i)[0][0]\n",
    "#     temp_testx += [testx[idx]]\n",
    "#     temp_testy += [testy[idx]]\n",
    "#     temp_testl += [testl[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utility_function\n",
    "\n",
    "# for i in tqdm(range(n_clusters)):\n",
    "#     x = np.array([])\n",
    "#     A = traincenterx[unique_result[i]]\n",
    "#     B = traincenterx[unique_result[i]]\n",
    "\n",
    "#     c = cdist(A, B, 'cosine')\n",
    "    \n",
    "#     for j in range(len(A)):\n",
    "#         c[j, j] = np.Inf\n",
    "\n",
    "#     # c = (c - minn) / (maxx - minn)\n",
    "#     x = np.where(c < 0.6)\n",
    "    \n",
    "#     xx = unique_result[i].copy()\n",
    "#     new_unique_result[i] = np.delete(xx, list(set(x[0])))\n",
    "#     extra.extend(list(set(xx[x[0]])))\n",
    "\n",
    "# i = 5\n",
    "# new_unique_result[5] = np.array(extra)\n",
    "# n_clusters = 6\n",
    "# for r in tqdm(range(n_clusters)):\n",
    "#     train_ids = utility_functions.train_calc_ids(new_unique_result[r], trainy, trainl, r)\n",
    "#     test_ids = utility_functions.test_calc_ids(new_unique_result[r], trainy, trainl, r)\n",
    "#     train_sample_count, test_sample_count = utility_functions.convert_emb_to_tfrecord(dataset_name, scenario_number, train_ids, test_ids, r, overwrite=True, all_samples=True, n_classes=len(new_unique_result[r]))\n",
    "#     train_dataset, test_dataset = utility_functions.prepare_data_sets(dataset_name, scenario_number, train_ids, test_ids, r)    \n",
    "#     training.softmax_train(dataset_name, scenario_number, train_dataset, test_dataset, new_unique_result[r], trainx, trainy, trainl, r, epochs=30,train_overwrite=False, freq = train_sample_count // 100)\n",
    "#     val_acc = utility_functions.test_acc_calc(r, test_dataset, dataset_name, scenario_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainx = np.array(new_trainx)\n",
    "# trainy = np.array(new_trainy)\n",
    "# # testx = np.array(new_testx)\n",
    "# # testy = np.array(new_testy)\n",
    "# trainl = np.array(new_trainl)\n",
    "# # testl = np.array(new_testl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/08-06:04:48 ('meth = ', 'kmeans', 'test_number = ', '0')\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(utility_functions)\n",
    "importlib.reload(clustering)\n",
    "importlib.reload(training)\n",
    "m = 'euclidean'\n",
    "\n",
    "for test_number in range(0, 1):\n",
    "# for meth in [\"full_random\"]:\n",
    "    # scenario_number = str(n_classes) + '_' + meth\n",
    "\n",
    "    os.makedirs(join('..', dataset_name, 'models', scenario_number), exist_ok=True)\n",
    "    os.makedirs(join('..', dataset_name, 'models', scenario_number, str(test_number)), exist_ok=True)\n",
    "\n",
    "    pprint((\"meth = \", method, \"test_number = \", str(test_number)))\n",
    "    dir_path = join('..', dataset_name, 'data', str(scenario_number), 'tfrecords')\n",
    "    # dir_path2 = join('..', dataset_name, 'models', str(scenario_number))\n",
    "    # try:\n",
    "    #     shutil.rmtree(dir_path)\n",
    "    #     # shutil.rmtree(dir_path2)\n",
    "    # except OSError as e:\n",
    "    #     print(\"Error: %s : %s\" % (dir_path, e.strerror))\n",
    "\n",
    "    if method == \"kmeans\" or method == 'sample_kmeans':\n",
    "        result_path = join('..', dataset_name, 'data', scenario_number,  'result_' + str(test_number) + '.npz')    \n",
    "        model_filename = join('..', dataset_name, 'models', scenario_number, 'kmeans_' + str(test_number) + '.sav')\n",
    "    elif method == 'full_random' or method == 'half_random':\n",
    "        result_path = join('..', dataset_name, 'data', scenario_number, 'result_' + str(test_number) + '.npz')\n",
    "    result = dict()\n",
    "    unique_result = dict()\n",
    "\n",
    "    if method == 'full_random':\n",
    "        if os.path.isfile(result_path): # and os.path.isfile(filename):\n",
    "            result = (np.load(result_path, allow_pickle=True)['res']).item()\n",
    "        else:\n",
    "            # # random \n",
    "            lbls = np.zeros((n_classes), dtype='int')\n",
    "            for i in range(n_classes):\n",
    "                lbls[i] = random.randint(0, n_clusters-1)\n",
    "            for i in range(n_clusters):\n",
    "                result[i] = torch.Tensor((lbls == i).nonzero()).squeeze(0).unsqueeze(1).cuda().int().cpu().numpy().squeeze(1)\n",
    "            savez_compressed(result_path, res = result)\n",
    "        for r in result:\n",
    "            unique_result[r] = result[r]\n",
    "    elif method == 'half_random':\n",
    "        result = (np.load(result_path, allow_pickle=True)['res']).item()\n",
    "        for r in result:\n",
    "            unique_result[r] = result[r]\n",
    "    elif method == 'kmeans':\n",
    "        if False: #os.path.isfile(result_path) and os.path.isfile(model_filename):\n",
    "            kmeans_model = pickle.load(open(model_filename, 'rb'))\n",
    "            result = (np.load(result_path, allow_pickle=True)['res']).item()\n",
    "        else:\n",
    "            centers, labels = clustering.init_centers(trainx, n_clusters)\n",
    "            kmeans_model = clustering.Fast_KMeans(n_clusters=n_clusters, max_iter=100, tol=0.0001, verbose=0, centroids=centers, mode=m, minibatch=None)\n",
    "            lbls = kmeans_model.fit_predict(torch.Tensor(traincenterx).cuda())\n",
    "            pickle.dump(kmeans_model, open(model_filename, 'wb'))\n",
    "            for i in range(n_clusters):\n",
    "                result[i] = (lbls == i).nonzero().cpu().numpy().squeeze(1)\n",
    "            savez_compressed(result_path, res = result)\n",
    "        for r in result:\n",
    "            unique_result[r] = result[r]\n",
    "    elif method == 'sample_kmeans':\n",
    "        kmeans_model = None\n",
    "\n",
    "        if os.path.isfile(model_filename) and os.path.isfile(result_path):\n",
    "            kmeans_model = pickle.load(open(model_filename, 'rb'))\n",
    "            result = (np.load(result_path, allow_pickle=True)['res']).item()\n",
    "        else:\n",
    "            centers, labels = clustering.init_centers(trainx, n_clusters)\n",
    "            kmeans_model = clustering.Fast_KMeans(n_clusters=n_clusters, max_iter=100, tol=0.0001, verbose=0, centroids=centers, mode=m, minibatch=None)\n",
    "            lbls = model.fit_predict(torch.Tensor(trainx).cuda())\n",
    "            pickle.dump(kmeans_model, open(model_filename, 'wb'))\n",
    "            for i in range(n_clusters):\n",
    "                result[i] = (lbls == i).nonzero().cpu().numpy().squeeze(1)\n",
    "            savez_compressed(result_path, res = result)\n",
    "\n",
    "        for r in result:\n",
    "            unique_result[r] = np.unique(trainl[result[r]])\n",
    "\n",
    "    if method == 'half_random':\n",
    "        alpha = 0.4\n",
    "        alpha_sw = True\n",
    "        while(alpha_sw):\n",
    "            temp = dict()\n",
    "            temp_unique_result = unique_result.copy()\n",
    "            for i in range(n_clusters):\n",
    "                while(True):\n",
    "                    A = torch.Tensor(traincenterx[temp_unique_result[i]])\n",
    "                    c = utility_functions.cos_sim(A, A).numpy()\n",
    "                    x = np.where((c > alpha) & (c < 0.99))\n",
    "\n",
    "                    if len(x[0]) == 0:\n",
    "                        break\n",
    "                    # کلاسهایی که باید منتقل شوند\n",
    "                    result = []\n",
    "                    sw = False\n",
    "                    for idx, l in enumerate(x[0]):\n",
    "                        if l in result:\n",
    "                            continue\n",
    "                        sw = False\n",
    "                        for k, res in enumerate(result):\n",
    "                            idxes = np.where(x[0] == res)[0]\n",
    "                            for id in idxes:\n",
    "                                if l == x[1][id]:\n",
    "                                    sw = True\n",
    "                        if sw:\n",
    "                            continue\n",
    "                        result.append(l)\n",
    "                \n",
    "                    t = result\n",
    "                    if i in temp:\n",
    "                        temp[i] = np.append(temp[i], temp_unique_result[i][t])\n",
    "                    else:\n",
    "                        temp[i] = temp_unique_result[i][t]\n",
    "                    temp_unique_result[i] = np.delete(temp_unique_result[i], t)\n",
    "\n",
    "            lst = np.argsort([len(temp_unique_result[i]) for i in range(n_clusters)])\n",
    "            for t in temp:\n",
    "                for l in lst:\n",
    "                    A = torch.Tensor(traincenterx[temp[t]])\n",
    "                    B = torch.Tensor(traincenterx[temp_unique_result[l]])\n",
    "\n",
    "                    c = utility_functions.cos_sim(A, B)\n",
    "                    d = np.where((c > alpha) & (c < 0.99))\n",
    "\n",
    "                    e = np.delete(temp[t], d[0])\n",
    "                    temp_unique_result[l] = np.append(temp_unique_result[l], list(set(e))).astype('int')\n",
    "                    temp[t] = temp[t][list(set(d[0]))]\n",
    "\n",
    "                    if len(temp[t]) == 0:\n",
    "                        break\n",
    "                lst = np.argsort([len(temp_unique_result[i]) for i in range(n_clusters)])\n",
    "\n",
    "\n",
    "            # if sum([len(temp[i]) for i in temp]) > 0:\n",
    "            #     alpha += 0.01\n",
    "            # else:\n",
    "            alpha_sw = False\n",
    "\n",
    "        unique_result = temp_unique_result\n",
    "        pprint(('alpha', alpha))\n",
    "        # # temp\n",
    "        # for t in temp:\n",
    "        #     if len(temp[t]) > 0:\n",
    "        #         for item in temp[t]:\n",
    "        #             r = np.random.randint(n_clusters)\n",
    "        #             unique_result[r] = np.append(unique_result[r], [item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/0\\exported\\hrnetv2\\1\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/0\\exported\\hrnetv2\\2\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/0\\exported\\hrnetv2\\3\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/0\\exported\\hrnetv2\\4\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/0\\exported\\hrnetv2\\5\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/0\\exported\\hrnetv2\\6\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/0\\exported\\hrnetv2\\7\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/0\\exported\\hrnetv2\\8\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/0\\exported\\hrnetv2\\9\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/0\\exported\\hrnetv2\\10\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10\\0\\0\\exported\\hrnetv2\\assets\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).dataset\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.step\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.epoch\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.monitor_value\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'vhat' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/1\\exported\\hrnetv2\\1\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/1\\exported\\hrnetv2\\2\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/1\\exported\\hrnetv2\\3\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/1\\exported\\hrnetv2\\4\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/1\\exported\\hrnetv2\\5\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/1\\exported\\hrnetv2\\6\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/1\\exported\\hrnetv2\\7\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/1\\exported\\hrnetv2\\8\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/1\\exported\\hrnetv2\\9\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/1\\exported\\hrnetv2\\10\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10\\0\\1\\exported\\hrnetv2\\assets\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).dataset\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.step\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.epoch\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.monitor_value\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'vhat' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/2\\exported\\hrnetv2\\1\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/2\\exported\\hrnetv2\\2\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/2\\exported\\hrnetv2\\3\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/2\\exported\\hrnetv2\\4\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/2\\exported\\hrnetv2\\5\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/2\\exported\\hrnetv2\\6\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/2\\exported\\hrnetv2\\7\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/2\\exported\\hrnetv2\\8\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/2\\exported\\hrnetv2\\9\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/2\\exported\\hrnetv2\\10\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10\\0\\2\\exported\\hrnetv2\\assets\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).dataset\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.step\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.epoch\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.monitor_value\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'vhat' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/3\\exported\\hrnetv2\\1\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/3\\exported\\hrnetv2\\2\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/3\\exported\\hrnetv2\\3\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/3\\exported\\hrnetv2\\4\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/3\\exported\\hrnetv2\\5\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/3\\exported\\hrnetv2\\6\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/3\\exported\\hrnetv2\\7\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/3\\exported\\hrnetv2\\8\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/3\\exported\\hrnetv2\\9\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/3\\exported\\hrnetv2\\10\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10\\0\\3\\exported\\hrnetv2\\assets\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).dataset\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.step\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.epoch\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.monitor_value\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'vhat' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/4\\exported\\hrnetv2\\1\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/4\\exported\\hrnetv2\\2\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/4\\exported\\hrnetv2\\3\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/4\\exported\\hrnetv2\\4\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/4\\exported\\hrnetv2\\5\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/4\\exported\\hrnetv2\\6\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/4\\exported\\hrnetv2\\7\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/4\\exported\\hrnetv2\\8\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/4\\exported\\hrnetv2\\9\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/4\\exported\\hrnetv2\\10\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10\\0\\4\\exported\\hrnetv2\\assets\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).dataset\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.step\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.epoch\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.monitor_value\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'vhat' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/5\\exported\\hrnetv2\\1\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/5\\exported\\hrnetv2\\2\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/5\\exported\\hrnetv2\\3\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/5\\exported\\hrnetv2\\4\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/5\\exported\\hrnetv2\\5\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/5\\exported\\hrnetv2\\6\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/5\\exported\\hrnetv2\\7\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/5\\exported\\hrnetv2\\8\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/5\\exported\\hrnetv2\\9\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/5\\exported\\hrnetv2\\10\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10\\0\\5\\exported\\hrnetv2\\assets\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).dataset\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.step\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.epoch\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.monitor_value\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'vhat' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/6\\exported\\hrnetv2\\1\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/6\\exported\\hrnetv2\\2\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/6\\exported\\hrnetv2\\3\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/6\\exported\\hrnetv2\\4\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/6\\exported\\hrnetv2\\5\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/6\\exported\\hrnetv2\\6\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/6\\exported\\hrnetv2\\7\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/6\\exported\\hrnetv2\\8\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/6\\exported\\hrnetv2\\9\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/6\\exported\\hrnetv2\\10\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10\\0\\6\\exported\\hrnetv2\\assets\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).dataset\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.step\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.epoch\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.monitor_value\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'vhat' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/7\\exported\\hrnetv2\\1\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/7\\exported\\hrnetv2\\2\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/7\\exported\\hrnetv2\\3\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/7\\exported\\hrnetv2\\4\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/7\\exported\\hrnetv2\\5\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/7\\exported\\hrnetv2\\6\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/7\\exported\\hrnetv2\\7\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/7\\exported\\hrnetv2\\8\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/7\\exported\\hrnetv2\\9\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/7\\exported\\hrnetv2\\10\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10\\0\\7\\exported\\hrnetv2\\assets\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).dataset\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.step\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.epoch\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.monitor_value\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'vhat' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/8\\exported\\hrnetv2\\1\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/8\\exported\\hrnetv2\\2\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/8\\exported\\hrnetv2\\3\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/8\\exported\\hrnetv2\\4\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/8\\exported\\hrnetv2\\5\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/8\\exported\\hrnetv2\\6\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/8\\exported\\hrnetv2\\7\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/8\\exported\\hrnetv2\\8\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/8\\exported\\hrnetv2\\9\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/8\\exported\\hrnetv2\\10\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10\\0\\8\\exported\\hrnetv2\\assets\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).dataset\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.step\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.epoch\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.monitor_value\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'vhat' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/9\\exported\\hrnetv2\\1\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/9\\exported\\hrnetv2\\2\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/9\\exported\\hrnetv2\\3\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/9\\exported\\hrnetv2\\4\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/9\\exported\\hrnetv2\\5\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/9\\exported\\hrnetv2\\6\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/9\\exported\\hrnetv2\\7\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/9\\exported\\hrnetv2\\8\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/9\\exported\\hrnetv2\\9\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10/0/9\\exported\\hrnetv2\\10\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ../glint360k_224/models/new_fake_20000_kmeans10\\0\\9\\exported\\hrnetv2\\assets\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).dataset\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).monitor.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.step\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.epoch\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).schedule.monitor_value\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.total\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).metrics.loss.count\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'vhat' for (root).model.layer_with_weights-0.kernel\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "test_number = 0\n",
    "for r in (range(n_clusters)):\n",
    "    pprint(('cluster', str(r)))\n",
    "    train_ids = utility_functions.train_calc_ids(unique_result[r], trainy, trainl, r)\n",
    "    test_ids = utility_functions.test_calc_ids(unique_result[r], trainy, trainl, r)\n",
    "    train_sample_count, test_sample_count = utility_functions.convert_emb_to_tfrecord(dataset_name, scenario_number, train_ids, test_ids, r, overwrite=False, all_samples=True, n_classes=len(unique_result[r]))\n",
    "    train_dataset, test_dataset = utility_functions.prepare_data_sets(dataset_name, scenario_number, train_ids, test_ids, r)    \n",
    "    # training.softmax_train(dataset_name, scenario_number, train_dataset, test_dataset, unique_result[r], trainx, trainy, trainl, r, test_number=test_number, epochs=50,train_overwrite=False, freq = train_sample_count // 100)\n",
    "    training.softmax_train(dataset_name, scenario_number, train_dataset, unique_result[r], trainx, trainy, trainl, r, test_number=test_number, epochs=50,train_overwrite=False, freq = train_sample_count // 100)\n",
    "    # val_acc = utility_functions.test_acc_calc(r, test_dataset, dataset_name, scenario_number, test_number=test_number)\n",
    "    # pprint(('val_acc : ', str(val_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency = 1000\n",
    "# sub_index = 0\n",
    "# num_ids = len(result[0])\n",
    "\n",
    "# name = \"hrnetv2\"\n",
    "# if test_number != None:\n",
    "#     export_dir = os.path.join('./glint360k_224/models/' , scenario_number , str(test_number), str(sub_index), 'exported', name)\n",
    "# else:\n",
    "#     export_dir = os.path.join('./glint360k_224/models/' , scenario_number, str(sub_index), 'exported', name)\n",
    "\n",
    "# input_shape = (512,)\n",
    "# # num_ids = len(cluster)\n",
    "# regularizer = keras.regularizers.L2(5e-4)\n",
    "# name = \"hrnetv2\"\n",
    "\n",
    "# model = keras.Sequential([keras.Input(input_shape), \\\n",
    "#     # tf.keras.layers.Dense(256, activation='relu'), \\\n",
    "#     # tf.keras.layers.Dense(128, activation='relu'), \\\n",
    "#     training.L2Normalization(), \\\n",
    "#     training.ArcLayer(num_ids, regularizer)], \\\n",
    "#     name=\"training_model\")\n",
    "\n",
    "# loss_fun = training.ArcLoss(n_classes=num_ids)\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(0.001, amsgrad=True, epsilon=0.001)\n",
    "\n",
    "# path = None\n",
    "# if test_number != None:\n",
    "#     path = './glint360k_224/models/' + scenario_number + '/' + str(test_number) + '/' + str(sub_index)\n",
    "# else:\n",
    "#     path = './glint360k_224/models/' + scenario_number + '/' + str(sub_index)\n",
    "\n",
    "# supervisor = training.TrainingSupervisor(model,\n",
    "#                                 optimizer,\n",
    "#                                 loss_fun,\n",
    "#                                 train_dataset,\n",
    "#                                 path,\n",
    "#                                 frequency,\n",
    "#                                 \"categorical_accuracy\",\n",
    "#                                 'max',\n",
    "#                                 name,\n",
    "#                                 num_ids)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = []\n",
    "# batch_size = 500\n",
    "# batch_numbers = len(testx) // batch_size + (1 if len(testx) % batch_size != 0 else 0)\n",
    "# for i in tqdm(range(batch_numbers)):\n",
    "#     preds.extend(np.argmax(model(testx[i * batch_size : np.min([(i + 1) * batch_size, len(testx)])]), axis=1))\n",
    "\n",
    "# trues = 0\n",
    "# falses = 0\n",
    "# for idx in range(len(preds)):\n",
    "#     if testl[idx] == preds[idx]:\n",
    "#         trues += weights[idx]\n",
    "#     else:\n",
    "#         falses += weights[idx]\n",
    "#     if idx % 1000 == 0 and idx > 0:\n",
    "#         print((trues + falses) / n_classes, trues / (trues + falses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # supervisor.restore(True)\n",
    "# checkpoint = tf.train.Checkpoint(model)\n",
    "# supervisor.checkpoint.restore(supervisor.manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in test_dataset:\n",
    "#     print(t)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argmax(model(t[0]), 1), t[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # only for one cluster\n",
    "# sub = 0\n",
    "# test_number = 0\n",
    "# model = tf.keras.models.load_model(os.path.join('..', dataset_name, 'models', str(scenario_number),  str(test_number), str(sub), 'exported', 'hrnetv2'))\n",
    "\n",
    "# lbls = []\n",
    "# preds = []\n",
    "# for test in tqdm(test_dataset):\n",
    "#     lbls.extend(test[1].numpy())\n",
    "#     preds += list(np.argmax(model(test[0]), axis=1))\n",
    "\n",
    "# trues = 0\n",
    "# falses = 0\n",
    "# for idx in range(np.min([len(lbls), len(weights)])):\n",
    "#     if lbls[idx] == preds[idx]:\n",
    "#         trues += weights[idx]\n",
    "#     else:\n",
    "#         falses += weights[idx]\n",
    "# val_acc = trues / (trues + falses)\n",
    "# pprint(('val_acc for cluster ', sub, ' is : ', val_acc ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_classes = 1000\n",
    "# n_clusters = 1\n",
    "\n",
    "# methods = [\"full_random\", \"half_random\", \"kmeans\", \"sample_kmeans\"]\n",
    "# method = methods[2]\n",
    "# # scenario_number = 'normal_' + str(n_classes) + '_' + method + str(n_clusters)\n",
    "# scenario_number = 'new_fake_' + str(n_classes) + '_' + method + str(n_clusters)\n",
    "# dataset_name = 'glint360k_224'\n",
    "# # scenario_number = str(n_classes) + '_half_random'\n",
    "\n",
    "# result_path = join('..', dataset_name, 'data', scenario_number,  'result_' + str(test_number) + '.npz')    \n",
    "# result = (np.load(result_path, allow_pickle=True)['res']).item()\n",
    "# unique_result = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:17,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:03<00:12,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:05<00:12,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:07<00:11,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:09<00:09,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:10<00:07,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:12<00:05,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:14<00:03,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:16<00:01,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:17<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "test_number = 0\n",
    "\n",
    "model = None\n",
    "batch_softmax = None\n",
    "batch_argmax_softmax = None\n",
    "batch_max_softmax = None\n",
    "\n",
    "softmax_prediction = dict()\n",
    "batch_size = 1000\n",
    "# batch_number = len(temp_testx) // batch_size + (1 if (len(temp_testx) % batch_size != 0) else 0)\n",
    "batch_number = len(testx) // batch_size + (1 if (len(testx) % batch_size != 0) else 0)\n",
    "\n",
    "max_softmax = dict()\n",
    "argmax_softmax = dict()\n",
    "\n",
    "for idx in tqdm(range(n_clusters)):    \n",
    "    # print(idx)\n",
    "    if False: #os.path.isfile(join('..', dataset_name, 'models', str(scenario_number), str(test_number), str(idx) + '_predicted_max.npz')) and os.path.isfile(join('..', dataset_name, 'models', str(scenario_number), str(test_number), str(idx) + '_predicted_argmax.npz')):\n",
    "        max_softmax[idx] = np.load(join('..', dataset_name, 'models', str(scenario_number), str(test_number), str(idx) + '_predicted_max.npz'))['res']\n",
    "        argmax_softmax[idx] = np.load(join('..', dataset_name, 'models', str(scenario_number), str(test_number), str(idx) + '_predicted_argmax.npz'))['res']\n",
    "    else:\n",
    "        # model_path=join('..', dataset_name, 'models', str(scenario_number), str(0) + '_best_model.h5')\n",
    "        model_path = join('..', dataset_name, 'models', str(scenario_number), str(test_number), str(idx), 'exported', 'hrnetv2')\n",
    "        # with tf.device('/cpu:0'):\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "        max_softmax[idx] = []\n",
    "        argmax_softmax[idx] = []\n",
    "\n",
    "        for batch_counter in range(batch_number):\n",
    "            # print(len(testx[batch_counter * batch_size : np.min([len(testx), (batch_counter + 1) * batch_size])][0]))\n",
    "            # batch_softmax = model.predict(np.array(temp_testx[batch_counter * batch_size : np.min([len(temp_testx), (batch_counter + 1) * batch_size])]))\n",
    "            batch_softmax = model(np.array(testx[batch_counter * batch_size : np.min([len(testx), (batch_counter + 1) * batch_size])]))\n",
    "            batch_max_softmax = np.max(batch_softmax, axis=1)\n",
    "            batch_argmax_softmax = np.argmax(batch_softmax, axis=1)\n",
    "            max_softmax[idx] += list(batch_max_softmax)\n",
    "            argmax_softmax[idx] += list(batch_argmax_softmax)\n",
    "        savez_compressed(join('..', dataset_name, 'models', str(scenario_number), str(test_number), str(idx) + '_predicted_max.npz'), res=max_softmax[idx])\n",
    "        savez_compressed(join('..', dataset_name, 'models', str(scenario_number), str(test_number), str(idx) + '_predicted_argmax.npz'), res=argmax_softmax[idx])\n",
    "\n",
    "max_softmax = np.array(list(max_softmax.values())).transpose()\n",
    "argmax_softmax = np.array(list(argmax_softmax.values())).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc_batch = None\n",
    "# batch_size = 50000\n",
    "# batch_numbers = len(testx) // batch_size + 1\n",
    "# for batch in tqdm(range(batch_numbers)):\n",
    "#     cc_batch = utility_functions.cos_sim(torch.Tensor(np.array(testx[batch * batch_size:np.min([len(testx), (batch + 1) * batch_size])])), torch.Tensor(traincenterx))\n",
    "#     torch.save(f=join('..', dataset_name, 'data', str(n_classes), str(n_classes) + '_cos_sim_' + str(batch) + '.npz'), obj=cc_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "models = dict()\n",
    "for idx in (range(n_clusters)):    \n",
    "    model_path = join('..', dataset_name, 'models', str(scenario_number), str(test_number), str(idx), 'exported', 'hrnetv2')\n",
    "    # model_path=join('..', dataset_name, 'models', str(scenario_number), str(0) + '_best_model.h5')\n",
    "    with tf.device('/cpu:0'):\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        models[idx] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_path = join('..', dataset_name, 'data', str(scenario_number))\n",
    "\n",
    "# if os.path.isfile(join(pre_path, 'weights.pt')):\n",
    "#     weights = np.array(torch.load(join(pre_path, 'weights.pt')))\n",
    "# else:\n",
    "#     weights = []\n",
    "#     for i in range(n_classes):\n",
    "#         w = (len(np.where(testl == i)[0]))\n",
    "#         for s in range(w):\n",
    "#             weights.append(1/w)\n",
    "#     weights = np.array(weights)\n",
    "\n",
    "#     torch.save(torch.Tensor(weights), join(pre_path, 'weights.pt'))\n",
    "#     weights = np.array(torch.load(join(pre_path, 'weights.pt')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 50000\n",
    "# batch_numbers = len(testx) // batch_size + 1\n",
    "# next_p = 0\n",
    "# points = []\n",
    "# while (next_p < len(testx)):\n",
    "#     points.append(next_p)\n",
    "#     next_p += batch_size\n",
    "\n",
    "# len(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:27<00:00,  1.37s/it]\n",
      "100%|██████████| 10/10 [01:53<00:00, 11.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# ابتدا نزدیکترین دسته به هر نمونه آزمون را پیدا میکنیم\n",
    "# شماره دسته، شماره خوشه، شماره دسته در خوشه و مقدار شباهت کسینوسی و مقدار سافتمکس نزدیکترین دسته را در لیستهای مجزا ذخیره میکنیم\n",
    "batch_size = 5000\n",
    "batch_numbers = len(testx) //batch_size #+ 1\n",
    "\n",
    "sim_clusters = []\n",
    "sim_classes = []\n",
    "sim_classes_in_clusters=[]\n",
    "sim_values = []\n",
    "\n",
    "pre_path = join('..', dataset_name, 'data', str(scenario_number))\n",
    "\n",
    "if False: #os.path.isfile(join(pre_path, 'sim_clusters.pt')):\n",
    "    sim_clusters = np.array(torch.load(join(pre_path, 'sim_clusters.pt')))\n",
    "    sim_classes = np.array(torch.load(join(pre_path, 'sim_classes.pt')))\n",
    "    sim_classes_in_clusters = np.array(torch.load(join(pre_path, 'sim_classes_in_clusters.pt')))\n",
    "    sim_values = np.array(torch.load(join(pre_path, 'sim_values.pt')))\n",
    "    sim_softmax = np.array(torch.load(join(pre_path, 'sim_softmax.pt')))\n",
    "else:\n",
    "    for batch in tqdm(range(batch_numbers)):\n",
    "        if batch == batch_numbers - 1 and (len(testx) % batch_size):\n",
    "            batch_clusters = [0] * (len(testx) % batch_size)\n",
    "        else:\n",
    "            batch_clusters = [0] * batch_size\n",
    "        batch_classes_in_clusters = []\n",
    "        batch_sim = utility_functions.euc_sim(torch.Tensor(testx[batch*batch_size:np.min([len(testx), (batch+1)*batch_size])]), torch.Tensor(traincenterx)) #cos_sim(torch.Tensor(testx[batch*batch_size:np.min([len(testx), (batch+1)*batch_size])]), torch.Tensor(traincenterx))\n",
    "        \n",
    "        # just for euclidean\n",
    "        v = batch_sim\n",
    "        v_min, v_max = v.min(), v.max() #(dim=1)[0], v.max(dim=1)[0]\n",
    "        new_min, new_max = 0, 0.9\n",
    "        v_p = ((v.transpose(0,1) - v_min)/(v_max - v_min)*(new_max - new_min) + new_min).transpose(0,1)\n",
    "        batch_sim = v_p\n",
    "        # delete 5 lines above for cosine sim\n",
    "\n",
    "        batch_classes = (batch_sim.max(1)[1]).numpy()\n",
    "        batch_values = (batch_sim.max(1)[0]).numpy()\n",
    "        for r in unique_result:\n",
    "            batch_clusters += (r * (np.in1d(batch_classes, unique_result[r])).astype(np.int32))\n",
    "        for idx, b in enumerate(batch_classes):\n",
    "            batch_classes_in_clusters.append(np.where(unique_result[batch_clusters[idx]] == batch_classes[idx].item())[0][0])\n",
    "        \n",
    "        sim_clusters.extend(list(batch_clusters))\n",
    "        sim_classes.extend(list(batch_classes))\n",
    "        sim_classes_in_clusters.extend(list(batch_classes_in_clusters))\n",
    "        sim_values.extend(list(batch_values))\n",
    "\n",
    "    torch.save(torch.Tensor(sim_clusters), join(pre_path, 'sim_clusters.pt'))\n",
    "    torch.save(torch.Tensor(sim_classes), join(pre_path, 'sim_classes.pt'))\n",
    "    torch.save(torch.Tensor(sim_classes_in_clusters), join(pre_path, 'sim_classes_in_clusters.pt'))\n",
    "    torch.save(torch.Tensor(sim_values), join(pre_path, 'sim_values.pt'))\n",
    "\n",
    "    sim_clusters = np.array(torch.load(join(pre_path, 'sim_clusters.pt')))\n",
    "    sim_classes = np.array(torch.load(join(pre_path, 'sim_classes.pt')))\n",
    "    sim_classes_in_clusters = np.array(torch.load(join(pre_path, 'sim_classes_in_clusters.pt')))\n",
    "    sim_values = np.array(torch.load(join(pre_path, 'sim_values.pt')))\n",
    "\n",
    "    # نمونه های مربوط به هر خوشه را جدا میکنیم\n",
    "    ids = dict()\n",
    "    for r in unique_result:\n",
    "        ids[r] = np.where(sim_clusters == r)[0]\n",
    "\n",
    "    # دار سافتمکس دسته ای که بیشترین شباهت کسینوسی به داده آزمون را دارد\n",
    "    sim_softmax = np.zeros(len(testx))\n",
    "    m = dict()\n",
    "    batch_size = 1000\n",
    "    for i in tqdm(ids):\n",
    "        m[i] = []\n",
    "        batch_numbers = len(ids[i]) // batch_size + (1 if len(ids[i]) % batch_size != 0 else 0)\n",
    "        for batch in (range(batch_numbers)):\n",
    "            pr = models[i](testx[ids[i][batch * batch_size : np.min([len(ids[i]), (batch + 1) * batch_size])]])#[0][sim_classes_in_clusters[idx].int().item()]\n",
    "            for pidx, p in enumerate(pr):\n",
    "                m[i].append(pr[pidx][sim_classes_in_clusters[ids[i][pidx]].astype('int')])\n",
    "        sim_softmax[ids[i]] = m[i]\n",
    "\n",
    "    torch.save(torch.Tensor(sim_softmax), join(pre_path, 'sim_softmax.pt'))\n",
    "    sim_softmax = np.array(torch.load(join(pre_path, 'sim_softmax.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # دار سافتمکس دسته ای که بیشترین شباهت کسینوسی به داده آزمون را دارد\n",
    "# sim_softmax = np.zeros(len(testx))\n",
    "# m = dict()\n",
    "# batch_size = 1000\n",
    "# for i in tqdm(ids):\n",
    "#     m[i] = []\n",
    "#     batch_numbers = len(ids[i]) // batch_size + (1 if len(ids[i]) % batch_size != 0 else 0)\n",
    "#     for batch in (range(batch_numbers)):\n",
    "#         pr = models[i](testx[ids[i][batch * batch_size : np.min([len(ids[i]), (batch + 1) * batch_size])]])#[0][sim_classes_in_clusters[idx].int().item()]\n",
    "#         for pidx, p in enumerate(pr):\n",
    "#             m[i].append(pr[pidx][sim_classes_in_clusters[ids[i][pidx]].astype('int')])\n",
    "#     sim_softmax[ids[i]] = m[i]\n",
    "\n",
    "# torch.save(torch.Tensor(sim_softmax), join(pre_path, 'sim_softmax.pt'))\n",
    "# sim_softmax = np.array(torch.load(join(pre_path, 'sim_softmax.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [00:00, 618952.08it/s]\n",
      "100%|██████████| 100000/100000 [00:10<00:00, 9261.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# در مرحله دوم، ابتدا بهترین دسته هایی که بیشترین مقدار سافتمکس را دریافت کرده اند، پیدا میکنیم\n",
    "# شماره دسته، شماره خوشه، شماره دسته در خوشه، مقدار سافتمکس و مقدار شباهت کسینوسی دسته با داده آزمون\n",
    "if False: #os.path.isfile(join(pre_path, 'sofmax_values.pt')):\n",
    "    softmax_clusters = torch.load(join(pre_path, 'softmax_clusters.pt'))\n",
    "    softmax_classes = torch.load(join(pre_path, 'softmax_classes.pt'))\n",
    "    softmax_classes_in_clusters = torch.load(join(pre_path, 'softmax_classes_in_clusters.pt'))\n",
    "    sofmax_values = torch.load(join(pre_path, 'sofmax_values.pt'))\n",
    "    softmax_sims = torch.load(join(pre_path, 'softmax_sims.pt'))\n",
    "else:\n",
    "    sofmax_values = max_softmax.max(1)\n",
    "    softmax_clusters = max_softmax.argmax(1)\n",
    "    softmax_classes = []\n",
    "    softmax_classes_in_clusters = []\n",
    "    for idx, cl in tqdm(enumerate(softmax_clusters)):\n",
    "        softmax_classes.append(unique_result[cl][argmax_softmax[idx][cl]])\n",
    "        softmax_classes_in_clusters.append(argmax_softmax[idx][cl])\n",
    "\n",
    "    torch.save(torch.Tensor(softmax_clusters), join(pre_path, 'softmax_clusters.pt'))\n",
    "    torch.save(torch.Tensor(softmax_classes), join(pre_path, 'softmax_classes.pt'))\n",
    "    torch.save(torch.Tensor(softmax_classes_in_clusters), join(pre_path, 'softmax_classes_in_clusters.pt'))\n",
    "    torch.save(torch.Tensor(sofmax_values), join(pre_path, 'sofmax_values.pt'))\n",
    "\n",
    "    # مقدار شباهت کسینوسی دسته ای که بیشترین مقدار سافتمکس را دارد\n",
    "    softmax_sims = []\n",
    "    for idx in tqdm(range(len(testx))):\n",
    "        softmax_sims.append(utility_functions.euc_sim(torch.Tensor(np.array([testx[idx]])), torch.Tensor(np.array([traincenterx[softmax_classes[idx]]])))[0][0].item())\n",
    "        # softmax_sims.append(utility_functions.cos_sim(torch.Tensor(np.array([testx[idx]])), torch.Tensor(np.array([traincenterx[softmax_classes[idx]]])))[0][0].item())\n",
    "\n",
    "    # # just for euclidean\n",
    "    v = torch.Tensor(softmax_sims)\n",
    "    v_min, v_max = v.min(), v.max()\n",
    "    new_min, new_max = 0, 0.9\n",
    "    v_p = ((v - v_min)/(v_max - v_min)*(new_max - new_min) + new_min)\n",
    "    softmax_sims = v_p\n",
    "    # # delete 5 lines above for cosine sim\n",
    "\n",
    "    torch.save(torch.Tensor(softmax_sims), join(pre_path, 'softmax_sims.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.97189)\n",
      "(1, 0.97189)\n",
      "(2, 0.97189)\n",
      "(3, 0.97189)\n",
      "(4, 0.97189)\n",
      "(5, 0.97158)\n",
      "(6, 0.96856)\n",
      "(7, 0.96218)\n",
      "(8, 0.95674)\n",
      "(9, 0.95384)\n",
      "(10, 0.95264)\n",
      "(11, 0.95235)\n",
      "(12, 0.95235)\n",
      "(13, 0.95235)\n",
      "(14, 0.95235)\n",
      "(15, 0.95235)\n",
      "(16, 0.95235)\n",
      "(17, 0.95235)\n",
      "(18, 0.95235)\n",
      "(19, 0.95235)\n"
     ]
    }
   ],
   "source": [
    "true_prec = np.zeros(n_classes)\n",
    "false_prec = np.zeros(n_classes)\n",
    "\n",
    "true_recall = np.zeros(n_classes)\n",
    "false_recall = np.zeros(n_classes)\n",
    "\n",
    "if method != 'full_random':\n",
    "    main_preds = np.zeros((len(testx), 20))\n",
    "    if n_clusters > 1:\n",
    "        for th in range(0, 20):#20):\n",
    "            trues = 0\n",
    "            falses = 0\n",
    "            try:\n",
    "                res = (sim_values * (th / 10) * ((sim_softmax+1)/2)) > (sofmax_values * ((np.array(softmax_sims)+1)/2))\n",
    "            except:\n",
    "                res = (sim_values * (th / 10) * ((sim_softmax+1)/2)) > (sofmax_values.numpy() * ((np.array(softmax_sims.numpy())+1)/2))\n",
    "            for idx in range(len(testx)):\n",
    "                if sofmax_values[idx] > 0.5:\n",
    "                    main_preds[idx, th] = softmax_classes[idx]\n",
    "                    if softmax_classes[idx] == testl[idx]:\n",
    "                        true_prec[testl[idx]] += weights[idx]\n",
    "                        true_recall[testl[idx]] += weights[idx]\n",
    "                        trues += weights[idx]\n",
    "                    else:\n",
    "                        falses += weights[idx]\n",
    "                        try:\n",
    "                            false_prec[softmax_classes[idx].int()] += weights[idx]\n",
    "                        except:\n",
    "                            false_prec[softmax_classes[idx]] += weights[idx]\n",
    "\n",
    "                        false_recall[testl[idx]] += weights[idx]\n",
    "                    continue\n",
    "                if res[idx]:\n",
    "                    main_preds[idx, th] = sim_classes[idx]\n",
    "                    if sim_classes[idx] == testl[idx]:\n",
    "                        trues += weights[idx]\n",
    "                        true_recall[testl[idx]] += weights[idx]\n",
    "                        true_prec[testl[idx]] += weights[idx]\n",
    "                    else:\n",
    "                        falses += weights[idx]\n",
    "                        false_recall[testl[idx]] += weights[idx]\n",
    "                        try:\n",
    "                            false_prec[softmax_classes[idx].int()] += weights[idx]\n",
    "                        except:\n",
    "                            false_prec[softmax_classes[idx]] += weights[idx]\n",
    "\n",
    "                else:\n",
    "                    main_preds[idx, th] = softmax_classes[idx]\n",
    "                    if softmax_classes[idx] == testl[idx]:\n",
    "                        true_prec[testl[idx]] += weights[idx]\n",
    "                        true_recall[testl[idx]] += weights[idx]\n",
    "                        trues += weights[idx]\n",
    "                    else:\n",
    "                        falses += weights[idx]\n",
    "                        false_recall[testl[idx]] += weights[idx]\n",
    "                        try:\n",
    "                            false_prec[softmax_classes[idx].int()] += weights[idx]\n",
    "                        except:\n",
    "                            false_prec[softmax_classes[idx]] += weights[idx]                    \n",
    "            main_recall = trues / (trues + falses)\n",
    "            print((th, main_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9655426801716112, 0.959807, 0.9597214265510549)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_precision_array = np.divide(true_prec, (true_prec + false_prec), out=np.zeros_like(true_prec), where=(false_prec + true_prec)!=0)\n",
    "main_recall_array = np.divide(true_recall, (true_recall + false_recall), out=np.zeros_like(true_recall), where=(false_recall + true_recall)!=0)\n",
    "\n",
    "f_score_x = 2 * main_precision_array * main_recall_array\n",
    "f_score_y = main_precision_array + main_recall_array\n",
    "main_fscore_array =  np.divide(f_score_x, f_score_y, out=np.zeros_like(f_score_x), where=(f_score_y)!=0)\n",
    "\n",
    "main_precision = np.sum(main_precision_array) / n_classes\n",
    "main_recall = np.sum(main_recall_array) / n_classes\n",
    "main_fscore = np.sum(main_fscore_array) / n_classes\n",
    "\n",
    "main_precision, main_recall, main_fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models[0].load_weights('F:\\\\test\\\\deepface\\\\glint360k_224\\\\models\\\\new_fake_20000_kmeans2\\\\0\\\\0\\\\checkpoints\\\\hrnetv2\\\\ckpt-2')\n",
    "# checkpoint = tf.train.latest_checkpoint('F:\\\\test\\\\deepface\\\\glint360k_224\\\\models\\\\new_fake_20000_kmeans2\\\\0\\\\0\\\\checkpoints\\\\hrnetv2')\n",
    "\n",
    "# # latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "# checkpoint.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95235 {'precision': 0.9820500988181889, 'recall': 0.9523500000000001, 'f1-score': 0.9614659995714694, 'support': 100000} {'precision': 0.9820500988181887, 'recall': 0.95235, 'f1-score': 0.9614659995714694, 'support': 100000}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "if n_clusters > 1 and method != 'full_random':\n",
    "    main_report = (metrics.classification_report(testl, main_preds[:,11], output_dict=True, zero_division=0))\n",
    "    print(main_report['accuracy'], main_report['macro avg'], main_report['weighted avg'])\n",
    "\n",
    "#     fpr, tpr, ths = roc_curve(testl, main_preds[:,10])\n",
    "#     print(fpr, tpr, ths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # only first max (top k)\n",
    "# arg_list = []\n",
    "# for idx, arg in enumerate(argmax_softmax):\n",
    "#     t = []\n",
    "#     for i, a in enumerate(arg):\n",
    "#         t.append(unique_result[i][a])\n",
    "#     arg_list.append(t)\n",
    "# arg_list = np.array(arg_list)\n",
    "\n",
    "# trues = 0\n",
    "# falses = 0\n",
    "# for idx in range(len(testx)):\n",
    "#     if testl[idx] in arg_list[idx]:\n",
    "#         trues += weights[idx]\n",
    "#     else:\n",
    "#         falses += weights[idx]\n",
    "# trues / (trues + falses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn\n",
    "if n_clusters == 1:\n",
    "    trues = 0\n",
    "    falses = 0\n",
    "    knn_true_recall = np.zeros(n_classes)\n",
    "    knn_false_recall = np.zeros(n_classes)\n",
    "    knn_true_prec = np.zeros(n_classes)\n",
    "    knn_false_prec = np.zeros(n_classes)\n",
    "\n",
    "    index = 0 \n",
    "    cc = None\n",
    "    batch_size = 5000\n",
    "    batch_numbers = len(testx) // batch_size + 1\n",
    "\n",
    "    knn_main_preds = []\n",
    "\n",
    "    for batch in range(batch_numbers):\n",
    "        # cc = torch.load(join('..', dataset_name, 'data', str(n_classes), str(n_classes) + '_cos_sim_' + str(batch) + '.npz'))\n",
    "        cc = utility_functions.cos_sim(torch.Tensor(testx[batch * batch_size: (batch + 1) * batch_size]), torch.Tensor(traincenterx))\n",
    "        x = cc.max(1)[1]\n",
    "        \n",
    "        knn_main_preds.extend(x)\n",
    "        \n",
    "        # y = np.array(list(range(n_classes)))\n",
    "        for idx in range(len(x)):\n",
    "            real_idx = idx + batch*batch_size\n",
    "            real_class = testl[real_idx]\n",
    "            pred_class = x[idx]\n",
    "            if pred_class == real_class:\n",
    "                trues +=  weights[real_idx]\n",
    "                knn_true_recall[real_class] += weights[real_idx]\n",
    "                knn_true_prec[real_class] += weights[real_idx]\n",
    "            else:\n",
    "                falses += weights[idx + batch*batch_size]\n",
    "                knn_false_recall[real_class] += weights[real_idx]\n",
    "                knn_false_prec[pred_class] += weights[real_idx]\n",
    "        print(batch, trues / (trues + falses))\n",
    "    print(trues / (trues + falses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_clusters == 1:\n",
    "    knn_precision_array = np.divide(knn_true_prec, (knn_true_prec + knn_false_prec), out=np.zeros_like(knn_true_prec), where=(knn_false_prec + knn_true_prec)!=0)\n",
    "    knn_recall_array = np.divide(knn_true_recall, (knn_true_recall + knn_false_recall), out=np.zeros_like(knn_true_recall), where=(knn_false_recall + knn_true_recall)!=0)\n",
    "\n",
    "    knn_f_score_x = 2 * knn_precision_array * knn_recall_array\n",
    "    knn_f_score_y = knn_precision_array + knn_recall_array\n",
    "    knn_fscore_array =  np.divide(knn_f_score_x, knn_f_score_y, out=np.zeros_like(knn_f_score_x), where=(knn_f_score_y)!=0)\n",
    "\n",
    "    knn_precision = np.sum(knn_precision_array) / n_classes\n",
    "    knn_recall = np.sum(knn_recall_array) / n_classes\n",
    "    knn_fscore = np.sum(knn_fscore_array) / n_classes\n",
    "\n",
    "    print(knn_precision, knn_recall, knn_fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_clusters == 1:\n",
    "    knn_x = metrics.classification_report(testl, knn_main_preds, output_dict=True, zero_division=0)\n",
    "    print(knn_x['accuracy'], knn_x['macro avg'], knn_x['weighted avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97189"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only max_max\n",
    "max_max_trues = 0\n",
    "max_max_falses = 0\n",
    "\n",
    "max_max_true_prec = np.zeros(n_classes)\n",
    "max_max_false_prec = np.zeros(n_classes)\n",
    "max_max_true_recall = np.zeros(n_classes)\n",
    "max_max_false_recall = np.zeros(n_classes)\n",
    "\n",
    "index = 0\n",
    "for test_sample in range(len(testx)):\n",
    "    real_class = testl[test_sample]\n",
    "\n",
    "    if softmax_classes[test_sample] == real_class:\n",
    "        max_max_trues += weights[test_sample]\n",
    "        max_max_true_prec[real_class] += weights[test_sample]\n",
    "        max_max_true_recall[real_class] += weights[test_sample]\n",
    "    else:\n",
    "        max_max_falses += weights[test_sample]\n",
    "        max_max_false_recall[real_class] += weights[test_sample]\n",
    "        try:\n",
    "            max_max_false_prec[softmax_classes[test_sample].int()] += weights[test_sample]\n",
    "        except:\n",
    "            max_max_false_prec[softmax_classes[test_sample]] += weights[test_sample]\n",
    "\n",
    "max_max_trues / (max_max_trues + max_max_falses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # Exponentiate the elements of the input tensor\n",
    "    e = x.exp()\n",
    "    # Normalize the exponentiated values by their sum\n",
    "    return e / e.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = join('..', dataset_name, 'data', scenario_number,  'result_' + str(0) + '.npz')    \n",
    "model_filename = join('..', dataset_name, 'models', scenario_number, 'kmeans_' + str(0) + '.sav')\n",
    "kmeans_model = pickle.load(open(model_filename, 'rb'))\n",
    "result = (np.load(result_path, allow_pickle=True)['res']).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_result = dict()\n",
    "# centers, labels = clustering.init_centers(trainx, n_clusters)\n",
    "kmeans_model2 = clustering.Fast_KMeans(n_clusters=n_clusters, max_iter=100, tol=0.0001, verbose=0, mode='euclidean', minibatch=None)\n",
    "lbls = kmeans_model2.fit_predict(torch.Tensor(traincenterx).cuda())\n",
    "for i in range(n_clusters):\n",
    "    euclidean_result[i] = (lbls == i).nonzero().cpu().numpy().squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.9790209790209791\n",
      "2000 0.9825087456271864\n",
      "3000 0.9816727757414195\n",
      "4000 0.9805048737815546\n",
      "5000 0.9804039192161568\n",
      "6000 0.9803366105649058\n",
      "7000 0.9795743465219254\n",
      "8000 0.9798775153105862\n",
      "9000 0.9803355182757472\n",
      "10000 0.9806019398060194\n",
      "11000 0.9814562312517043\n",
      "12000 0.9810015832013999\n",
      "13000 0.9808476271056072\n",
      "14000 0.9810727805156775\n",
      "15000 0.9810012665822279\n",
      "16000 0.9808761952377977\n",
      "17000 0.9812952179283572\n",
      "18000 0.9813899227820677\n",
      "19000 0.9813694016104415\n",
      "20000 0.9815509224538773\n",
      "21000 0.9815723060806628\n",
      "22000 0.9818190082268988\n",
      "23000 0.9821312116864485\n",
      "24000 0.9821674096912628\n",
      "25000 0.9822407103715851\n",
      "26000 0.9820006922810661\n",
      "27000 0.9818154883152476\n",
      "28000 0.9816435127316882\n",
      "29000 0.9816558049722424\n",
      "30000 0.9816339455351488\n",
      "31000 0.9818070384826296\n",
      "32000 0.9818755663885503\n",
      "33000 0.9821520559983031\n",
      "34000 0.9820593511955531\n",
      "35000 0.9820862261078255\n",
      "36000 0.9820282769923058\n",
      "37000 0.9820815653630983\n",
      "38000 0.9820531038656878\n",
      "39000 0.9821543037358016\n",
      "40000 0.9821504462388441\n",
      "41000 0.9821955562059462\n",
      "42000 0.9821194733458727\n",
      "43000 0.9821864607799818\n",
      "44000 0.9821594963750824\n",
      "45000 0.9820670651763295\n",
      "46000 0.9820221299536966\n",
      "47000 0.9820642113997574\n",
      "48000 0.9818128788983563\n",
      "49000 0.9816942511377319\n",
      "50000 0.9815403691926161\n",
      "51000 0.9814905590086469\n",
      "52000 0.98153881656122\n",
      "53000 0.9815663855398955\n",
      "54000 0.9814447880594804\n",
      "55000 0.9814367011508882\n",
      "56000 0.9814289030553026\n",
      "57000 0.9814915527797758\n",
      "58000 0.9815175600420682\n",
      "59000 0.981644378908832\n",
      "60000 0.9817003049949168\n",
      "61000 0.9817216111211292\n",
      "62000 0.9816777148755665\n",
      "63000 0.9816669576673386\n",
      "64000 0.9815940375931627\n",
      "65000 0.9816002830725681\n",
      "66000 0.9816972470114089\n",
      "67000 0.9815823644423217\n",
      "68000 0.981426743724357\n",
      "69000 0.9813915740351589\n",
      "70000 0.9815002642819388\n",
      "71000 0.981521387022718\n",
      "72000 0.9816530325967695\n",
      "73000 0.9816577855097876\n",
      "74000 0.9816894366292347\n",
      "75000 0.9816135784856201\n",
      "76000 0.9816844515203748\n",
      "77000 0.9816495889663771\n",
      "78000 0.981602799964103\n",
      "79000 0.9816331438842546\n",
      "80000 0.9817252284346446\n",
      "81000 0.9817039295811163\n",
      "82000 0.9816953451787174\n",
      "83000 0.9817110637221238\n",
      "84000 0.9817502172593183\n",
      "85000 0.9817531558452254\n",
      "86000 0.9816048650597086\n",
      "87000 0.9815289479431271\n",
      "88000 0.9815115737321167\n",
      "89000 0.9815058257772384\n",
      "90000 0.9815002055532717\n",
      "91000 0.9814837199591213\n",
      "92000 0.9815762872142694\n",
      "93000 0.9815808432167396\n",
      "94000 0.9815108349911171\n",
      "95000 0.9815686150672098\n",
      "96000 0.9816251914042562\n",
      "97000 0.9816084370264224\n",
      "98000 0.9816532484362405\n",
      "99000 0.981676952758053\n"
     ]
    }
   ],
   "source": [
    "true_cluster = 0\n",
    "false_cluster = 0\n",
    "\n",
    "for test_sample in (range(len(testx))):\n",
    "    sims = utility_functions.cos_sim(torch.Tensor(testx[test_sample: test_sample+1]), torch.Tensor(traincenterx))[0]\n",
    "    cl = -1\n",
    "    for c in euclidean_result:\n",
    "        if sims.argmax().item() in euclidean_result[c]:\n",
    "            cl = c\n",
    "            break\n",
    "    if testl[test_sample] in euclidean_result[cl]:\n",
    "        true_cluster += 1\n",
    "    else:\n",
    "        false_cluster += 1\n",
    "        \n",
    "    if test_sample > 0 and test_sample % 1000 == 0:\n",
    "        print(test_sample, true_cluster / (true_cluster + false_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__GatherNd_device_/job:localhost/replica:0/task:0/device:GPU:0}} index innermost dimension length must be <= params rank; saw: 20000 vs. 1 [Op:GatherNd]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8220\\273028057.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Calculate the mean distance between each data point and its nearest cluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# mean_distance = tf.reduce_mean(tf.gather_nd(distance, tf.stack([tf.range(X.shape[0]), cluster_assignments], axis=1)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mmean_distance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather_nd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster_assignments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Define the training operation to optimize the cluster assignments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dianat\\miniconda3\\envs\\arcface-tf2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dianat\\miniconda3\\envs\\arcface-tf2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7208\u001b[0m   \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7209\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__GatherNd_device_/job:localhost/replica:0/task:0/device:GPU:0}} index innermost dimension length must be <= params rank; saw: 20000 vs. 1 [Op:GatherNd]"
     ]
    }
   ],
   "source": [
    "X = traincenterx\n",
    "# Choose the number of clusters\n",
    "n_clusters = 40\n",
    "\n",
    "# Use cosine similarity as the distance metric\n",
    "distance = 1 - tf.losses.cosine_similarity(X, X, axis=1)\n",
    "\n",
    "# Initialize the cluster assignments randomly\n",
    "cluster_assignments = tf.Variable(tf.random.uniform([X.shape[0]], maxval=n_clusters, dtype=tf.int32))\n",
    "\n",
    "# Calculate the mean distance between each data point and its nearest cluster\n",
    "# mean_distance = tf.reduce_mean(tf.gather_nd(distance, tf.stack([tf.range(X.shape[0]), cluster_assignments], axis=1)))\n",
    "mean_distance = tf.reduce_mean(tf.gather_nd(distance, tf.stack([tf.range(X.shape[0]), cluster_assignments], axis=1)[:, 0]))\n",
    "\n",
    "\n",
    "# Define the training operation to optimize the cluster assignments\n",
    "train_op = tf.train.AdamOptimizer().minimize(mean_distance)\n",
    "\n",
    "# Initialize the model variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Run the training loop\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(100):\n",
    "        _, d = sess.run([train_op, mean_distance])\n",
    "        if i % 10 == 0:\n",
    "            print(\"Step %d, mean distance: %f\" % (i, d))\n",
    "\n",
    "# Get the final cluster assignments\n",
    "cluster_assignments = sess.run(cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyclustering\n",
    "\n",
    "from pyclustering.cluster.kmedoids import kmedoids\n",
    "from pyclustering.cluster import cluster_visualizer\n",
    "from pyclustering.utils import read_sample\n",
    "from pyclustering.samples.definitions import FCPS_SAMPLES\n",
    "# # Load list of points for cluster analysis.\n",
    "# sample = read_sample(FCPS_SAMPLES.SAMPLE_TWO_DIAMONDS)\n",
    "# # Create instance of K-Medians algorithm.\n",
    "# initial_medians = [[0.0, 0.1], [2.5, 0.7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters for eps, min_samples: (0.1, 0, 20000)\n",
      "top-5 cluster memebers [1 1 1 1 1] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 1, 20000)\n",
      "top-5 cluster memebers [1 1 1 1 1] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 2, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 3, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 4, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 5, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 6, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 7, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 8, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 9, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 10, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 11, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 12, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 13, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 14, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 15, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 16, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 17, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 18, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.1, 19, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 0, 20000)\n",
      "top-5 cluster memebers [1 1 1 1 1] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 1, 20000)\n",
      "top-5 cluster memebers [1 1 1 1 1] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 2, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 3, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 4, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 5, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 6, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 7, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 8, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 9, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 10, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 11, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 12, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 13, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 14, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 15, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 16, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 17, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 18, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.2, 19, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 0, 19998)\n",
      "top-5 cluster memebers [3 1 1 1 1] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 1, 19998)\n",
      "top-5 cluster memebers [3 1 1 1 1] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 2, 1)\n",
      "top-5 cluster memebers [3] outlier_number 19997\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 3, 1)\n",
      "top-5 cluster memebers [3] outlier_number 19997\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 4, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 5, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 6, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 7, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 8, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 9, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 10, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 11, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 12, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 13, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 14, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 15, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 16, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 17, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 18, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.3, 19, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 0, 19989)\n",
      "top-5 cluster memebers [3 2 2 2 2] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 1, 19989)\n",
      "top-5 cluster memebers [3 2 2 2 2] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 2, 10)\n",
      "top-5 cluster memebers [3 2 2 2 2] outlier_number 19979\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 3, 1)\n",
      "top-5 cluster memebers [3] outlier_number 19997\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 4, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 5, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 6, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 7, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 8, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 9, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 10, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 11, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 12, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 13, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 14, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 15, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 16, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 17, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 18, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.4, 19, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 0, 19808)\n",
      "top-5 cluster memebers [14  7  6  5  4] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 1, 19808)\n",
      "top-5 cluster memebers [14  7  6  5  4] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 2, 139)\n",
      "top-5 cluster memebers [14  7  6  5  4] outlier_number 19669\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 3, 25)\n",
      "top-5 cluster memebers [14  7  6  5  4] outlier_number 19897\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 4, 10)\n",
      "top-5 cluster memebers [12  7  5  4  4] outlier_number 19951\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 5, 2)\n",
      "top-5 cluster memebers [9 6] outlier_number 19985\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 6, 1)\n",
      "top-5 cluster memebers [9] outlier_number 19991\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 7, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 8, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 9, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 10, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 11, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 12, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 13, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 14, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 15, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 16, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 17, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 18, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.5, 19, 0)\n",
      "top-5 cluster memebers [] outlier_number 20000\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 0, 14762)\n",
      "top-5 cluster memebers [3039   72   21   21   20] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 1, 14762)\n",
      "top-5 cluster memebers [3039   72   21   21   20] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 2, 1315)\n",
      "top-5 cluster memebers [3039   72   21   21   20] outlier_number 13447\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 3, 382)\n",
      "top-5 cluster memebers [3039   72   21   21   20] outlier_number 15313\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 4, 173)\n",
      "top-5 cluster memebers [2345   33   29   16   15] outlier_number 16777\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 5, 95)\n",
      "top-5 cluster memebers [1720   19   18   17   13] outlier_number 17694\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 6, 59)\n",
      "top-5 cluster memebers [851 459  46  25  20] outlier_number 18250\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 7, 37)\n",
      "top-5 cluster memebers [736 348  26  24  18] outlier_number 18613\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 8, 23)\n",
      "top-5 cluster memebers [676 265  14  12  11] outlier_number 18878\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 9, 17)\n",
      "top-5 cluster memebers [607 206  17  11  10] outlier_number 19048\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 10, 14)\n",
      "top-5 cluster memebers [533  95  66  12  12] outlier_number 19199\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 11, 10)\n",
      "top-5 cluster memebers [479  83  46  13  12] outlier_number 19323\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 12, 7)\n",
      "top-5 cluster memebers [418  70  40  13  12] outlier_number 19427\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 13, 8)\n",
      "top-5 cluster memebers [358  31  24  24  17] outlier_number 19514\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 14, 7)\n",
      "top-5 cluster memebers [294  32  22  22  15] outlier_number 19590\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 15, 5)\n",
      "top-5 cluster memebers [265  32  15  15  14] outlier_number 19659\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 16, 4)\n",
      "top-5 cluster memebers [243  32  16  16] outlier_number 19693\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 17, 3)\n",
      "top-5 cluster memebers [241  32  19] outlier_number 19708\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 18, 4)\n",
      "top-5 cluster memebers [219  32  19  15] outlier_number 19715\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.6, 19, 3)\n",
      "top-5 cluster memebers [207  32  20] outlier_number 19741\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 0, 258)\n",
      "top-5 cluster memebers [19741     2     2     1     1] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 1, 258)\n",
      "top-5 cluster memebers [19741     2     2     1     1] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 2, 3)\n",
      "top-5 cluster memebers [19741     2     2] outlier_number 255\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 3, 1)\n",
      "top-5 cluster memebers [19741] outlier_number 259\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 4, 1)\n",
      "top-5 cluster memebers [19729] outlier_number 271\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 5, 1)\n",
      "top-5 cluster memebers [19712] outlier_number 288\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 6, 1)\n",
      "top-5 cluster memebers [19678] outlier_number 322\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 7, 1)\n",
      "top-5 cluster memebers [19641] outlier_number 359\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 8, 2)\n",
      "top-5 cluster memebers [19604     2] outlier_number 394\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 9, 1)\n",
      "top-5 cluster memebers [19539] outlier_number 461\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 10, 1)\n",
      "top-5 cluster memebers [19478] outlier_number 522\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 11, 1)\n",
      "top-5 cluster memebers [19405] outlier_number 595\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 12, 1)\n",
      "top-5 cluster memebers [19306] outlier_number 694\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 13, 1)\n",
      "top-5 cluster memebers [19203] outlier_number 797\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 14, 1)\n",
      "top-5 cluster memebers [19078] outlier_number 922\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 15, 1)\n",
      "top-5 cluster memebers [18967] outlier_number 1033\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 16, 1)\n",
      "top-5 cluster memebers [18825] outlier_number 1175\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 17, 1)\n",
      "top-5 cluster memebers [18651] outlier_number 1349\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 18, 1)\n",
      "top-5 cluster memebers [18446] outlier_number 1554\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.7, 19, 2)\n",
      "top-5 cluster memebers [18246     4] outlier_number 1750\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 0, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 1, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 2, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 3, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 4, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 5, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 6, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 7, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 8, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 9, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 10, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 11, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 12, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 13, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 14, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 15, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 16, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 17, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 18, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.8, 19, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 0, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 1, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 2, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 3, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 4, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 5, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 6, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 7, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 8, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 9, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 10, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 11, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 12, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 13, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 14, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 15, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 16, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 17, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 18, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (0.9, 19, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 0, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 1, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 2, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 3, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 4, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 5, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 6, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 7, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 8, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 9, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 10, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 11, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 12, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 13, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 14, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 15, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 16, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 17, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 18, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.0, 19, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 0, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 1, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 2, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 3, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 4, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 5, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 6, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 7, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 8, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 9, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 10, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 11, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 12, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 13, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 14, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 15, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 16, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 17, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 18, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.1, 19, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 0, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 1, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 2, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 3, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 4, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 5, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 6, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 7, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 8, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 9, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 10, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 11, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 12, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 13, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 14, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 15, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 16, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 17, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 18, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.2, 19, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 0, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 1, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 2, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 3, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 4, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 5, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 6, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 7, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 8, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 9, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 10, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 11, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 12, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 13, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 14, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 15, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 16, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 17, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 18, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.3, 19, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 0, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 1, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 2, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 3, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 4, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 5, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 6, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 7, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 8, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 9, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 10, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 11, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 12, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 13, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 14, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 15, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 16, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 17, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 18, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.4, 19, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 0, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 1, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 2, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 3, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 4, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 5, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 6, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 7, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 8, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 9, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 10, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 11, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 12, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 13, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 14, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 15, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 16, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 17, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 18, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.5, 19, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 0, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 1, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 2, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 3, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 4, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 5, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 6, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 7, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 8, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 9, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 10, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 11, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 12, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 13, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 14, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 15, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 16, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 17, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 18, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.6, 19, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 0, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 1, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 2, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 3, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 4, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 5, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 6, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 7, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 8, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 9, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 10, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 11, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 12, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 13, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 14, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 15, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 16, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 17, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 18, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.7, 19, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 0, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 1, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 2, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 3, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 4, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 5, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 6, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 7, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 8, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 9, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 10, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 11, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 12, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 13, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 14, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 15, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 16, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 17, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 18, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.8, 19, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 0, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 1, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 2, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 3, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 4, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 5, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 6, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 7, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 8, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 9, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 10, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 11, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 12, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 13, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 14, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 15, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 16, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 17, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 18, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n",
      "Number of clusters for eps, min_samples: (1.9, 19, 1)\n",
      "top-5 cluster memebers [20000] outlier_number 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "data = traincenterx\n",
    "\n",
    "for i in range(1, 20):\n",
    "    for j in range(0, 20):\n",
    "        # Initialize the DBSCAN model and fit it to the data\n",
    "        dbscan = DBSCAN(eps=i / 10, min_samples=j, metric='cosine')\n",
    "        dbscan.fit(data)\n",
    "\n",
    "        # Extract the cluster labels\n",
    "        labels = dbscan.labels_\n",
    "\n",
    "        # Count the number of clusters\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "        print(f'Number of clusters for eps, min_samples: {i / 10, j, n_clusters}')\n",
    "        print(\"top-5 cluster memebers\", np.sort([len(np.where(labels == i)[0]) for i in range(n_clusters)])[::-1][:np.min([5, n_classes])], \"outlier_number\", len(np.where(labels == -1)[0]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1720,   19,   18,   17,   13,   12,   12,   11,   10,   10,   10,\n",
       "          10,    9,    9,    9,    9,    8,    8,    8,    8,    8,    8,\n",
       "           8,    7,    7,    7,    7,    7,    7,    6,    6,    6,    6,\n",
       "           6,    6,    6,    5,    5,    5,    5,    5,    5,    5,    5,\n",
       "           5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,\n",
       "           5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,\n",
       "           5,    5,    5,    5,    5,    5,    5,    4,    4,    4,    4,\n",
       "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
       "           4,    4,    3,    3,    3,    3,    3]),\n",
       " 17694)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort([len(np.where(labels == i)[0]) for i in range(n_clusters)])[::-1], len(np.where(labels == -1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 20000\n",
    "initial_medoids = random.sample(range(0, n_classes), 2)\n",
    "sample = list(traincenterx[:n_classes])\n",
    "# initial_medoids = [1, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of K-Medoids algorithm.\n",
    "kmedoids_instance = kmedoids(sample, initial_medoids)\n",
    "# Run cluster analysis and obtain results.\n",
    "kmedoids_instance.process()\n",
    "clusters = kmedoids_instance.get_clusters()\n",
    "# Show allocated clusters.\n",
    "# print(clusters)\n",
    "# # Display clusters.\n",
    "# visualizer = cluster_visualizer()\n",
    "# visualizer.append_clusters(clusters, sample)\n",
    "# visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrr = kmedoids_instance.predict(testx[20:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25773227,  0.4545543 , -0.10515788, ...,  0.25876018,\n",
       "         0.08961848,  0.7310423 ],\n",
       "       [ 0.43722457, -0.04950657,  0.07561445, ..., -0.00195827,\n",
       "         0.1699902 ,  0.8412865 ]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medoids_idx = kmedoids_instance.get_medoids()\n",
    "meds = traincenterx[medoids_idx]\n",
    "meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.7372627372627373\n",
      "2000 0.7681159420289855\n",
      "3000 0.7697434188603799\n",
      "4000 0.7593101724568858\n",
      "5000 0.7678464307138573\n",
      "6000 0.7655390768205299\n",
      "7000 0.7667476074846451\n",
      "8000 0.7619047619047619\n",
      "9000 0.7523608487945784\n",
      "10000 0.7584241575842415\n",
      "11000 0.7605672211617126\n",
      "12000 0.7564369635863678\n",
      "13000 0.7560187677870933\n",
      "14000 0.756660238554389\n",
      "15000 0.7572161855876275\n",
      "16000 0.7568276982688582\n",
      "17000 0.7561319922357508\n",
      "18000 0.7540136659074496\n",
      "19000 0.7553286669122677\n",
      "20000 0.7584620768961552\n",
      "21000 0.7582019903814105\n",
      "22000 0.7576473796645607\n",
      "23000 0.757010564758054\n",
      "24000 0.7570101245781425\n",
      "25000 0.7573697052117915\n",
      "26000 0.7558940040767663\n",
      "27000 0.7533054331321062\n",
      "28000 0.7514017356522982\n",
      "29000 0.7510430674804317\n",
      "30000 0.7475084163861204\n",
      "31000 0.7489758394890488\n",
      "32000 0.7471641511202775\n",
      "33000 0.7482500530286961\n",
      "34000 0.7468015646598629\n",
      "35000 0.7470643695894403\n",
      "36000 0.7456181772728535\n",
      "37000 0.7463852328315451\n",
      "38000 0.746059314228573\n",
      "39000 0.7453398630804339\n",
      "40000 0.7459063523411915\n",
      "41000 0.744225750591449\n",
      "42000 0.7447917906716507\n",
      "43000 0.7462152042975745\n",
      "44000 0.74668757528238\n",
      "45000 0.7466945178996023\n",
      "46000 0.7447229408056346\n",
      "47000 0.747047935150316\n",
      "48000 0.7455469677715048\n",
      "49000 0.745454174404604\n",
      "50000 0.745145097098058\n",
      "51000 0.746142232505245\n",
      "52000 0.7468702524951443\n",
      "53000 0.7449104733872945\n",
      "54000 0.7447639858521139\n",
      "55000 0.7455318994200105\n",
      "56000 0.746236674345101\n",
      "57000 0.7465658497219347\n",
      "58000 0.7457630040861365\n",
      "59000 0.746817850544906\n",
      "60000 0.7458542357627372\n",
      "61000 0.7465123522565204\n",
      "62000 0.74556861986097\n",
      "63000 0.7464960873636927\n",
      "64000 0.7464414618521585\n",
      "65000 0.7484961769818926\n",
      "66000 0.7489280465447493\n",
      "67000 0.7493320995209026\n",
      "68000 0.7502977897383862\n",
      "69000 0.7509456384690077\n",
      "70000 0.7513606948472165\n",
      "71000 0.7502570386332587\n",
      "72000 0.7493507034624519\n",
      "73000 0.7495376775660607\n",
      "74000 0.7499763516709234\n",
      "75000 0.7507766563112492\n",
      "76000 0.7513980079209485\n",
      "77000 0.7510162205685641\n",
      "78000 0.750323713798541\n",
      "79000 0.7495348160149872\n",
      "80000 0.7496031299608755\n",
      "81000 0.7499043221688622\n",
      "82000 0.7491250106706016\n",
      "83000 0.7499427717738341\n",
      "84000 0.7506934441256652\n",
      "85000 0.7517323325607934\n",
      "86000 0.7513633562400437\n",
      "87000 0.7509683796737968\n",
      "88000 0.7515141873387803\n",
      "89000 0.7511713351535376\n",
      "90000 0.7514360951544983\n",
      "91000 0.7521345919275613\n",
      "92000 0.7518179150226628\n",
      "93000 0.7522392232341588\n",
      "94000 0.7520239146392059\n",
      "95000 0.751107883074915\n",
      "96000 0.7499713544650577\n",
      "97000 0.7497448479912578\n",
      "98000 0.7491352129059907\n",
      "99000 0.7489217280633529\n",
      "100000 0.7494825051749483\n",
      "101000 0.7501608894961436\n",
      "102000 0.7506495034362408\n",
      "103000 0.751371345909263\n",
      "104000 0.7510408553763906\n",
      "105000 0.7510404662812735\n",
      "106000 0.7514268733313837\n",
      "107000 0.7510490556163026\n",
      "108000 0.751057860575365\n",
      "109000 0.7514701699984404\n",
      "110000 0.751747711384442\n",
      "111000 0.7520382699254962\n",
      "112000 0.7516807885643878\n",
      "113000 0.7517367102945992\n",
      "114000 0.7515986701871036\n",
      "115000 0.7516978113233798\n",
      "116000 0.7517176576064\n",
      "117000 0.7520961359304621\n",
      "118000 0.7524427759086787\n",
      "119000 0.7526995571465785\n",
      "120000 0.7528437263022808\n",
      "121000 0.7529193973603524\n",
      "122000 0.7534036606257326\n",
      "123000 0.7538475296948806\n",
      "124000 0.7537358569688954\n",
      "125000 0.7538659690722475\n",
      "126000 0.7542162363790763\n",
      "127000 0.7540176849001189\n",
      "128000 0.7540097342989508\n",
      "129000 0.7537693506251889\n",
      "130000 0.7537326635948954\n",
      "131000 0.7538721078465049\n",
      "132000 0.7543427701305293\n",
      "133000 0.7543552304118014\n",
      "134000 0.7540689994850784\n",
      "135000 0.7544907074762409\n",
      "136000 0.7546635686502305\n",
      "137000 0.7551988671615536\n",
      "138000 0.7555162643748958\n",
      "139000 0.7552535593269113\n",
      "140000 0.7551231776915879\n",
      "141000 0.7553918057318743\n",
      "142000 0.7551214428067408\n",
      "143000 0.7553583541373836\n",
      "144000 0.7552447552447552\n",
      "145000 0.7551603092392466\n",
      "146000 0.7550770200204108\n",
      "147000 0.7546479275651186\n",
      "148000 0.7550151688164269\n",
      "149000 0.7551291602069785\n",
      "150000 0.7552416317224552\n",
      "151000 0.7553459910861517\n",
      "152000 0.7554358195011875\n",
      "153000 0.7550538885366762\n",
      "154000 0.7550405516847293\n",
      "155000 0.753969329230134\n",
      "156000 0.7541810629419042\n",
      "157000 0.7543773606537538\n",
      "158000 0.7544509211967013\n",
      "159000 0.7543034320538865\n",
      "160000 0.7542077862013362\n",
      "161000 0.7543369295842883\n",
      "162000 0.7543533681890853\n",
      "163000 0.7543818749578224\n",
      "164000 0.754263693514064\n",
      "165000 0.7537045230028909\n",
      "166000 0.7536761826735984\n",
      "167000 0.7539415931641128\n",
      "168000 0.7543109862441295\n",
      "169000 0.7541789693552109\n",
      "170000 0.7542779160122588\n",
      "171000 0.7541183969684387\n",
      "172000 0.7537572455974093\n",
      "173000 0.7540129825839157\n",
      "174000 0.754484169631209\n",
      "175000 0.7537156930531825\n",
      "176000 0.7537286719961819\n",
      "177000 0.7537076061717165\n",
      "178000 0.7534845309857809\n",
      "179000 0.7534818241238875\n",
      "180000 0.753790256720796\n",
      "181000 0.7539074369754863\n",
      "182000 0.7540617908692809\n",
      "183000 0.7541598133343533\n",
      "184000 0.7541589447883436\n",
      "185000 0.7544607866984503\n",
      "186000 0.7542970199084951\n",
      "187000 0.7543756450500265\n",
      "188000 0.7545810926537625\n",
      "189000 0.7545727271284279\n",
      "190000 0.7548855006026284\n",
      "191000 0.7552840037486714\n",
      "192000 0.7553033577950115\n",
      "193000 0.7555038574929663\n",
      "194000 0.7560682676893418\n",
      "195000 0.756190993892339\n",
      "196000 0.7562461416013183\n",
      "197000 0.7564022517652195\n",
      "198000 0.7565012297917687\n",
      "199000 0.7566193134707866\n",
      "200000 0.7566312168439158\n",
      "201000 0.7565833005805941\n",
      "202000 0.7567140756728927\n",
      "203000 0.7567253363283925\n",
      "204000 0.7570207989176524\n",
      "205000 0.7572548426593041\n",
      "206000 0.757472051106548\n",
      "207000 0.7568175999149762\n",
      "208000 0.7567079004427864\n",
      "209000 0.7568719766891068\n",
      "210000 0.7572059180670568\n",
      "211000 0.7574798223705101\n",
      "212000 0.7575577473691162\n",
      "213000 0.7574424533218154\n",
      "214000 0.7574917874215541\n",
      "215000 0.757624383142404\n",
      "216000 0.7575103818963801\n",
      "217000 0.7579965069285395\n",
      "218000 0.758046981435865\n",
      "219000 0.7581380906936498\n",
      "220000 0.7587147331148495\n",
      "221000 0.7588517699014936\n",
      "222000 0.7584920788645096\n",
      "223000 0.758337406558715\n",
      "224000 0.7586082204990157\n",
      "225000 0.7586410727063435\n",
      "226000 0.7585939885221746\n",
      "227000 0.7586838824498571\n",
      "228000 0.7589703553931781\n",
      "229000 0.7591102222261038\n",
      "230000 0.7595488715266455\n",
      "231000 0.7598538534465219\n",
      "232000 0.7598501730595989\n",
      "233000 0.7597563958952965\n",
      "234000 0.7598300861962128\n",
      "235000 0.7595074063514623\n",
      "236000 0.7593823754984089\n",
      "237000 0.759136037400686\n",
      "238000 0.7592447090558443\n",
      "239000 0.7591641875975415\n",
      "240000 0.758996837513177\n",
      "241000 0.7589304608694569\n",
      "242000 0.7589844670063347\n",
      "243000 0.7592437891202094\n",
      "244000 0.759349346928906\n",
      "245000 0.7594417981967421\n",
      "246000 0.7594481323246653\n",
      "247000 0.7593410552993712\n",
      "248000 0.759355809049157\n",
      "249000 0.7593985566323027\n",
      "250000 0.7594809620761517\n",
      "251000 0.7595388066183003\n",
      "252000 0.7595327002670624\n",
      "253000 0.7597005545432627\n",
      "254000 0.759847402175582\n",
      "255000 0.7599342747675499\n",
      "256000 0.7601298432427999\n",
      "257000 0.7601877035497916\n",
      "258000 0.7604505408893764\n",
      "259000 0.7605491870687758\n",
      "260000 0.7604932288721966\n",
      "261000 0.7604836763077536\n",
      "262000 0.7607299208781646\n",
      "263000 0.7608982475351804\n",
      "264000 0.7610122688929208\n",
      "265000 0.761265051829993\n",
      "266000 0.7611700707892075\n",
      "267000 0.7613267366039828\n",
      "268000 0.761235965537442\n",
      "269000 0.7612722629283906\n",
      "270000 0.7613786615605127\n",
      "271000 0.7616946062929657\n",
      "272000 0.7616221999183826\n",
      "273000 0.7617224845330237\n",
      "274000 0.7616724026554648\n",
      "275000 0.7616954120166836\n",
      "276000 0.7618450657787472\n",
      "277000 0.7620730611080826\n",
      "278000 0.7619792734558509\n",
      "279000 0.7619291687126569\n",
      "280000 0.7620508498183935\n",
      "281000 0.7621004907455845\n",
      "282000 0.7620292126623665\n",
      "283000 0.7615202773135077\n",
      "284000 0.7615818254161076\n",
      "285000 0.7613131181995852\n",
      "286000 0.7611441918035252\n",
      "287000 0.7610043170581288\n",
      "288000 0.7612299957291815\n",
      "289000 0.7611357746166968\n",
      "290000 0.7615008224109572\n",
      "291000 0.7615265926921213\n",
      "292000 0.7613604062999785\n",
      "293000 0.7613148077992908\n",
      "294000 0.7612219006057802\n",
      "295000 0.7613906393537649\n",
      "296000 0.7614366167681866\n",
      "297000 0.7614216787148865\n",
      "298000 0.7613766396757058\n",
      "299000 0.7615927705927404\n",
      "300000 0.7614641284529051\n",
      "301000 0.7617051106142505\n",
      "302000 0.7619378743778994\n",
      "303000 0.7620634915396319\n",
      "304000 0.762392228972931\n",
      "305000 0.7627450401801962\n",
      "306000 0.762847180237973\n",
      "307000 0.762769502379471\n",
      "308000 0.7627670040032337\n",
      "309000 0.7627580493267012\n",
      "310000 0.7628846358560134\n",
      "311000 0.7629139456143228\n",
      "312000 0.7628533241880635\n",
      "313000 0.7628729620672139\n",
      "314000 0.7630580794328681\n",
      "315000 0.763067418833591\n",
      "316000 0.7627570798826586\n",
      "317000 0.7629281926555437\n",
      "318000 0.7629126952430967\n",
      "319000 0.7629693950802662\n",
      "320000 0.7630476154762016\n",
      "321000 0.7631751926006461\n",
      "322000 0.7628889351275306\n",
      "323000 0.7630131176064471\n",
      "324000 0.7631581384008074\n",
      "325000 0.7631299596001243\n",
      "326000 0.7631878429820768\n",
      "327000 0.7631719780673454\n",
      "328000 0.7632507217965799\n",
      "329000 0.7631405375667552\n",
      "330000 0.7632007175735831\n",
      "331000 0.7630581176491914\n",
      "332000 0.7630308342444752\n",
      "333000 0.7629106218900243\n",
      "334000 0.762623465199206\n",
      "335000 0.7626424995746282\n",
      "336000 0.762950110267529\n",
      "337000 0.7628315642980288\n",
      "338000 0.7628083940580057\n",
      "339000 0.7627853605151608\n",
      "340000 0.7626712862609227\n",
      "341000 0.7628247424494357\n",
      "342000 0.7628457226733255\n",
      "343000 0.762741216497911\n",
      "344000 0.7628408056953323\n",
      "345000 0.7627717021110084\n",
      "346000 0.7626625356574114\n",
      "347000 0.7626001077806692\n",
      "348000 0.7626213717776673\n",
      "349000 0.7625852074922421\n",
      "350000 0.7625578212633678\n",
      "351000 0.7625134971125438\n",
      "352000 0.76263135616092\n",
      "353000 0.7626777261254217\n",
      "354000 0.76328315456736\n",
      "355000 0.7629781324559649\n",
      "356000 0.7629753849006042\n",
      "357000 0.7628634093461923\n",
      "358000 0.7629699358381681\n",
      "359000 0.7630814398845686\n",
      "360000 0.7630701025830484\n",
      "361000 0.7630449777147432\n",
      "362000 0.7628597711056047\n",
      "363000 0.762631507902182\n",
      "364000 0.7625528501295327\n",
      "365000 0.7626198284388261\n",
      "366000 0.7626263316220447\n",
      "367000 0.7627308917414394\n",
      "368000 0.7628240140651792\n",
      "369000 0.7628190709510272\n",
      "370000 0.7629573974124394\n",
      "371000 0.7629494260123288\n",
      "372000 0.7630113897543286\n",
      "373000 0.7631883024442294\n",
      "374000 0.7633829856069904\n",
      "375000 0.7633792976552063\n",
      "376000 0.7634128632636615\n",
      "377000 0.7633799379842494\n",
      "378000 0.7632440125819773\n",
      "379000 0.763027010482822\n",
      "380000 0.7629269396659483\n",
      "381000 0.7630426166860454\n",
      "382000 0.7631681592456564\n",
      "383000 0.7629666763272158\n",
      "384000 0.7627480136770477\n",
      "385000 0.7625330843296511\n",
      "386000 0.7624332579449276\n",
      "387000 0.7625251614336914\n",
      "388000 0.7625083440506597\n",
      "389000 0.7624659062573104\n",
      "390000 0.7625672754685244\n",
      "391000 0.762627205557019\n",
      "392000 0.762638360616427\n",
      "393000 0.7629191783227015\n",
      "394000 0.7631173524940292\n",
      "395000 0.7631145237606993\n",
      "396000 0.7632455473597289\n",
      "397000 0.7628620582819691\n",
      "398000 0.7627920532862983\n",
      "399000 0.7627349304888961\n",
      "400000 0.762868092829768\n",
      "401000 0.7626689210251346\n",
      "402000 0.7626921326066353\n",
      "403000 0.7625762715228994\n",
      "404000 0.762559993663382\n",
      "405000 0.7625734257446278\n",
      "406000 0.7625375306957372\n",
      "407000 0.7625288389954816\n",
      "408000 0.7622309749240811\n",
      "409000 0.7623966689567996\n",
      "410000 0.7623713112894847\n",
      "411000 0.7624458334651254\n",
      "412000 0.7625224210620848\n",
      "413000 0.7626446425069189\n",
      "414000 0.762488496404598\n",
      "415000 0.7625571986573526\n",
      "416000 0.7626544166961137\n",
      "417000 0.7626288665974422\n",
      "418000 0.7626752089109835\n",
      "419000 0.7626401846296309\n",
      "420000 0.7625886605031893\n",
      "421000 0.7625753858066845\n",
      "422000 0.7623797100006872\n",
      "423000 0.7624993794340912\n",
      "424000 0.7623779189200025\n",
      "425000 0.7621299714588907\n",
      "426000 0.7619090096032638\n",
      "427000 0.7619349837588202\n",
      "428000 0.7619351356655709\n",
      "429000 0.7620028857741591\n",
      "430000 0.7621121811344624\n",
      "431000 0.7620840786912327\n",
      "432000 0.7620167545908458\n",
      "433000 0.7620190253602186\n",
      "434000 0.7620374146603349\n",
      "435000 0.7619798575175689\n",
      "436000 0.7619363258341151\n",
      "437000 0.7619090116498589\n",
      "438000 0.7618978038862925\n",
      "439000 0.7620233211313869\n",
      "440000 0.7619232683562083\n",
      "441000 0.7619982721127616\n",
      "442000 0.7619417150639931\n",
      "443000 0.7619463612948955\n",
      "444000 0.7617617978337886\n",
      "445000 0.7618589621146918\n",
      "446000 0.7618121932462035\n",
      "447000 0.761635880009217\n",
      "448000 0.7616947283599813\n",
      "449000 0.7619448509023365\n",
      "450000 0.7618383070259844\n",
      "451000 0.7618209272263254\n",
      "452000 0.7617881376368636\n",
      "453000 0.7619232628625544\n",
      "454000 0.7620864271224073\n",
      "455000 0.7620246988468157\n",
      "456000 0.7621891180063202\n",
      "457000 0.7622237150465754\n",
      "458000 0.7622559776070358\n",
      "459000 0.7622815636567241\n",
      "460000 0.7622157343136211\n",
      "461000 0.7621393446001202\n",
      "462000 0.762058956582345\n",
      "463000 0.7619940345701197\n",
      "464000 0.7620371507820026\n",
      "465000 0.7621402964724807\n",
      "466000 0.7620799096997646\n",
      "467000 0.7618998674521039\n",
      "468000 0.7618552097110903\n",
      "469000 0.7617574376174038\n",
      "470000 0.7617919961872421\n",
      "471000 0.761656556992448\n",
      "472000 0.7615513526454393\n",
      "473000 0.7614275656922501\n",
      "474000 0.7615659038694012\n",
      "475000 0.7616383965507441\n",
      "476000 0.7617925172426109\n",
      "477000 0.7619397024324897\n",
      "478000 0.7618896194777836\n",
      "479000 0.7618752361686093\n",
      "480000 0.7619109126855985\n",
      "481000 0.7619464408597904\n",
      "482000 0.7620212406198328\n",
      "483000 0.7620957306506612\n",
      "484000 0.762138921200576\n",
      "485000 0.7620582225603658\n",
      "486000 0.7620354690628208\n",
      "487000 0.7621052112829337\n",
      "488000 0.7623570443503189\n",
      "489000 0.7626446571683902\n",
      "490000 0.762586198803676\n",
      "491000 0.7624546589518147\n",
      "492000 0.7624252796234154\n",
      "493000 0.7625623477437166\n",
      "494000 0.7625733551146657\n",
      "495000 0.7624651263330782\n",
      "496000 0.7623916080814354\n",
      "497000 0.7624873994217315\n",
      "498000 0.7624984688785765\n",
      "499000 0.7625676100849498\n",
      "500000 0.7626964746070508\n",
      "501000 0.7628607527729485\n",
      "502000 0.7630064481943263\n",
      "503000 0.7629825785634621\n",
      "504000 0.7630976922664836\n",
      "505000 0.763081657264045\n",
      "506000 0.7631565945521847\n",
      "507000 0.763310131538202\n",
      "508000 0.7632445605422037\n",
      "509000 0.7632480093359345\n",
      "510000 0.7633886992378446\n",
      "511000 0.7633253163888133\n",
      "512000 0.7633305403700383\n",
      "513000 0.7632694673109799\n",
      "514000 0.7632767251425581\n",
      "515000 0.763035411581725\n",
      "516000 0.7631322419917791\n",
      "517000 0.7633176724996663\n",
      "518000 0.763419375638271\n",
      "519000 0.7632779127593203\n",
      "520000 0.763360070461403\n",
      "521000 0.7632826040640996\n",
      "522000 0.7631326376769393\n",
      "523000 0.7631820971661623\n",
      "524000 0.7632828944982929\n",
      "525000 0.7632537842785061\n",
      "526000 0.7631563438092323\n",
      "527000 0.7630706583099462\n",
      "528000 0.7632409787102676\n",
      "529000 0.7632669881531415\n",
      "530000 0.7632400693583598\n",
      "531000 0.7632076022455702\n",
      "532000 0.7632072120165188\n",
      "533000 0.7632237087735295\n",
      "534000 0.7631933273533196\n",
      "535000 0.7631593211975305\n",
      "536000 0.7631254419301456\n",
      "537000 0.7630916888422926\n",
      "538000 0.7630970946150658\n",
      "539000 0.7633084168675012\n",
      "540000 0.7634245121768293\n",
      "541000 0.763366426309748\n",
      "542000 0.7634063405787074\n",
      "543000 0.763484781795982\n",
      "544000 0.763511464133338\n",
      "545000 0.7634609844752579\n",
      "546000 0.7635590411006573\n",
      "547000 0.7635890976433315\n",
      "548000 0.7634876578692374\n",
      "549000 0.7635377713337499\n",
      "550000 0.7635822480322763\n",
      "551000 0.7635920805951351\n",
      "552000 0.7634478923045429\n",
      "553000 0.763470590469095\n",
      "554000 0.7634679359784549\n",
      "555000 0.763501327024636\n",
      "556000 0.7634806412218683\n",
      "557000 0.7634187371297358\n",
      "558000 0.7634108182601823\n",
      "559000 0.763447650361985\n",
      "560000 0.7634414938544752\n",
      "561000 0.7633854485107869\n",
      "562000 0.7633936594418871\n",
      "563000 0.7634995319724122\n",
      "564000 0.7636599935106498\n",
      "565000 0.7637278518091118\n",
      "566000 0.7637778025127164\n",
      "567000 0.7638681413260294\n",
      "568000 0.7638718945917349\n",
      "569000 0.7638492726726315\n",
      "570000 0.7639284843359924\n",
      "571000 0.7638357901299647\n",
      "572000 0.7638955176651789\n",
      "573000 0.7639655079135987\n",
      "574000 0.7639707944759678\n",
      "575000 0.7639030192990969\n",
      "576000 0.7638354794522926\n",
      "577000 0.7638756258654664\n",
      "578000 0.7638498895330631\n",
      "579000 0.7640194058386773\n",
      "580000 0.7640314413251011\n",
      "581000 0.764107118576388\n",
      "582000 0.7639454227741876\n",
      "583000 0.7639660995435685\n",
      "584000 0.7640911573781551\n",
      "585000 0.764084163958694\n",
      "586000 0.7640123481017951\n",
      "587000 0.7641247629901823\n",
      "588000 0.7640939386157507\n",
      "589000 0.7642211133767175\n",
      "590000 0.7642648063308367\n",
      "591000 0.7641831401300505\n",
      "592000 0.7641574929772078\n",
      "593000 0.7642061311869626\n",
      "594000 0.7640845722481948\n",
      "595000 0.7642323290212958\n",
      "596000 0.7642369727567572\n",
      "597000 0.7643052524200127\n",
      "598000 0.7643983872936667\n",
      "599000 0.7644027305463597\n",
      "600000 0.7642837261937897\n",
      "601000 0.7642932374488561\n",
      "602000 0.7641831159748904\n",
      "603000 0.7641695453241371\n",
      "604000 0.764157675235637\n",
      "605000 0.7641508030565238\n",
      "606000 0.7641538545315932\n",
      "607000 0.7641914922710177\n",
      "608000 0.7641714405074992\n",
      "609000 0.7640151658207458\n",
      "610000 0.7639085837564201\n",
      "611000 0.7640167528367384\n",
      "612000 0.7638582289898219\n",
      "613000 0.7639432888363967\n",
      "614000 0.7640068990115652\n",
      "615000 0.7641077006378851\n",
      "616000 0.7641789542549444\n",
      "617000 0.7640830403840513\n",
      "618000 0.7640068543578409\n",
      "619000 0.7640052277783073\n",
      "620000 0.7640019935451717\n",
      "621000 0.7640857261099419\n",
      "622000 0.7639794791326702\n",
      "623000 0.7639088861815632\n",
      "624000 0.7639619167276975\n",
      "625000 0.7640211775661159\n",
      "626000 0.7639732204900631\n",
      "627000 0.7640593874650917\n",
      "628000 0.7641452800234394\n",
      "629000 0.7641752556832183\n",
      "630000 0.7642718027431703\n",
      "631000 0.7642412611073517\n",
      "632000 0.7642392970897198\n",
      "633000 0.7642247010668229\n",
      "634000 0.7644215072215974\n",
      "635000 0.7644334418370995\n",
      "636000 0.7644311880012767\n",
      "637000 0.7643708565606647\n",
      "638000 0.7643467643467643\n",
      "639000 0.7643227475387362\n",
      "640000 0.7643347432269637\n",
      "641000 0.7643623020868923\n",
      "642000 0.7644427345128746\n",
      "643000 0.7645229167606271\n",
      "644000 0.764637011433212\n",
      "645000 0.7646205199681861\n",
      "646000 0.7646133674715674\n",
      "647000 0.7646294209746198\n",
      "648000 0.764588326252583\n",
      "649000 0.7645381131924296\n",
      "650000 0.7644634392870164\n",
      "651000 0.7643905308901215\n",
      "652000 0.7643960668771981\n",
      "653000 0.7645057205119135\n",
      "654000 0.7645324701338377\n",
      "655000 0.7644889091772379\n",
      "656000 0.7644500541919905\n",
      "657000 0.764399140944991\n",
      "658000 0.7644851603568992\n",
      "659000 0.7645284301541272\n"
     ]
    }
   ],
   "source": [
    "true_cluster = 0\n",
    "false_cluster = 0\n",
    "\n",
    "for test_sample in (range(len(trainx))):\n",
    "    sims = utility_functions.euc_sim(torch.Tensor(trainx[test_sample: test_sample+1]), torch.Tensor(meds))\n",
    "    if trainl[test_sample] in clusters[np.argmax(sims).item()]:\n",
    "        true_cluster += 1\n",
    "    else:\n",
    "        false_cluster += 1\n",
    "        \n",
    "    if test_sample > 0 and test_sample % 1000 == 0:\n",
    "        print(test_sample, true_cluster / (true_cluster + false_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_clusters = kmedians_instance.predict(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "eucl_result = dict()\n",
    "# centers, labels = clustering.init_centers(trainx, n_clusters)\n",
    "kmeans_model3 = clustering.Fast_KMeans(n_clusters=n_clusters, max_iter=100, tol=0.0001, verbose=0, mode='cosine', minibatch=None)\n",
    "lbls = kmeans_model3.fit_predict(torch.Tensor(trainx).cuda())\n",
    "for i in range(n_clusters):\n",
    "    eucl_result[i] = (lbls == i).nonzero().cpu().numpy().squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [02:55<00:00, 113.78it/s]\n"
     ]
    }
   ],
   "source": [
    "new_result = dict()\n",
    "for i in tqdm(range(n_classes)):\n",
    "    idxes = np.where(trainl == i)[0]\n",
    "    cnts = [0] * n_clusters\n",
    "    for ii in range(n_clusters):\n",
    "        cnts[ii] = np.sum(np.in1d(idxes,eucl_result[ii]))\n",
    "    if np.argmax(cnts) in new_result:\n",
    "        new_result[np.argmax(cnts)].append(i)\n",
    "    else:\n",
    "        new_result[np.argmax(cnts)] = [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.4745254745254745\n",
      "2000 0.496751624187906\n",
      "3000 0.4915028323892036\n",
      "4000 0.5008747813046738\n",
      "5000 0.5026994601079784\n",
      "6000 0.5054157640393268\n",
      "7000 0.5062133980859878\n",
      "8000 0.5024371953505812\n",
      "9000 0.5061659815576047\n",
      "10000 0.5036496350364964\n",
      "11000 0.5046813926006727\n",
      "12000 0.5024581284892926\n",
      "13000 0.5032689793092839\n",
      "14000 0.5052496250267838\n",
      "15000 0.5056996200253316\n",
      "16000 0.5055934004124742\n",
      "17000 0.5060878771836951\n",
      "18000 0.506082995389145\n",
      "19000 0.5044997631703595\n",
      "20000 0.5038748062596871\n",
      "21000 0.5034522165611162\n",
      "22000 0.5019771828553248\n",
      "23000 0.5021520803443329\n",
      "24000 0.5028123828173826\n",
      "25000 0.5019799208031679\n",
      "26000 0.5015576323987538\n",
      "27000 0.5003148031554386\n",
      "28000 0.5008035427306168\n",
      "29000 0.5002586117720078\n",
      "30000 0.5000166661111296\n",
      "31000 0.4991451888648753\n",
      "32000 0.4995156401362457\n",
      "33000 0.49956061937517043\n",
      "34000 0.499338254757213\n",
      "35000 0.49858575469272304\n",
      "36000 0.4985139301686064\n",
      "37000 0.49858111942920463\n",
      "38000 0.49861845740901556\n",
      "39000 0.49860259993333506\n",
      "40000 0.49843753906152344\n",
      "41000 0.4984756469354406\n",
      "42000 0.4980833789671674\n",
      "43000 0.49875584288737473\n",
      "44000 0.49817049612508807\n",
      "45000 0.4982333725917202\n",
      "46000 0.497489185017717\n",
      "47000 0.4971170826152635\n",
      "48000 0.49690631445178224\n",
      "49000 0.4973163812983408\n",
      "50000 0.496790064198716\n",
      "51000 0.49679418050626456\n",
      "52000 0.4956443145324128\n",
      "53000 0.49591517141186015\n",
      "54000 0.4956574878242995\n",
      "55000 0.49610007090780167\n",
      "56000 0.4959911430153033\n",
      "57000 0.4961667339169488\n",
      "58000 0.4959397251771521\n",
      "59000 0.49622887747665295\n",
      "60000 0.4960917318044699\n",
      "61000 0.4963853051589318\n",
      "62000 0.4965887646973436\n",
      "63000 0.4961984730401105\n",
      "64000 0.49630474523835566\n",
      "65000 0.4963308256796049\n",
      "66000 0.496356115816427\n",
      "67000 0.4963806510350592\n",
      "68000 0.4963603476419464\n",
      "69000 0.49609425950348546\n",
      "70000 0.49606434193797233\n",
      "71000 0.4958803397135252\n",
      "72000 0.4960347772947598\n",
      "73000 0.4953767756606074\n",
      "74000 0.4957905974243591\n",
      "75000 0.49574005679924266\n",
      "76000 0.49603294693490874\n",
      "77000 0.49595459799223385\n",
      "78000 0.4960833835463648\n",
      "79000 0.4959557473956026\n",
      "80000 0.49566880413994824\n",
      "81000 0.49588276687942123\n",
      "82000 0.4957378568554042\n",
      "83000 0.4958614956446308\n",
      "84000 0.495839335246009\n",
      "85000 0.49613533958424016\n",
      "86000 0.49584307159219076\n",
      "87000 0.496270157814278\n",
      "88000 0.49573300303405643\n",
      "89000 0.49554499387647327\n",
      "90000 0.49540560660437105\n",
      "91000 0.4954341161086142\n",
      "92000 0.49538591971826396\n",
      "93000 0.49553230610423543\n",
      "94000 0.4953457941936788\n",
      "95000 0.4951632088083283\n",
      "96000 0.4954323392464662\n",
      "97000 0.49522169874537375\n",
      "98000 0.4951071927837471\n",
      "99000 0.4949646973262896\n"
     ]
    }
   ],
   "source": [
    "cluster_center = kmeans_model3.centroids.cpu()\n",
    "\n",
    "true_cluster = 0\n",
    "false_cluster = 0\n",
    "\n",
    "for test_sample in (range(len(testx))):\n",
    "    sims = utility_functions.cos_sim(torch.Tensor(testx[test_sample: test_sample+1]), torch.Tensor(cluster_center))\n",
    "    if testl[test_sample] in new_result[np.argmax(sims).item()]:\n",
    "        true_cluster += 1\n",
    "    else:\n",
    "        false_cluster += 1\n",
    "        \n",
    "    if test_sample > 0 and test_sample % 1000 == 0:\n",
    "        print(test_sample, true_cluster / (true_cluster + false_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.5554445554445554\n",
      "2000 0.5332333833083458\n",
      "3000 0.5461512829056981\n",
      "4000 0.5373656585853537\n",
      "5000 0.5412917416516697\n",
      "6000 0.5499083486085652\n",
      "7000 0.5460648478788744\n",
      "8000 0.551931008623922\n",
      "9000 0.5388290189978892\n",
      "10000 0.5536446355364464\n",
      "11000 0.5580401781656213\n",
      "12000 0.5620364969585868\n",
      "13000 0.5582647488654718\n",
      "14000 0.5613170487822299\n",
      "15000 0.5616292247183521\n",
      "16000 0.5566527092056747\n",
      "17000 0.5629668843009235\n",
      "18000 0.5553580356646853\n",
      "19000 0.5564970264722909\n",
      "20000 0.5579221038948052\n",
      "21000 0.5547831055664016\n",
      "22000 0.5560656333803009\n",
      "23000 0.5580626929263945\n",
      "24000 0.5571434523561518\n",
      "25000 0.5550977960881565\n",
      "26000 0.5608245836698589\n",
      "27000 0.5639050405540535\n",
      "28000 0.5696939395021606\n",
      "29000 0.5688769352780939\n",
      "30000 0.5665144495183494\n",
      "31000 0.5662720557401374\n",
      "32000 0.5635448892222118\n",
      "33000 0.5622253871094816\n",
      "34000 0.562806976265404\n",
      "35000 0.564412445358704\n",
      "36000 0.5643176578428377\n",
      "37000 0.5642820464311775\n",
      "38000 0.563537801636799\n",
      "39000 0.563293248891054\n",
      "40000 0.5633109172270693\n",
      "41000 0.5654496231799224\n",
      "42000 0.5647722673269684\n",
      "43000 0.562731099276761\n",
      "44000 0.5615781459512283\n",
      "45000 0.561431968178485\n",
      "46000 0.564509467185496\n",
      "47000 0.5687964085870514\n",
      "48000 0.569300639570009\n",
      "49000 0.5697434746229669\n",
      "50000 0.5669886602267955\n",
      "51000 0.5667535930668026\n",
      "52000 0.566969865964116\n",
      "53000 0.5663855398954737\n",
      "54000 0.5669709820188515\n",
      "55000 0.5692805585352994\n",
      "56000 0.5686684166354172\n",
      "57000 0.5700250872791706\n",
      "58000 0.5709039499318977\n",
      "59000 0.5707530380841003\n",
      "60000 0.569807169880502\n",
      "61000 0.57069556236783\n",
      "62000 0.5712165932807536\n",
      "63000 0.5699115887049412\n",
      "64000 0.5688036124435556\n",
      "65000 0.5689604775311149\n",
      "66000 0.5691883456311267\n",
      "67000 0.5687825554842465\n",
      "68000 0.5703886707548418\n",
      "69000 0.569687395834843\n",
      "70000 0.5684204511364124\n",
      "71000 0.5691609977324263\n",
      "72000 0.5686448799322231\n",
      "73000 0.5692661744359666\n",
      "74000 0.5692085242091323\n",
      "75000 0.5688857481900241\n",
      "76000 0.5690188286996224\n",
      "77000 0.5682004129816496\n",
      "78000 0.5686850168587583\n",
      "79000 0.5687016620042784\n",
      "80000 0.5680553993075087\n",
      "81000 0.5676966951025296\n",
      "82000 0.5663711418153438\n",
      "83000 0.5656437874242479\n",
      "84000 0.5656123141391174\n",
      "85000 0.5657109916353925\n",
      "86000 0.5647259915582377\n",
      "87000 0.5635222583648464\n",
      "88000 0.564232224633811\n",
      "89000 0.564083549623038\n",
      "90000 0.5646270596993367\n",
      "91000 0.5646311578993637\n",
      "92000 0.5649503809741199\n",
      "93000 0.5654562854162859\n",
      "94000 0.5656535568770545\n",
      "95000 0.5668466647719498\n",
      "96000 0.5667545129738232\n",
      "97000 0.5672931206894775\n",
      "98000 0.5675860450403567\n",
      "99000 0.566782153715619\n",
      "100000 0.5666243337566624\n",
      "101000 0.5660637023395808\n",
      "102000 0.5665336614346919\n",
      "103000 0.5666061494548597\n",
      "104000 0.5666868587802041\n",
      "105000 0.5670898372396453\n",
      "106000 0.5663720153583457\n",
      "107000 0.566555452752778\n",
      "108000 0.5665410505458283\n",
      "109000 0.5672058054513262\n",
      "110000 0.566840301451805\n",
      "111000 0.5673011954847254\n",
      "112000 0.5677360023571218\n",
      "113000 0.5673224130759905\n",
      "114000 0.5665213463039798\n",
      "115000 0.5660124694567873\n",
      "116000 0.5660899475004526\n",
      "117000 0.5662344766284049\n",
      "118000 0.5657663918102389\n",
      "119000 0.5665750707977244\n",
      "120000 0.5666702777476854\n",
      "121000 0.5671936595565326\n",
      "122000 0.566331423512922\n",
      "123000 0.5664506792627703\n",
      "124000 0.5661325311892647\n",
      "125000 0.5658994728042176\n",
      "126000 0.5665351862286807\n",
      "127000 0.5665152242895726\n",
      "128000 0.5662924508402278\n",
      "129000 0.5657863117340176\n",
      "130000 0.5654110352997285\n",
      "131000 0.5657437729482981\n",
      "132000 0.5658593495503822\n",
      "133000 0.5655596574461846\n",
      "134000 0.5649957836135551\n",
      "135000 0.5650180369034303\n",
      "136000 0.565039962941449\n",
      "137000 0.5645579229348691\n",
      "138000 0.5640104057216977\n",
      "139000 0.5637153689541802\n",
      "140000 0.5647031092635053\n",
      "141000 0.5644073446287615\n",
      "142000 0.563573495961296\n",
      "143000 0.5644366123313823\n",
      "144000 0.5645933014354066\n",
      "145000 0.564078868421597\n",
      "146000 0.5640783282306285\n",
      "147000 0.5649485377650492\n",
      "148000 0.5650299660137431\n",
      "149000 0.5645868148535916\n",
      "150000 0.5641562389584069\n",
      "151000 0.5634201098005973\n",
      "152000 0.5638318168959415\n",
      "153000 0.5639963137495834\n",
      "154000 0.5642236089376043\n",
      "155000 0.5639576518861169\n",
      "156000 0.5642207421747296\n",
      "157000 0.5638371730116368\n",
      "158000 0.5634394719020765\n",
      "159000 0.5642857592090615\n",
      "160000 0.5641339741626615\n",
      "161000 0.5642511537195422\n",
      "162000 0.5638915809161672\n",
      "163000 0.5637388727676518\n",
      "164000 0.5635331491881147\n",
      "165000 0.5637238562190532\n",
      "166000 0.5637014234854006\n",
      "167000 0.5635834515960982\n",
      "168000 0.5637287873286468\n",
      "169000 0.563529209886332\n",
      "170000 0.5636084493620626\n",
      "171000 0.5631546014350793\n",
      "172000 0.5625316131882954\n",
      "173000 0.562840677221519\n",
      "174000 0.5630197527600416\n",
      "175000 0.5621339306632533\n",
      "176000 0.5619570343350322\n",
      "177000 0.5617990858808707\n",
      "178000 0.5618114504974692\n",
      "179000 0.5617678113530092\n",
      "180000 0.5615691023938756\n",
      "181000 0.5615880575245441\n",
      "182000 0.5612331800374724\n",
      "183000 0.5616034885055273\n",
      "184000 0.5609480383258787\n",
      "185000 0.5605321052318636\n",
      "186000 0.5609485970505534\n",
      "187000 0.5611253415757135\n",
      "188000 0.5609225482843176\n",
      "189000 0.5606319543282839\n",
      "190000 0.5608917847800801\n",
      "191000 0.5607352841084602\n",
      "192000 0.560679371461607\n",
      "193000 0.560675851420459\n",
      "194000 0.5605383477404756\n",
      "195000 0.5605048179240106\n",
      "196000 0.5608236692669936\n",
      "197000 0.5603271049385536\n",
      "198000 0.5606991883879374\n",
      "199000 0.5611278335284747\n",
      "200000 0.5613521932390338\n",
      "201000 0.5614549181347357\n",
      "202000 0.561155637843377\n",
      "203000 0.561174575494702\n",
      "204000 0.560850191910824\n",
      "205000 0.5612997009770684\n",
      "206000 0.5617351372080718\n",
      "207000 0.5628668460538838\n",
      "208000 0.5627424868149672\n",
      "209000 0.5629590289041679\n",
      "210000 0.56314493740506\n",
      "211000 0.5630968573608656\n",
      "212000 0.5630397969820897\n",
      "213000 0.5630020516335604\n",
      "214000 0.5635487684636987\n",
      "215000 0.5632624964535049\n",
      "216000 0.5624973958453896\n",
      "217000 0.5621771328242727\n",
      "218000 0.5621442103476589\n",
      "219000 0.5621983461262734\n",
      "220000 0.5627656237926191\n",
      "221000 0.5627531097144357\n",
      "222000 0.5631641298913068\n",
      "223000 0.5629077896511675\n",
      "224000 0.5626358810898165\n",
      "225000 0.5624863889493824\n",
      "226000 0.5625373339056022\n",
      "227000 0.5627376090854226\n",
      "228000 0.5623834983179898\n",
      "229000 0.5621503836227789\n",
      "230000 0.5619149481958774\n",
      "231000 0.5620451859515759\n",
      "232000 0.5620579221641286\n",
      "233000 0.5621435101136905\n",
      "234000 0.5623693915837966\n",
      "235000 0.5621678205624657\n",
      "236000 0.5620823640577794\n",
      "237000 0.5623436188032962\n",
      "238000 0.5624051999781513\n",
      "239000 0.5621106187840218\n",
      "240000 0.5624601564160149\n",
      "241000 0.5624416496197111\n",
      "242000 0.5623117259846033\n",
      "243000 0.5620100328805231\n",
      "244000 0.5618501563518182\n",
      "245000 0.5614793409006494\n",
      "246000 0.5614367421270645\n",
      "247000 0.5612649341500642\n",
      "248000 0.5612719303551196\n",
      "249000 0.561110196344593\n",
      "250000 0.5611577553689785\n",
      "251000 0.5611491587682917\n",
      "252000 0.5610255514859068\n",
      "253000 0.5611875051877265\n",
      "254000 0.5611277120956216\n",
      "255000 0.5612605440762978\n",
      "256000 0.5614509318322976\n",
      "257000 0.5617954793950218\n",
      "258000 0.5619861938519618\n",
      "259000 0.5619901081463006\n",
      "260000 0.5616209168426275\n",
      "261000 0.5611894207301887\n",
      "262000 0.5610551104766776\n",
      "263000 0.5610701099995817\n",
      "264000 0.561497873114117\n",
      "265000 0.5620318413892778\n",
      "266000 0.5620655561445258\n",
      "267000 0.5620840371384377\n",
      "268000 0.5619232764056851\n",
      "269000 0.5619681711220404\n",
      "270000 0.562068288635968\n",
      "271000 0.5617654547400194\n",
      "272000 0.5619207282326167\n",
      "273000 0.561957648506782\n",
      "274000 0.5617643731227259\n",
      "275000 0.5623615914123948\n",
      "276000 0.5624979619639059\n",
      "277000 0.5625936368460763\n",
      "278000 0.5623145240484746\n",
      "279000 0.5621807807140476\n",
      "280000 0.5623015632087028\n",
      "281000 0.5620905263682335\n",
      "282000 0.562320700990422\n",
      "283000 0.5621711584058007\n",
      "284000 0.5622902736257971\n",
      "285000 0.5625804821737468\n",
      "286000 0.5626518788395845\n",
      "287000 0.5624440332960512\n",
      "288000 0.562921656522026\n",
      "289000 0.5627869799758478\n",
      "290000 0.562984265571498\n",
      "291000 0.5629602647413583\n",
      "292000 0.5635151934411183\n",
      "293000 0.563353026098887\n",
      "294000 0.5636749534865527\n",
      "295000 0.563293005786421\n",
      "296000 0.5634237722169858\n",
      "297000 0.5633415375705806\n",
      "298000 0.5635283103076836\n",
      "299000 0.5638543014906304\n",
      "300000 0.5641481195062683\n",
      "301000 0.5639183922977\n",
      "302000 0.5637663451445525\n",
      "303000 0.5639090300032014\n",
      "304000 0.5640178815201266\n",
      "305000 0.5641030685145295\n",
      "306000 0.5643216852232509\n",
      "307000 0.5642554910244592\n",
      "308000 0.5644689465293944\n",
      "309000 0.5640370095889657\n",
      "310000 0.5637530201515479\n",
      "311000 0.563779537686374\n",
      "312000 0.5640398588466062\n",
      "313000 0.5640748751601432\n",
      "314000 0.5643453364798201\n",
      "315000 0.5643474147701119\n",
      "316000 0.5644665681437717\n",
      "317000 0.5642474313961154\n",
      "318000 0.5643661497919817\n",
      "319000 0.5643775411362347\n",
      "320000 0.5643701113434021\n",
      "321000 0.5642287718729848\n",
      "322000 0.5641100493476728\n",
      "323000 0.5643233302683274\n",
      "324000 0.5640661602896287\n",
      "325000 0.5639182648668774\n",
      "326000 0.563930785488388\n",
      "327000 0.5638759514496897\n",
      "328000 0.5638397443910232\n",
      "329000 0.563761204373239\n",
      "330000 0.5636679888848821\n",
      "331000 0.5635632520747671\n",
      "332000 0.5637061334152608\n",
      "333000 0.5637160248768022\n",
      "334000 0.5638935212768824\n",
      "335000 0.5636729442598679\n",
      "336000 0.5638673694423528\n",
      "337000 0.5637015913899365\n",
      "338000 0.5636551371149789\n",
      "339000 0.5636443550314011\n",
      "340000 0.5636806950567792\n",
      "341000 0.563710956859364\n",
      "342000 0.563580223449639\n",
      "343000 0.5634648295486019\n",
      "344000 0.5634402225574926\n",
      "345000 0.563082425848041\n",
      "346000 0.5630503958081046\n",
      "347000 0.5631741695268889\n",
      "348000 0.5631621748213367\n",
      "349000 0.5632935149182954\n",
      "350000 0.5635926754494988\n",
      "351000 0.5637818695673231\n",
      "352000 0.5636802168175659\n",
      "353000 0.563683955569531\n",
      "354000 0.5642159202940105\n",
      "355000 0.5643617905301676\n",
      "356000 0.5642287521664265\n",
      "357000 0.5642729292074812\n",
      "358000 0.5641827816123418\n",
      "359000 0.5640012144812967\n",
      "360000 0.5639984333376852\n",
      "361000 0.564017828205462\n",
      "362000 0.5639625304902472\n",
      "363000 0.5638248930443718\n",
      "364000 0.5641193293425018\n",
      "365000 0.5640696874803083\n",
      "366000 0.5636705910639589\n",
      "367000 0.5634834782466532\n",
      "368000 0.5637022725481724\n",
      "369000 0.5635567383286224\n",
      "370000 0.5635525309391056\n",
      "371000 0.5634836563782847\n",
      "372000 0.5635952591525292\n",
      "373000 0.5636419205310441\n",
      "374000 0.5636535731187885\n",
      "375000 0.5638491630688984\n",
      "376000 0.5639719043300416\n",
      "377000 0.5639985039827481\n",
      "378000 0.5638979791058754\n",
      "379000 0.564117245073232\n",
      "380000 0.5642958834318857\n",
      "381000 0.5642478628665016\n",
      "382000 0.5640089947408514\n",
      "383000 0.5639410863157015\n",
      "384000 0.5636912403873948\n",
      "385000 0.5632011345425076\n",
      "386000 0.5631021681290981\n",
      "387000 0.5631406637192152\n",
      "388000 0.563075868361164\n",
      "389000 0.5632813283256342\n",
      "390000 0.5629216335342729\n",
      "391000 0.5628502228894555\n",
      "392000 0.5628327478756432\n",
      "393000 0.5626041664016123\n",
      "394000 0.5625391813726361\n",
      "395000 0.5625049050508732\n",
      "396000 0.5626425185794985\n",
      "397000 0.5626635701169519\n",
      "398000 0.562538787590986\n",
      "399000 0.5625274122119994\n",
      "400000 0.5625685935785161\n",
      "401000 0.5624848815838365\n",
      "402000 0.5626752172258278\n",
      "403000 0.5626090257840551\n",
      "404000 0.5627040527127408\n",
      "405000 0.5625195987170402\n",
      "406000 0.5623188120226305\n",
      "407000 0.5623892815988167\n",
      "408000 0.5625574447121453\n",
      "409000 0.562397157953159\n",
      "410000 0.5621083851014997\n",
      "411000 0.5621908462509824\n",
      "412000 0.562379702961886\n",
      "413000 0.5621947646615868\n",
      "414000 0.5622836659814832\n",
      "415000 0.5622853920833926\n",
      "416000 0.5623279751731366\n",
      "417000 0.5622192752535365\n",
      "418000 0.5622211430116196\n",
      "419000 0.5621848157880291\n",
      "420000 0.5621224711369734\n",
      "421000 0.5620200427077371\n",
      "422000 0.5618351615280532\n",
      "423000 0.5618308230949809\n",
      "424000 0.5619915990764173\n",
      "425000 0.5618622073830415\n",
      "426000 0.5621348306694116\n",
      "427000 0.562237559162625\n",
      "428000 0.5624986857507343\n",
      "429000 0.5625138402940786\n",
      "430000 0.5624847384075851\n",
      "431000 0.5624047275992399\n",
      "432000 0.5624755498251162\n",
      "433000 0.5623751446301509\n",
      "434000 0.5623005476945906\n",
      "435000 0.5623435348424486\n",
      "436000 0.5623610955020746\n",
      "437000 0.5623854407655817\n",
      "438000 0.5622886705738115\n",
      "439000 0.5622880130113599\n",
      "440000 0.5623350856020781\n",
      "441000 0.5627402205437176\n",
      "442000 0.5627159214571913\n",
      "443000 0.562915207866348\n",
      "444000 0.5627239578289238\n",
      "445000 0.5626391850804829\n",
      "446000 0.5626153304589003\n",
      "447000 0.5624797259961387\n",
      "448000 0.5624808873194479\n",
      "449000 0.5627604392863268\n",
      "450000 0.562834304812656\n",
      "451000 0.562879018006612\n",
      "452000 0.5627222063668\n",
      "453000 0.5628861746442061\n",
      "454000 0.5628996411902176\n",
      "455000 0.5629174441374855\n",
      "456000 0.5628935024265298\n",
      "457000 0.5627252456777994\n",
      "458000 0.5627214787740639\n",
      "459000 0.5627721943960906\n",
      "460000 0.5628183416992572\n",
      "461000 0.562489018461999\n",
      "462000 0.5627606866651804\n",
      "463000 0.5627201669110866\n",
      "464000 0.5628306835545613\n",
      "465000 0.5627471768877916\n",
      "466000 0.5628206806423163\n",
      "467000 0.5628082166847609\n",
      "468000 0.5626120457007571\n",
      "469000 0.5628069023306986\n",
      "470000 0.5629285895136393\n",
      "471000 0.5628055991388553\n",
      "472000 0.5628293160395846\n",
      "473000 0.5628106494489441\n",
      "474000 0.5629038757302199\n",
      "475000 0.5627482889509706\n",
      "476000 0.5625681458652397\n",
      "477000 0.5627807908159521\n",
      "478000 0.5626055175616788\n",
      "479000 0.5625645875478339\n",
      "480000 0.5623425784529615\n",
      "481000 0.5623855251860184\n",
      "482000 0.5623515303910158\n",
      "483000 0.5622886909136834\n",
      "484000 0.5621889210972705\n",
      "485000 0.5624050259690186\n",
      "486000 0.562239583869169\n",
      "487000 0.5619639384724056\n",
      "488000 0.5619599140165696\n",
      "489000 0.5619190962799667\n",
      "490000 0.5618417105271214\n",
      "491000 0.561740200121792\n",
      "492000 0.5617976386226857\n",
      "493000 0.5618629576816274\n",
      "494000 0.5616891463782462\n",
      "495000 0.5616433098114954\n",
      "496000 0.5615734645696279\n",
      "497000 0.5616487693183716\n",
      "498000 0.5616655388242192\n",
      "499000 0.5615599968737538\n",
      "500000 0.5615968768062464\n",
      "501000 0.5615976814417536\n",
      "502000 0.561628363290113\n",
      "503000 0.5615634958976224\n",
      "504000 0.561584203205946\n",
      "505000 0.5614919574416685\n",
      "506000 0.5615759652648907\n",
      "507000 0.5616478074007744\n",
      "508000 0.5618768466991206\n",
      "509000 0.5619281690998642\n",
      "510000 0.5621185056499889\n",
      "511000 0.5620987043078194\n",
      "512000 0.5621395270712362\n",
      "513000 0.5621840893097674\n",
      "514000 0.5622479333697794\n",
      "515000 0.5619600738639342\n",
      "516000 0.562060926238515\n",
      "517000 0.5623238639770523\n",
      "518000 0.5623657097187071\n",
      "519000 0.5623264695058391\n",
      "520000 0.5622393033859551\n",
      "521000 0.5622868286241294\n",
      "522000 0.5622805320296321\n",
      "523000 0.5621079118395567\n",
      "524000 0.5620924387548879\n",
      "525000 0.5621665482541938\n",
      "526000 0.5620863838661904\n",
      "527000 0.5618490287494711\n",
      "528000 0.5620519658106709\n",
      "529000 0.5619932665533713\n",
      "530000 0.5620536565025349\n",
      "531000 0.5620384895696995\n",
      "532000 0.5619839060452894\n",
      "533000 0.5618582329113829\n",
      "534000 0.561897824161378\n",
      "535000 0.5620980147700658\n",
      "536000 0.5620194738442652\n",
      "537000 0.5619989534470141\n",
      "538000 0.5623037875394283\n",
      "539000 0.5623403296097781\n",
      "540000 0.5623619215519972\n",
      "541000 0.562359404141582\n",
      "542000 0.5622351250274445\n",
      "543000 0.5623728133097361\n",
      "544000 0.5623261721945364\n",
      "545000 0.5624283258195857\n",
      "546000 0.5625740612196681\n",
      "547000 0.5624907449894972\n",
      "548000 0.5625737909237392\n",
      "549000 0.5624415984670338\n",
      "550000 0.562391704742355\n",
      "551000 0.5623546962709687\n",
      "552000 0.5622924596151094\n",
      "553000 0.5622648060310922\n",
      "554000 0.5623726310963337\n",
      "555000 0.5625269143659201\n",
      "556000 0.5624935926374233\n",
      "557000 0.5625142504232488\n",
      "558000 0.5624667339305843\n",
      "559000 0.5625410330214078\n",
      "560000 0.5624579241822782\n",
      "561000 0.5625194964001846\n",
      "562000 0.5623904583799673\n",
      "563000 0.5624359459397053\n",
      "564000 0.5622294995930859\n",
      "565000 0.5620273238454445\n",
      "566000 0.5620838125727693\n",
      "567000 0.5620448641184055\n",
      "568000 0.5619726021609117\n",
      "569000 0.5620499788225328\n",
      "570000 0.5619569088475284\n",
      "571000 0.5618396465155052\n",
      "572000 0.5620392272041482\n",
      "573000 0.561953644059958\n",
      "574000 0.5617551188935211\n",
      "575000 0.561962500934781\n",
      "576000 0.5618097885246727\n",
      "577000 0.5618430470657764\n",
      "578000 0.5618554293158662\n",
      "579000 0.5618280452019945\n",
      "580000 0.56173523838752\n",
      "581000 0.5617425787563188\n",
      "582000 0.561811749464348\n",
      "583000 0.5616148171272434\n",
      "584000 0.5617610243818075\n",
      "585000 0.561657159560411\n",
      "586000 0.561613376086389\n",
      "587000 0.5617792814663007\n",
      "588000 0.5616810175492899\n",
      "589000 0.5616866524844609\n",
      "590000 0.5617431156896344\n",
      "591000 0.5617790832841231\n",
      "592000 0.5617203349318667\n",
      "593000 0.5616027628958468\n",
      "594000 0.5617785155243846\n",
      "595000 0.5617587197332441\n",
      "596000 0.5617591245652273\n",
      "597000 0.5616841512828287\n",
      "598000 0.5615776562246552\n",
      "599000 0.5616217669085695\n",
      "600000 0.5614723975460041\n",
      "601000 0.5613135419075842\n",
      "602000 0.5613545492449348\n",
      "603000 0.5615148233585019\n",
      "604000 0.5616629773791765\n",
      "605000 0.5616073361862212\n",
      "606000 0.561723495505783\n",
      "607000 0.5617733743436996\n",
      "608000 0.5617951286264332\n",
      "609000 0.5621238717177804\n",
      "610000 0.5622400619015379\n",
      "611000 0.5622151191241913\n",
      "612000 0.5622245715284779\n",
      "613000 0.5622372557304148\n",
      "614000 0.5623264457224011\n",
      "615000 0.5623486791078388\n",
      "616000 0.5622474638839872\n",
      "617000 0.5622389590940695\n",
      "618000 0.5623291871696\n",
      "619000 0.5623512724535178\n",
      "620000 0.562276512457238\n",
      "621000 0.5623598029632803\n",
      "622000 0.5622949159245725\n",
      "623000 0.5623442016947003\n",
      "624000 0.5624750601361216\n",
      "625000 0.5623799001921597\n",
      "626000 0.5623649163499739\n",
      "627000 0.5622574764633549\n",
      "628000 0.5623319071147976\n",
      "629000 0.5624347179098285\n",
      "630000 0.5624197421908854\n",
      "631000 0.562594987963569\n",
      "632000 0.5624706290021693\n",
      "633000 0.5625156990273317\n",
      "634000 0.5625464313147771\n",
      "635000 0.5624794291662533\n",
      "636000 0.5624204993388375\n",
      "637000 0.562419839215323\n",
      "638000 0.5623000590908165\n",
      "639000 0.5624435642510731\n",
      "640000 0.5624569336610411\n",
      "641000 0.5623594970990685\n",
      "642000 0.5622748251170948\n",
      "643000 0.5620877728028417\n",
      "644000 0.5622429157718699\n",
      "645000 0.5622704460923317\n",
      "646000 0.5622003681108853\n",
      "647000 0.562227879091377\n",
      "648000 0.5621673423343483\n",
      "649000 0.5620992879826071\n",
      "650000 0.5620175968960047\n",
      "651000 0.5620052810978785\n",
      "652000 0.5620052729980476\n",
      "653000 0.5620282357913694\n",
      "654000 0.5620908836530831\n",
      "655000 0.5620189892839859\n",
      "656000 0.5619747530872666\n",
      "657000 0.5619702253116814\n",
      "658000 0.5618958025899656\n",
      "659000 0.5618686466333132\n"
     ]
    }
   ],
   "source": [
    "cluster_center = kmeans_model3.centroids.cpu()\n",
    "\n",
    "true_cluster = 0\n",
    "false_cluster = 0\n",
    "\n",
    "for test_sample in (range(len(trainx))):\n",
    "    sims = utility_functions.cos_sim(torch.Tensor(trainx[test_sample: test_sample+1]), torch.Tensor(cluster_center))\n",
    "    if trainl[test_sample] in new_result[np.argmax(sims).item()]:\n",
    "        true_cluster += 1\n",
    "    else:\n",
    "        false_cluster += 1\n",
    "        \n",
    "    if test_sample > 0 and test_sample % 1000 == 0:\n",
    "        print(test_sample, true_cluster / (true_cluster + false_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.6023976023976024\n",
      "2000 0.5942028985507246\n",
      "3000 0.5921359546817727\n",
      "4000 0.5828542864283929\n",
      "5000 0.5822835432913417\n",
      "6000 0.5854024329278453\n",
      "7000 0.5864876446221968\n",
      "8000 0.5888013998250219\n",
      "9000 0.5968225752694145\n",
      "10000 0.5988401159884011\n",
      "11000 0.6007635669484592\n",
      "12000 0.6007832680609949\n",
      "13000 0.6019536958695485\n",
      "14000 0.6028855081779873\n",
      "15000 0.6002266515565629\n",
      "16000 0.6026498343853509\n",
      "17000 0.6017881301099935\n",
      "18000 0.6028553969223932\n",
      "19000 0.6037050681543077\n",
      "20000 0.6039698015099245\n",
      "21000 0.604209323365554\n",
      "22000 0.6041998090995864\n",
      "23000 0.603365071083866\n",
      "24000 0.6029748760468314\n",
      "25000 0.6032958681652734\n",
      "26000 0.6042459905388254\n",
      "27000 0.6038294877967483\n",
      "28000 0.6032641691368166\n",
      "29000 0.6027033550567222\n",
      "30000 0.6020799306689777\n",
      "31000 0.6010773846004968\n",
      "32000 0.6008562232430237\n",
      "33000 0.6005878609739099\n",
      "34000 0.600864680450575\n",
      "35000 0.6006685523270764\n",
      "36000 0.6004555429015861\n",
      "37000 0.6011999675684441\n",
      "38000 0.6005368279782111\n",
      "39000 0.599292325837799\n",
      "40000 0.5996100097497562\n",
      "41000 0.5994244042828224\n",
      "42000 0.5995571534011095\n",
      "43000 0.6001488337480524\n",
      "44000 0.600281811777005\n",
      "45000 0.5990533543699029\n",
      "46000 0.5992913197539184\n",
      "47000 0.5997106444543733\n",
      "48000 0.5996958396700068\n",
      "49000 0.5996816391502214\n",
      "50000 0.5991480170396593\n",
      "51000 0.5995764788925707\n",
      "52000 0.5994115497778889\n",
      "53000 0.6003094281239977\n",
      "54000 0.6001185163237718\n",
      "55000 0.6000618170578716\n",
      "56000 0.599810717665756\n",
      "57000 0.599182470482974\n",
      "58000 0.5984034758021414\n",
      "59000 0.5981085066354808\n",
      "60000 0.5981566973883768\n",
      "61000 0.5982852740119015\n",
      "62000 0.5981032563990903\n",
      "63000 0.5976571800447612\n",
      "64000 0.5972875423821503\n",
      "65000 0.5973600406147598\n",
      "66000 0.5970515598248511\n",
      "67000 0.5971701914896793\n",
      "68000 0.5972706283731122\n",
      "69000 0.5972377211924464\n",
      "70000 0.5972200397137184\n",
      "71000 0.5978929874227124\n",
      "72000 0.5974222580241941\n",
      "73000 0.5978137285790606\n",
      "74000 0.5976946257483007\n",
      "75000 0.597138704817269\n",
      "76000 0.597426349653294\n",
      "77000 0.5972649705847975\n",
      "78000 0.5971077293880848\n",
      "79000 0.5972329464183997\n",
      "80000 0.5971550355620555\n",
      "81000 0.5971901581461957\n",
      "82000 0.5974805185302619\n",
      "83000 0.5974144889820605\n",
      "84000 0.5976476470518208\n",
      "85000 0.5975812049270008\n",
      "86000 0.5980860687666423\n",
      "87000 0.5983034677762324\n",
      "88000 0.5982432017817979\n",
      "89000 0.5980606959472365\n",
      "90000 0.5976711369873668\n",
      "91000 0.5974989285832024\n",
      "92000 0.5973304637993065\n",
      "93000 0.5970150858592919\n",
      "94000 0.5967383325709301\n",
      "95000 0.5968884538057494\n",
      "96000 0.5967542004770784\n",
      "97000 0.5968392078432181\n",
      "98000 0.5970143161804471\n",
      "99000 0.597165685195099\n"
     ]
    }
   ],
   "source": [
    "cluster_center = kmeans_model2.centroids.cpu()\n",
    "\n",
    "true_cluster = 0\n",
    "false_cluster = 0\n",
    "\n",
    "for test_sample in (range(len(testx))):\n",
    "    sims = utility_functions.cos_sim(torch.Tensor(testx[test_sample: test_sample+1]), torch.Tensor(cluster_center))\n",
    "    if testl[test_sample] in cosine_result[np.argmax(sims).item()]:\n",
    "        true_cluster += 1\n",
    "    else:\n",
    "        false_cluster += 1\n",
    "        \n",
    "    if test_sample > 0 and test_sample % 1000 == 0:\n",
    "        print(test_sample, true_cluster / (true_cluster + false_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.6843156843156843\n",
      "2000 0.6846576711644178\n",
      "3000 0.7014328557147618\n",
      "4000 0.7088227943014247\n",
      "5000 0.6940611877624475\n",
      "6000 0.6973837693717714\n",
      "7000 0.6981859734323668\n",
      "8000 0.7045369328833896\n",
      "9000 0.7144761693145206\n",
      "10000 0.7163283671632836\n",
      "11000 0.7132987910189983\n",
      "12000 0.7076910257478544\n",
      "13000 0.7098684716560265\n",
      "14000 0.71173487608028\n",
      "15000 0.7092193853743084\n",
      "16000 0.7070808074495344\n",
      "17000 0.7079583553908594\n",
      "18000 0.7078495639131159\n",
      "19000 0.704068206936477\n",
      "20000 0.7021148942552873\n",
      "21000 0.7018237226798724\n",
      "22000 0.6960592700331804\n",
      "23000 0.6895787139689579\n",
      "24000 0.6864297320944961\n",
      "25000 0.6819327226910924\n",
      "26000 0.6832044921349179\n",
      "27000 0.6887152327691567\n",
      "28000 0.6894753758794329\n",
      "29000 0.6898382814385711\n",
      "30000 0.6896103463217893\n",
      "31000 0.6877842650237089\n",
      "32000 0.684072372738352\n",
      "33000 0.6834944395624375\n",
      "34000 0.6816270109702656\n",
      "35000 0.6809234021885089\n",
      "36000 0.6816755090136385\n",
      "37000 0.681981568065728\n",
      "38000 0.6815873266492987\n",
      "39000 0.6812645829594113\n",
      "40000 0.6820829479263019\n",
      "41000 0.6853247481768737\n",
      "42000 0.6839599057165305\n",
      "43000 0.681705076626125\n",
      "44000 0.6808708892979705\n",
      "45000 0.6817404057687607\n",
      "46000 0.6835068802852112\n",
      "47000 0.6837939618305994\n",
      "48000 0.6817774629695215\n",
      "49000 0.6808432480969776\n",
      "50000 0.6816463670726586\n",
      "51000 0.6817121232917002\n",
      "52000 0.6814484336839676\n",
      "53000 0.6816663836531386\n",
      "54000 0.682746615803411\n",
      "55000 0.6852057235322994\n",
      "56000 0.6862556025785254\n",
      "57000 0.6869353169242645\n",
      "58000 0.6873157359355874\n",
      "59000 0.6861578617311571\n",
      "60000 0.6862718954684088\n",
      "61000 0.6847264798937722\n",
      "62000 0.6866502153191077\n",
      "63000 0.686909731591562\n",
      "64000 0.6862705270230153\n",
      "65000 0.6849125398070799\n",
      "66000 0.6838229723792064\n",
      "67000 0.6837808390919539\n",
      "68000 0.6837105336686226\n",
      "69000 0.6821495340647237\n",
      "70000 0.681504549935001\n",
      "71000 0.68169462401938\n",
      "72000 0.6825599644449383\n",
      "73000 0.6830454377337297\n",
      "74000 0.6828826637477872\n",
      "75000 0.6829108945214064\n",
      "76000 0.6831357482138393\n",
      "77000 0.6831599589615719\n",
      "78000 0.6828758605658902\n",
      "79000 0.6824470576321818\n",
      "80000 0.6827664654191823\n",
      "81000 0.6819792348242615\n",
      "82000 0.6799429275252741\n",
      "83000 0.6794978373754533\n",
      "84000 0.6790157260032619\n",
      "85000 0.6785096645921813\n",
      "86000 0.6784572272415437\n",
      "87000 0.6781991011597568\n",
      "88000 0.678276383222918\n",
      "89000 0.6781721553690409\n",
      "90000 0.6778480239108454\n",
      "91000 0.6775200272524478\n",
      "92000 0.6780687166443843\n",
      "93000 0.6781647509166568\n",
      "94000 0.6769396070254572\n",
      "95000 0.6762349870001368\n",
      "96000 0.6766908678034603\n",
      "97000 0.6764260162266368\n",
      "98000 0.6758604504035673\n",
      "99000 0.6756901445439945\n",
      "100000 0.6758632413675864\n",
      "101000 0.6755972713141454\n",
      "102000 0.6764345447593652\n",
      "103000 0.6766827506529063\n",
      "104000 0.6771954115825809\n",
      "105000 0.6770316473176446\n",
      "106000 0.6767860680559618\n",
      "107000 0.6772179699255148\n",
      "108000 0.6770215090601013\n",
      "109000 0.6772414931973101\n",
      "110000 0.6764211234443324\n",
      "111000 0.6771380438014072\n",
      "112000 0.6773868090463478\n",
      "113000 0.676932062548119\n",
      "114000 0.6766782747519758\n",
      "115000 0.6764202050416953\n",
      "116000 0.6760458961560676\n",
      "117000 0.6759942222716046\n",
      "118000 0.6756129185345887\n",
      "119000 0.6760615457012966\n",
      "120000 0.6758777010191582\n",
      "121000 0.6752506177634896\n",
      "122000 0.6749780739502135\n",
      "123000 0.6752871927870505\n",
      "124000 0.6752526189304925\n",
      "125000 0.6747466020271837\n",
      "126000 0.6756533678304141\n",
      "127000 0.6754986181211171\n",
      "128000 0.6751978500167968\n",
      "129000 0.6754676320338602\n",
      "130000 0.6751101914600657\n",
      "131000 0.6752162197235135\n",
      "132000 0.6750782191044007\n",
      "133000 0.6746941752317652\n",
      "134000 0.6744800411937224\n",
      "135000 0.6752320353182569\n",
      "136000 0.6753112109469783\n",
      "137000 0.6756593017569215\n",
      "138000 0.6752632227302701\n",
      "139000 0.6755059316120028\n",
      "140000 0.6764737394732895\n",
      "141000 0.6767327891291551\n",
      "142000 0.6773895958479166\n",
      "143000 0.6775896672051245\n",
      "144000 0.6771064089832709\n",
      "145000 0.677464293349701\n",
      "146000 0.6776802898610284\n",
      "147000 0.6781382439575241\n",
      "148000 0.6782319038384875\n",
      "149000 0.6780491406097946\n",
      "150000 0.6776688155412297\n",
      "151000 0.6765981682240515\n",
      "152000 0.6768968625206413\n",
      "153000 0.6777994915065915\n",
      "154000 0.6781579340393894\n",
      "155000 0.6772407919948904\n",
      "156000 0.677239248466356\n",
      "157000 0.6768109757262692\n",
      "158000 0.6771666002113911\n",
      "159000 0.6779391324582864\n",
      "160000 0.6778645133467915\n",
      "161000 0.677281507568276\n",
      "162000 0.677551373139672\n",
      "163000 0.6769099576076221\n",
      "164000 0.676825141310114\n",
      "165000 0.6770140787025534\n",
      "166000 0.6767007427666099\n",
      "167000 0.6764390632391423\n",
      "168000 0.676484068547211\n",
      "169000 0.67664688374625\n",
      "170000 0.6761666107846425\n",
      "171000 0.6763118344337168\n",
      "172000 0.6754611891791327\n",
      "173000 0.675695516210889\n",
      "174000 0.6741742863546761\n",
      "175000 0.6733961520219884\n",
      "176000 0.6731666297350584\n",
      "177000 0.6731939367574195\n",
      "178000 0.6730636344739637\n",
      "179000 0.6730632789760951\n",
      "180000 0.6721962655763023\n",
      "181000 0.672051535626875\n",
      "182000 0.671386420953731\n",
      "183000 0.6712804848060939\n",
      "184000 0.6706050510595051\n",
      "185000 0.6700071891503289\n",
      "186000 0.670603921484293\n",
      "187000 0.6711568387334828\n",
      "188000 0.6710123882319775\n",
      "189000 0.6705414257067422\n",
      "190000 0.670306998384219\n",
      "191000 0.6699546075674997\n",
      "192000 0.6700173436596685\n",
      "193000 0.6699239900311398\n",
      "194000 0.6700841748238411\n",
      "195000 0.6700222050143333\n",
      "196000 0.6703333146259458\n",
      "197000 0.6703976121948619\n",
      "198000 0.6700976257695668\n",
      "199000 0.6701775367962975\n",
      "200000 0.6702416487917561\n",
      "201000 0.669912089989602\n",
      "202000 0.6701204449482924\n",
      "203000 0.6702183733085059\n",
      "204000 0.6695310317106289\n",
      "205000 0.6688747859766537\n",
      "206000 0.6688608307726661\n",
      "207000 0.6693880705890309\n",
      "208000 0.6694150508891784\n",
      "209000 0.6695518203262185\n",
      "210000 0.6690158618292293\n",
      "211000 0.6690205259690712\n",
      "212000 0.6690817496143886\n",
      "213000 0.6691611776470533\n",
      "214000 0.6699127574170214\n",
      "215000 0.670136417970149\n",
      "216000 0.6697931954018731\n",
      "217000 0.6699692628144571\n",
      "218000 0.6701574763418516\n",
      "219000 0.6702754781941634\n",
      "220000 0.6710151317494011\n",
      "221000 0.6708023945593007\n",
      "222000 0.6707672487961766\n",
      "223000 0.6703467697454272\n",
      "224000 0.6700461158655542\n",
      "225000 0.6700325776329883\n",
      "226000 0.6707536692315521\n",
      "227000 0.6708340491892106\n",
      "228000 0.6707514440726137\n",
      "229000 0.6702721822175449\n",
      "230000 0.670375346194147\n",
      "231000 0.6705728546629668\n",
      "232000 0.6702686626350749\n",
      "233000 0.6706752331535057\n",
      "234000 0.670544142973748\n",
      "235000 0.6708311879523917\n",
      "236000 0.6705734297736027\n",
      "237000 0.6707186889506795\n",
      "238000 0.671077012281461\n",
      "239000 0.6709637198170719\n",
      "240000 0.6711180370081792\n",
      "241000 0.6710221119414442\n",
      "242000 0.6706005347085343\n",
      "243000 0.6703429203995045\n",
      "244000 0.6700382375482068\n",
      "245000 0.6695850221019506\n",
      "246000 0.6691517514156446\n",
      "247000 0.6689608544094963\n",
      "248000 0.6690093991556486\n",
      "249000 0.6689732169750322\n",
      "250000 0.6689493242027031\n",
      "251000 0.6694555001772902\n",
      "252000 0.6693941690707577\n",
      "253000 0.6690289761700546\n",
      "254000 0.6691587828394376\n",
      "255000 0.6689581609483884\n",
      "256000 0.6692864480998121\n",
      "257000 0.6699001171201668\n",
      "258000 0.6701175576838849\n",
      "259000 0.6701557136845031\n",
      "260000 0.6703897292702721\n",
      "261000 0.6697790429921724\n",
      "262000 0.6699554581852741\n",
      "263000 0.669845361804708\n",
      "264000 0.6700959466062628\n",
      "265000 0.6703936966275599\n",
      "266000 0.6705200356389638\n",
      "267000 0.6703982382088457\n",
      "268000 0.6703706329453994\n",
      "269000 0.670473343965264\n",
      "270000 0.6705197388157822\n",
      "271000 0.6704292604086332\n",
      "272000 0.6702512123117195\n",
      "273000 0.6698913190794173\n",
      "274000 0.6698004751807475\n",
      "275000 0.6703539259857237\n",
      "276000 0.6703164118970584\n",
      "277000 0.6704055220017257\n",
      "278000 0.6701162945457031\n",
      "279000 0.670338099146598\n",
      "280000 0.6701833207738543\n",
      "281000 0.6699513524862901\n",
      "282000 0.6699586171680242\n",
      "283000 0.6697255486729728\n",
      "284000 0.669659613874599\n",
      "285000 0.6694853702267711\n",
      "286000 0.6695955608546823\n",
      "287000 0.6694994094097233\n",
      "288000 0.6699247572057041\n",
      "289000 0.6700184428427584\n",
      "290000 0.670394240019862\n",
      "291000 0.6706712348067533\n",
      "292000 0.6710216745833062\n",
      "293000 0.6708543656847588\n",
      "294000 0.671194995935388\n",
      "295000 0.6706994213578937\n",
      "296000 0.6705416535754947\n",
      "297000 0.670358012262585\n",
      "298000 0.6705212398616112\n",
      "299000 0.6708338768097766\n",
      "300000 0.6712044293185689\n",
      "301000 0.6715459417078349\n",
      "302000 0.6712096979811325\n",
      "303000 0.6713344180382242\n",
      "304000 0.6714945016628234\n",
      "305000 0.6715518965511589\n",
      "306000 0.6716742755742628\n",
      "307000 0.6716297341051006\n",
      "308000 0.6717543124859984\n",
      "309000 0.6718360134756846\n",
      "310000 0.6716365431079254\n",
      "311000 0.6718917302516713\n",
      "312000 0.6717927186130813\n",
      "313000 0.6719435401164853\n",
      "314000 0.6721984961831332\n",
      "315000 0.6719153272529294\n",
      "316000 0.6717447096686403\n",
      "317000 0.6718843158223475\n",
      "318000 0.6718217867239411\n",
      "319000 0.6715182711025984\n",
      "320000 0.6719291502214055\n",
      "321000 0.6716801505291261\n",
      "322000 0.6716097154977779\n",
      "323000 0.6717688180531949\n",
      "324000 0.6717448402937028\n",
      "325000 0.6718717788560651\n",
      "326000 0.6719335216763139\n",
      "327000 0.6721477915969676\n",
      "328000 0.6719796585986019\n",
      "329000 0.6719432463731113\n",
      "330000 0.6715373589777001\n",
      "331000 0.6716293908477617\n",
      "332000 0.6716274950979063\n",
      "333000 0.6716886736075868\n",
      "334000 0.6717704438010664\n",
      "335000 0.6718338154214465\n",
      "336000 0.6717003818441016\n",
      "337000 0.6717101729668458\n",
      "338000 0.6718412075703918\n",
      "339000 0.6716204376978239\n",
      "340000 0.6718215534660192\n",
      "341000 0.6720420174720895\n",
      "342000 0.6721763971450376\n",
      "343000 0.6720884195672899\n",
      "344000 0.6720649067880616\n",
      "345000 0.6715371839501915\n",
      "346000 0.6714286952927881\n",
      "347000 0.6715542606505457\n",
      "348000 0.6718515176680527\n",
      "349000 0.6717516568720433\n",
      "350000 0.6718837946177296\n",
      "351000 0.6720066324597366\n",
      "352000 0.6716713872971952\n",
      "353000 0.671584499760624\n",
      "354000 0.6721054460298135\n",
      "355000 0.672119233466948\n",
      "356000 0.6720177752309685\n",
      "357000 0.6719476976254969\n",
      "358000 0.6722299658380843\n",
      "359000 0.6722627513572386\n",
      "360000 0.6720786886703093\n",
      "361000 0.6720563100933239\n",
      "362000 0.671851735216201\n",
      "363000 0.6715849267632872\n",
      "364000 0.6717563962736366\n",
      "365000 0.6719378851016846\n",
      "366000 0.6717304051081827\n",
      "367000 0.6718564799550955\n",
      "368000 0.6719438262396026\n",
      "369000 0.6717217568516074\n",
      "370000 0.6717495358120654\n",
      "371000 0.6718768952105251\n",
      "372000 0.6719014196198397\n",
      "373000 0.6719284934892936\n",
      "374000 0.6717441931973444\n",
      "375000 0.671795541878555\n",
      "376000 0.6719210853162625\n",
      "377000 0.6718682443813146\n",
      "378000 0.6719479578096355\n",
      "379000 0.6720298891031949\n",
      "380000 0.6719113897068691\n",
      "381000 0.6719562415846677\n",
      "382000 0.6718621155442002\n",
      "383000 0.6718807522695763\n",
      "384000 0.6716649175392773\n",
      "385000 0.6713463081914073\n",
      "386000 0.671130385672576\n",
      "387000 0.6711558884860762\n",
      "388000 0.6709828067453434\n",
      "389000 0.6710471181308018\n",
      "390000 0.6709828949156541\n",
      "391000 0.6710289743504493\n",
      "392000 0.6710569615893837\n",
      "393000 0.6710033816707845\n",
      "394000 0.6709348453430322\n",
      "395000 0.6710286809400483\n",
      "396000 0.6708366898063389\n",
      "397000 0.6710184609106778\n",
      "398000 0.6708751988060332\n",
      "399000 0.6709105992215558\n",
      "400000 0.6710983222541944\n",
      "401000 0.6709634140563241\n",
      "402000 0.6709510672859023\n",
      "403000 0.6709933722248829\n",
      "404000 0.6710948735275408\n",
      "405000 0.671072417105143\n",
      "406000 0.6710008103428317\n",
      "407000 0.6710106363375029\n",
      "408000 0.6714615895549275\n",
      "409000 0.6713699966503749\n",
      "410000 0.6709373879575904\n",
      "411000 0.6707501928219153\n",
      "412000 0.6705250715410885\n",
      "413000 0.6704705315483498\n",
      "414000 0.6703993468614811\n",
      "415000 0.6704104327459451\n",
      "416000 0.6700104086288254\n",
      "417000 0.6700103836681447\n",
      "418000 0.6700103588268927\n",
      "419000 0.6701153457867642\n",
      "420000 0.6702412613303301\n",
      "421000 0.6703974574882245\n",
      "422000 0.6702827718417729\n",
      "423000 0.6699913238975794\n",
      "424000 0.6699253067799369\n",
      "425000 0.6700548939884847\n",
      "426000 0.6703505390832416\n",
      "427000 0.6702326224060365\n",
      "428000 0.6700311447870448\n",
      "429000 0.6697933105051037\n",
      "430000 0.6699263490084907\n",
      "431000 0.6699891647583184\n",
      "432000 0.6697299311807149\n",
      "433000 0.6696982224059529\n",
      "434000 0.6696413141905203\n",
      "435000 0.6698122533051648\n",
      "436000 0.6696590145435446\n",
      "437000 0.6695980100732035\n",
      "438000 0.6694436770692305\n",
      "439000 0.6695816182651064\n",
      "440000 0.669821204951807\n",
      "441000 0.6700211564146114\n",
      "442000 0.6700708821925743\n",
      "443000 0.6702490513565432\n",
      "444000 0.6702214634651724\n",
      "445000 0.6701850108202004\n",
      "446000 0.6702428918320811\n",
      "447000 0.6704436902825721\n",
      "448000 0.6704092178365674\n",
      "449000 0.6707869247507244\n",
      "450000 0.6707785093810903\n",
      "451000 0.6707546103001989\n",
      "452000 0.6705095785186316\n",
      "453000 0.6704731336133916\n",
      "454000 0.6704390518963614\n",
      "455000 0.6703216036887831\n",
      "456000 0.6702002846484986\n",
      "457000 0.670005098457115\n",
      "458000 0.6700203711345608\n",
      "459000 0.6700246840420827\n",
      "460000 0.6701268040721651\n",
      "461000 0.6699963774482051\n",
      "462000 0.6700872941833459\n",
      "463000 0.6700309502571269\n",
      "464000 0.670198986640115\n",
      "465000 0.6702587736370459\n",
      "466000 0.6702260295578765\n",
      "467000 0.6700242611900189\n",
      "468000 0.6701695081848116\n",
      "469000 0.6700710659465545\n",
      "470000 0.6700581488124493\n",
      "471000 0.6699242676767141\n",
      "472000 0.6699265467657908\n",
      "473000 0.6697469984207222\n",
      "474000 0.6699078693926807\n",
      "475000 0.6698870107641879\n",
      "476000 0.6699145590030273\n",
      "477000 0.6700342347290676\n",
      "478000 0.6700383472001105\n",
      "479000 0.6702741747929545\n",
      "480000 0.6702069370688811\n",
      "481000 0.6703582736834227\n",
      "482000 0.6704508911807237\n",
      "483000 0.6704333945478373\n",
      "484000 0.6705543996809924\n",
      "485000 0.6703924321805522\n",
      "486000 0.6704451225409002\n",
      "487000 0.6703785002494861\n",
      "488000 0.6701379710287478\n",
      "489000 0.6700845192545618\n",
      "490000 0.6701047548882554\n",
      "491000 0.6698153364249767\n",
      "492000 0.6698827847910879\n",
      "493000 0.6701953951411863\n",
      "494000 0.6700634209242491\n",
      "495000 0.6702087470530362\n",
      "496000 0.6702083261928907\n",
      "497000 0.6702662570095432\n",
      "498000 0.6701833932060377\n",
      "499000 0.6701770136733193\n",
      "500000 0.6701306597386805\n",
      "501000 0.6701663270133194\n",
      "502000 0.6703432861687526\n",
      "503000 0.6703008542726555\n",
      "504000 0.6702010511883905\n",
      "505000 0.6702263955912958\n",
      "506000 0.670378121782368\n",
      "507000 0.6700992700211637\n",
      "508000 0.6699671851039664\n",
      "509000 0.6700811982687657\n",
      "510000 0.6700653528130337\n",
      "511000 0.6700436985446212\n",
      "512000 0.6702779877383052\n",
      "513000 0.670277445852932\n",
      "514000 0.6702457777319499\n",
      "515000 0.67031520327145\n",
      "516000 0.6703843597202331\n",
      "517000 0.6703178523832642\n",
      "518000 0.67030179478418\n",
      "519000 0.6702010208072817\n",
      "520000 0.670194865009875\n",
      "521000 0.6701887328431232\n",
      "522000 0.6700178735289779\n",
      "523000 0.6698361953418827\n",
      "524000 0.6698727674183828\n",
      "525000 0.6700044380867846\n",
      "526000 0.6700139353347238\n",
      "527000 0.6698317460498178\n",
      "528000 0.6698907767220138\n",
      "529000 0.6698475050141682\n",
      "530000 0.6698534531066922\n",
      "531000 0.6699347082208885\n",
      "532000 0.6698182898152447\n",
      "533000 0.6699349532177238\n",
      "534000 0.670053052335108\n",
      "535000 0.6703464105674569\n",
      "536000 0.6702021078318884\n",
      "537000 0.6700546181478246\n",
      "538000 0.6701047024076163\n",
      "539000 0.6701304821326862\n",
      "540000 0.6702098699817223\n",
      "541000 0.6703240844286794\n",
      "542000 0.6701168448028694\n",
      "543000 0.6703099994290986\n",
      "544000 0.6704289146527305\n",
      "545000 0.6705143660286862\n",
      "546000 0.6705372334482904\n",
      "547000 0.6704887193990504\n",
      "548000 0.6706064404991962\n",
      "549000 0.6705033324165165\n",
      "550000 0.6705224172319687\n",
      "551000 0.6705686559552524\n",
      "552000 0.6705187128284188\n",
      "553000 0.6706099988969279\n",
      "554000 0.6707551069402402\n",
      "555000 0.6707933859578631\n",
      "556000 0.670682246974376\n",
      "557000 0.670720519352748\n",
      "558000 0.6707479018854805\n",
      "559000 0.6707948644099027\n",
      "560000 0.6709488018771395\n",
      "561000 0.6710130641478358\n",
      "562000 0.6708457814132003\n",
      "563000 0.6710467654586759\n",
      "564000 0.6710005833322991\n",
      "565000 0.6710802281765873\n",
      "566000 0.671046517585658\n",
      "567000 0.6709600159435345\n",
      "568000 0.6709354384939463\n",
      "569000 0.6709373094247637\n",
      "570000 0.6708444371150226\n",
      "571000 0.6709357776956608\n",
      "572000 0.6712051202707687\n",
      "573000 0.6712379210507486\n",
      "574000 0.67090301236409\n",
      "575000 0.6709623113698933\n",
      "576000 0.6709345990718766\n",
      "577000 0.6709208476241809\n",
      "578000 0.6708569708356906\n",
      "579000 0.6707691351137562\n",
      "580000 0.6707729814258941\n",
      "581000 0.6709644217479832\n",
      "582000 0.6711053761076012\n",
      "583000 0.6709645438000964\n",
      "584000 0.6709389196251376\n",
      "585000 0.670990305999477\n",
      "586000 0.6709476604988729\n",
      "587000 0.6708216851419333\n",
      "588000 0.6709937568133387\n",
      "589000 0.6710175364727734\n",
      "590000 0.6709242865690058\n",
      "591000 0.6708211999641287\n",
      "592000 0.6706931238291827\n",
      "593000 0.6704794764258407\n",
      "594000 0.6706874230851463\n",
      "595000 0.6707518138624977\n",
      "596000 0.6706482036103967\n",
      "597000 0.6704796139369951\n",
      "598000 0.670356738533882\n",
      "599000 0.6704930375742277\n",
      "600000 0.6704672158879735\n",
      "601000 0.6704714301640097\n",
      "602000 0.6704723081855346\n",
      "603000 0.6704930837593968\n",
      "604000 0.6705435918152454\n",
      "605000 0.670549304877182\n",
      "606000 0.6706556589840611\n",
      "607000 0.6707946115410024\n",
      "608000 0.6706469232780867\n",
      "609000 0.6708248426521467\n",
      "610000 0.6708038183543962\n",
      "611000 0.6706470202176429\n",
      "612000 0.6707603418948662\n",
      "613000 0.6708243542832719\n",
      "614000 0.6710347377284401\n",
      "615000 0.6710883396937566\n",
      "616000 0.6712083259605098\n",
      "617000 0.6711674697447816\n",
      "618000 0.6709002088993383\n",
      "619000 0.6709973004890137\n",
      "620000 0.6710795627748988\n",
      "621000 0.6711164716320908\n",
      "622000 0.6710310755127403\n",
      "623000 0.6708849584511101\n",
      "624000 0.6708066172970877\n",
      "625000 0.6708613266218774\n",
      "626000 0.67086953535218\n",
      "627000 0.6709989298262682\n",
      "628000 0.6709240908852055\n",
      "629000 0.670930570857598\n",
      "630000 0.6708703636978354\n",
      "631000 0.6708880017622793\n",
      "632000 0.6707378627565462\n",
      "633000 0.6707145802297311\n",
      "634000 0.6708790680140883\n",
      "635000 0.6708241404344245\n",
      "636000 0.6710288191370768\n",
      "637000 0.670984817920223\n",
      "638000 0.670984841716549\n",
      "639000 0.6710177292367304\n",
      "640000 0.6711130138859158\n",
      "641000 0.6710098736195419\n",
      "642000 0.6709724751207553\n",
      "643000 0.6711435907564685\n",
      "644000 0.6713871562311239\n",
      "645000 0.6714113621529268\n",
      "646000 0.6712574748336303\n",
      "647000 0.6712879887357207\n",
      "648000 0.671219643179563\n",
      "649000 0.6712254680655345\n",
      "650000 0.6712281981104644\n",
      "651000 0.6711448983949333\n",
      "652000 0.6709652285809378\n",
      "653000 0.6709055575718873\n",
      "654000 0.6708904114825512\n",
      "655000 0.6710356167395164\n",
      "656000 0.6711742817465217\n",
      "657000 0.6710872586190889\n",
      "658000 0.6709883419630062\n",
      "659000 0.6707926088124297\n"
     ]
    }
   ],
   "source": [
    "true_cluster = 0\n",
    "false_cluster = 0\n",
    "\n",
    "for test_sample in (range(len(trainx))):\n",
    "    sims = utility_functions.cos_sim(torch.Tensor(trainx[test_sample: test_sample+1]), torch.Tensor(cluster_center))\n",
    "    if trainl[test_sample] in cosine_result[np.argmax(sims).item()]:\n",
    "        true_cluster += 1\n",
    "    else:\n",
    "        false_cluster += 1\n",
    "        \n",
    "    if test_sample > 0 and test_sample % 1000 == 0:\n",
    "        print(test_sample, true_cluster / (true_cluster + false_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_mean approach\n",
    "cluster_center = []\n",
    "for u in unique_result:\n",
    "    cluster_center.append(traincenterx[unique_result[u]].mean(0))\n",
    "\n",
    "cluster_center = centers\n",
    "cluster_sim_rate = 0\n",
    "\n",
    "true_cluster = 0\n",
    "false_cluster = 0\n",
    "\n",
    "max_max_trues = 0\n",
    "max_max_falses = 0\n",
    "\n",
    "index = 0\n",
    "for test_sample in (range(len(testx))):\n",
    "    real_class = testl[test_sample]\n",
    "    if False: #sofmax_values[test_sample] > 0.4:\n",
    "        if softmax_classes[test_sample] == real_class:\n",
    "            max_max_trues += weights[test_sample]\n",
    "            true_cluster += 1\n",
    "        else:\n",
    "            max_max_falses += weights[test_sample]\n",
    "            false_cluster += 1\n",
    "    else:\n",
    "        sims = utility_functions.euc_sim(torch.Tensor(testx[test_sample: test_sample+1]), torch.Tensor(cluster_center))\n",
    "        # if test_sample // 5\n",
    "        if testl[test_sample] in unique_result[np.argmax(sims).item()]:\n",
    "            true_cluster += 1\n",
    "        else:\n",
    "            false_cluster += 1\n",
    "        \n",
    "    if test_sample > 0 and test_sample % 1000 == 0:\n",
    "        print(test_sample, true_cluster / (true_cluster + false_cluster))\n",
    "\n",
    "        # v = sims\n",
    "        # v_min, v_max = v.min(), v.max()\n",
    "        # new_min, new_max = 0, 0.9\n",
    "        # v_p = (v - v_min)/(v_max - v_min)*(new_max - new_min) + new_min\n",
    "        # sims = v_p\n",
    "\n",
    "        # print(sims)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.5834165834165834\n",
      "2000 0.5972013993003499\n",
      "3000 0.5924691769410196\n",
      "4000 0.5836040989752562\n",
      "5000 0.585882823435313\n",
      "6000 0.5890684885852357\n",
      "7000 0.5880588487358949\n",
      "8000 0.5909261342332208\n",
      "9000 0.5984890567714698\n",
      "10000 0.597040295970403\n",
      "11000 0.5996727570220889\n",
      "12000 0.6001999833347221\n",
      "13000 0.6025690331512961\n",
      "14000 0.6023855438897222\n",
      "15000 0.6025598293447103\n",
      "16000 0.6053371664270983\n",
      "17000 0.6056114346214928\n",
      "18000 0.6071329370590522\n",
      "19000 0.6075996000210515\n",
      "20000 0.607769611519424\n",
      "21000 0.6073044140755202\n",
      "22000 0.6063360756329258\n",
      "23000 0.6060606060606061\n",
      "24000 0.6063914003583184\n",
      "25000 0.6066157353705852\n",
      "26000 0.6070920349217338\n",
      "27000 0.6073478760045924\n",
      "28000 0.6069426091925288\n",
      "29000 0.6063584014344333\n",
      "30000 0.6051464951168294\n",
      "31000 0.6048191993806651\n",
      "32000 0.6046998531295897\n",
      "33000 0.6052846883427775\n",
      "34000 0.605688067998\n",
      "35000 0.606182680494843\n",
      "36000 0.6058720591094692\n",
      "37000 0.606334963919894\n",
      "38000 0.6059314228572932\n",
      "39000 0.6050870490500243\n",
      "40000 0.6047348816279593\n",
      "41000 0.6050828028584668\n",
      "42000 0.6044141806147473\n",
      "43000 0.6045208250970908\n",
      "44000 0.6045089884320811\n",
      "45000 0.6036976956067643\n",
      "46000 0.6047259842177344\n",
      "47000 0.6043275674985639\n",
      "48000 0.6048832315993417\n",
      "49000 0.6048243913389523\n",
      "50000 0.6047679046419072\n",
      "51000 0.6052038195329503\n",
      "52000 0.6057191207861388\n",
      "53000 0.6069696798173619\n",
      "54000 0.607136904872132\n",
      "55000 0.6071889602007237\n",
      "56000 0.6070605882037821\n",
      "57000 0.6066911106822688\n",
      "58000 0.6064205789555352\n",
      "59000 0.6061931153709259\n",
      "60000 0.6060565657239046\n",
      "61000 0.6055146636940378\n",
      "62000 0.6058289382429316\n",
      "63000 0.6056411802987254\n",
      "64000 0.6052874173841034\n",
      "65000 0.6055752988415563\n",
      "66000 0.60561203618127\n",
      "67000 0.605199922389218\n",
      "68000 0.6051381597329452\n",
      "69000 0.6049187692931987\n",
      "70000 0.6045913629805288\n",
      "71000 0.604583034041774\n",
      "72000 0.6041304981875252\n",
      "73000 0.6045670607251955\n",
      "74000 0.6044242645369657\n",
      "75000 0.6043386088185491\n",
      "76000 0.604544677043723\n",
      "77000 0.6050440903364891\n",
      "78000 0.6053127524006102\n",
      "79000 0.6054480323033886\n",
      "80000 0.6054049324383445\n",
      "81000 0.6051900593819829\n",
      "82000 0.6052365215058353\n",
      "83000 0.6057155937880266\n",
      "84000 0.6059213580790704\n",
      "85000 0.6059340478347314\n",
      "86000 0.6060627202009279\n",
      "87000 0.6064183170308387\n",
      "88000 0.6062544743809729\n",
      "89000 0.6059931910877406\n",
      "90000 0.6059488227908579\n",
      "91000 0.6061252074153032\n",
      "92000 0.6061021075857871\n",
      "93000 0.6057031644821024\n",
      "94000 0.6053871767321625\n",
      "95000 0.6050567888759065\n",
      "96000 0.6051811960292081\n",
      "97000 0.6052514922526572\n",
      "98000 0.60532035387394\n",
      "99000 0.6052565125604792\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([   17,    33,    36, ..., 19967, 19986, 19998], dtype=int64),\n",
       " 1: array([   13,    32,    44, ..., 19978, 19995, 19999], dtype=int64),\n",
       " 2: array([    7,    11,    14, ..., 19982, 19985, 19987], dtype=int64),\n",
       " 3: array([    6,    19,    20, ..., 19990, 19991, 19996], dtype=int64),\n",
       " 4: array([   16,    25,    45, ..., 19988, 19992, 19997], dtype=int64),\n",
       " 5: array([    8,    22,    30, ..., 19913, 19970, 19976], dtype=int64),\n",
       " 6: array([    0,     4,    12, ..., 19972, 19980, 19984], dtype=int64),\n",
       " 7: array([    1,     9,    10, ..., 19975, 19989, 19994], dtype=int64),\n",
       " 8: array([    2,    23,    27, ..., 19969, 19977, 19993], dtype=int64),\n",
       " 9: array([    3,     5,    15, ..., 19973, 19974, 19979], dtype=int64)}"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60539"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_cluster / len(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15697, 2369)"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = utility_functions.cos_sim(torch.Tensor(testx[0]), torch.Tensor(cluster_center))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([   17,    33,    36, ..., 19967, 19986, 19998], dtype=int64),\n",
       " 1: array([   13,    32,    44, ..., 19978, 19995, 19999], dtype=int64),\n",
       " 2: array([    7,    11,    14, ..., 19982, 19985, 19987], dtype=int64),\n",
       " 3: array([    6,    19,    20, ..., 19990, 19991, 19996], dtype=int64),\n",
       " 4: array([   16,    25,    45, ..., 19988, 19992, 19997], dtype=int64),\n",
       " 5: array([    8,    22,    30, ..., 19913, 19970, 19976], dtype=int64),\n",
       " 6: array([    0,     4,    12, ..., 19972, 19980, 19984], dtype=int64),\n",
       " 7: array([    1,     9,    10, ..., 19975, 19989, 19994], dtype=int64),\n",
       " 8: array([    2,    23,    27, ..., 19969, 19977, 19993], dtype=int64),\n",
       " 9: array([    3,     5,    15, ..., 19973, 19974, 19979], dtype=int64)}"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEHCAYAAACTC1DDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAuiElEQVR4nO3dd5hU5fn/8fe9u7DUpYNUAQVp9hVQY2JFogaMsaegIfGXojExJpqq0eQbjUmMRmNCLBhjRIOJookSLNiRKkhn6Yv0upRdtty/P86DjitlB2b2zM5+Xtc1157zzDlzPjO7cM9pz2PujoiISKrlxB1ARESykwqMiIikhQqMiIikhQqMiIikhQqMiIikRV7cATJJ27ZtvXv37nHHEBGpU6ZNm7bB3dtVb1eBSdC9e3emTp0adwwRkTrFzJbvrV2HyEREJC1UYEREJC1UYEREJC1UYEREJC1UYEREJC3SWmDM7GEzW2dmsxPaWpvZBDNbFH62Cu1mZveaWZGZzTKzExLWGRGWX2RmIxLaTzSz98M695qZ7W8bIiJSe9K9BzMaGFqt7WbgZXfvBbwc5gE+C/QKj2uAByAqFsAtwCBgIHBLQsF4APh6wnpDD7ANERGpJWm9D8bdXzez7tWahwOnh+lHgYnATaH9bx6NHzDJzFqaWcew7AR33wRgZhOAoWY2EShw90mh/W/AhcAL+9mGiEgsyioq2V5awa7ySkrLqyiriH6WV1ZRWeUfPirCzyr/6Kc7OE5VFdE8QGjbM+KKw4fL8eH0PuxlmJZLT+pKfl5uSt9zHDdadnD31WF6DdAhTHcGViYsVxza9tdevJf2/W3jE8zsGqI9Jrp165bsexGRes7dWbVlF/NXl1C0fjtrtpayvqQsemwvY9uuckrKKthdURV31P0adlznrCgwH3J3N7O0jnh2oG24+yhgFEBhYaFGXxOR/aqorGLGyi1MXLCOyUs3MX9NCSWlFR8+3yw/j/bN82nXPJ/+nQpo0bgBzRs1oHmjPJrl59G4YS75eTk0ahD9bJibQ26OkZdr5ObkkGtGTg7k5hi5ZpgZOQY5ZuSYEZ1pBjMwMyxMAxjR87YnjEVte2PVmpvnp74cxFFg1ppZR3dfHQ6BrQvtq4CuCct1CW2r+Ohw1572iaG9y16W3982RESSVlFZxf/mruU/s1bzxqL1bCutIDfHOKZLCy48rjNHHdacvh2b06tDcwoaNYg7bsaIo8CMA0YAd4Sfzya0X2tmY4hO6G8NBWI88H8JJ/aHAD9y901mts3MBgPvAl8B/niAbYiI1NjmHbsZM2Ulj72zjA+2ltK+eT5DBxzG6Ue159Qj29KisYrJ/qS1wJjZE0R7H23NrJjoarA7gKfMbCSwHLg0LP5f4DygCNgJXA0QCsntwJSw3G17TvgD3yK6Uq0x0cn9F0L7vrYhInJA20rL+f3/FjJmygpKy6s4uWcbbh3Wn7P6diA3Z++HnOSTzPdyNUF9VVhY6OpNWaT+cndemL2GW8fNYcP2Mi4+sQtf/VQP+hxWEHe0jGZm09y9sHq7uusXEQFWbdnFz5+Zzcvz19G/UwEPjijkmC4t445Vp6nAiEi999rC9Vz7+HQqqpyfnt+Xq07pTl6uetI6VCowIlKvPfbOMm59bi69OzRn1JdPpGvrJnFHyhoqMCJSL1VWObc/P5fRby/jrD7tufeK42mahntB6jN9miJS7+zaXcm3/zGdV+avY+SnevDj8/rq6rA0UIERkXpld0UV33p8GhMXrueXFw7gS4MPjztS1lKBEZF6o7LK+f4/Z/LqgvX8+qKjuWKg+h9MJ10mISL1grvzs2dn89zMD7j5s31UXGqBCoyI1Au/Gb+Af7y7gm+efgTf+MwRccepF1RgRCTrPTF5BQ9MXMwXB3Xjh+ceFXecekMFRkSy2vw127h13BxO69WW24YPwKr3Uy9powIjIllr5+4Kvv34dAoaN+D3lx6nS5Frma4iE5Gs9fNn57Bkww4eHzmIds3z445T72gPRkSy0tPTihk7rZjrzuzFKUe2jTtOvaQCIyJZZ/H67fzs2dkM6tGa68/qFXeceksFRkSySmWV8/2nZpKfl8M9lx+v8y4x0jkYEckqj7y1lPdWbuGey4/jsBaN4o5Tr2kPRkSyxtINO7hr/ALO7tuBYcd2ijtOvacCIyJZoarKuenpWTTMy+FXn9f9LplABUZEssLj7y5n8tJN/OyCfnQo0KGxTKACIyJ1XvHmndzxwnxO69WWS07sEnccCVRgRKTO+/mzcwD49UVH69BYBlGBEZE67dX563hl/jquP7sXXVo1iTuOJFCBEZE6a3dFFbc/P5eebZty1Sk94o4j1ajAiEid9ejby1iyYQc/u6AfDfP031mm0W9EROqk9SVl3PvyIs44qh1n9GkfdxzZCxUYEamT7ho/n13llfzsgn5xR5F9UIERkTpnVvEW/jmtmKtP7U7Pds3ijiP7oAIjInWKu3Pbc3Np07Qh16mn5IymAiMidcpL89YxdflmvndObwoaNYg7juyHCoyI1BmVVc5d4+fTo21TLi3sGnccOYDYCoyZfc/M5pjZbDN7wswamVkPM3vXzIrM7EkzaxiWzQ/zReH57gmv86PQvsDMzk1oHxraiszs5hjeooik2L9nrGLh2u3cOOQoGuTq+3Gmi+U3ZGadge8Ahe4+AMgFLgfuBO529yOBzcDIsMpIYHNovzssh5n1C+v1B4YCfzKzXDPLBe4HPgv0A64Iy4pIHVVWUcndExZydOcWfHbAYXHHkRqI8ytAHtDYzPKAJsBq4ExgbHj+UeDCMD08zBOeP8uiDoeGA2PcvczdlwJFwMDwKHL3Je6+GxgTlhWROurvk1awassubhrahxyNUlknxFJg3H0V8FtgBVFh2QpMA7a4e0VYrBjoHKY7AyvDuhVh+TaJ7dXW2Vf7J5jZNWY21cymrl+//tDfnIikXElpOfe/WsSpR7bhU73axh1HaiiuQ2StiPYoegCdgKZEh7hqnbuPcvdCdy9s165dHBFE5AD++sZSNu3YzQ/P7RN3FElCXIfIzgaWuvt6dy8H/gWcCrQMh8wAugCrwvQqoCtAeL4FsDGxvdo6+2oXkTpmw/YyHnxjCecdfRjHdm0ZdxxJQlwFZgUw2MyahHMpZwFzgVeBi8MyI4Bnw/S4ME94/hV399B+ebjKrAfQC5gMTAF6havSGhJdCDCuFt6XiKTYfa8UUVZRxY1Djoo7iiQp78CLpJ67v2tmY4HpQAUwAxgF/AcYY2a/DG0PhVUeAh4zsyJgE1HBwN3nmNlTRMWpAvi2u1cCmNm1wHiiK9Qedvc5tfX+RCQ1VmzcyePvLufSwq7qEqYOsmhHQAAKCwt96tSpcccQkeC7Y2bwwuw1vPaDMzisRaO448g+mNk0dy+s3q47lUQkI839YBvPzvyAq0/toeJSR6nAiEhGumv8fJrn5/HNzxwRdxQ5SCowIpJx3l2ykVcXrOdbZxxJiybq0LKuUoERkYzi7tz54nw6FOQz4uTucceRQ6ACIyIZ5aV565i+YgvfPbs3jRvmxh1HDoEKjIhkjD3d8fds25RLTuwSdxw5RCowIpIxnn0v6o7/hiG9yVN3/HWefoMikhF2V1Rx90sL6d+pgPMGdIw7jqSACoyIZIQxU1awctMubjz3KHXHnyVUYEQkdjt3V3Dvy0UM7N6a03urV/NsoQIjIrF75K1lbNhexg+HHkXU/61kgxoXGDO7viZtIiLJ2LqznL+8tpgz+7SnsHvruONICiWzBzNiL21XpSiHiNRTf359MdtKK9QdfxY6YHf9ZnYFcCXQw8wSx1RpTtR1vojIQVm3rZRH3lrKsGM70a9TQdxxJMVqMh7M28BqoC3wu4T2EmBWOkKJSP3wx1eKqKh0bjind9xRJA0OWGDcfTmwHDg5/XFEpL5YsXEnT0xewWUndaV726Zxx5E0SOYk/0VmtsjMtprZNjMrMbNt6QwnItnr9xMWkJdrfOesXnFHkTRJZsjk3wCfc/d56QojIvXD/DXRYGLXfLonHQo0mFi2SuYqsrUqLiKSCr8dv4BmGkws6yWzBzPVzJ4EngHK9jS6+79SHUpEste05Zt4ad46bhzSm5ZNGsYdR9IomQJTAOwEhiS0OaACIyI14u7c8cJ82jbL5+pTe8QdR9KsxgXG3a9OZxARyX4T5q5lyrLN/PLCATTNT+b7rdRFNf4Nm9kjRHssH+PuX01pIhHJShWVVdz54nx6tmvKZSd1jTuO1IJkvkI8nzDdCPg88EFq44hItnpqajGL1+/gL18+kQYaTKxeSOYQ2dOJ82b2BPBmyhOJSNbZubuCu19aSOHhrRjSr0PccaSWHMrXiF5A+1QFEZHs9eAbS1lfUsaPzuuj7vjrkWTOwZQQnYOx8HMNcFOacolIltiwvYy/vLaYof0P48TD1R1/fZLMIbLm6QwiItnp3pcXUVpRxQ+Hqjv++iap6wTNbBjw6TA70d2f39/yIlK/LV6/ncffXcGVA7vRs12zuONILUums8s7gOuBueFxvZn9X7qCiUjdd8cL82ncIJfrz1aHlvVRMif5zwPOcfeH3f1hYChwwcFu2MxamtlYM5tvZvPM7GQza21mE0KvzRPMrFVY1szsXjMrMrNZZnZCwuuMCMsvMrMRCe0nmtn7YZ17TWcWRWrVpCUbmTB3Ld864wjaNsuPO47EINmryFomTLc4xG3fA7zo7n2AY4F5wM3Ay+7eC3g5zAN8luiqtV7ANcADAGbWGrgFGAQMBG7ZU5TCMl9PWG/oIeYVkRqqqnJ+9Z95dGrRiK+qS5h6K5kC82tghpmNNrNHgWnArw5mo2bWguhczkMA7r7b3bcAw4FHw2KPAheG6eHA3zwyCWhpZh2Bc4EJ7r7J3TcDE4Ch4bkCd5/k7g78LeG1RCTNxs38gPdXbeUHQ4+iUYPcuONITJK5iuwJM5sInBSabnL3NQe53R7AeuARMzuWqFhdD3Rw99VhmTXAnjuyOgMrE9YvDm37ay/eS/snmNk1RHtFdOvW7SDfjojsUVpeyV3jFzCgcwHDj93rPzupJ5I5yf95YKe7j3P3cUCpmV14kNvNA04AHnD344EdfHQ4DICw5/GJvs9Szd1HuXuhuxe2a9cu3ZsTyXqPvLWMVVt28ePz+pKTo1Of9Vkyh8hucfete2bCIa1bDnK7xUCxu78b5scSFZy14fAW4ee68PwqILF3vC6hbX/tXfbSLiJptK6klPtfLeLsvu055Yi2cceRmCVTYPa27EH1tx0Ora00sz13Xp1FdOnzOGDPlWAjgGfD9DjgK+FqssHA1nAobTwwxMxahZP7Q4Dx4bltZjY4XD32lYTXEpE0+e34BZRVVPLj8/rGHUUyQLIjWv4euD/Mf5vo3MnBug543MwaAkuAq4mK2FNmNhJYDlwalv0v0WXSRUSDnl0N4O6bzOx2YEpY7jZ33xSmvwWMBhoDL4SHiKTJrOIt/HNaMV8/raduqhQALDrVUYMFzZoCPwPOJjo3MgH4lbvvSF+82lVYWOhTp06NO4ZInePuXPznd1i+cQev3Hg6BY0axB1JapGZTXP3wurtyVxF9okT8dU28Ed3v+4g84lIHTZu5gdMW76ZO79wtIqLfCiVo/6cmsLXEpE6YufuCu54YT5Hd27BJSdqpEr5iIaVE5FD8ueJi1m9tZRbPtdPlyXLx6jAiMhBW7lpJ395fQmfO7YThd011ot8XCoLjL66iNQztz8/l9wc48fn9Yk7imSgpAuMmTXZx1P3HGIWEalDJi5Yx//mruW6M3vRsUXjuONIBkqmq5hTzGwuMD/MH2tmf9rzvLuPTn08EclEZRWV/OK5ufRs25Svfqp73HEkQyWzB3M3Ue/FGwHcfSYfjW4pIvXIw28uY+mGHdwyrD/5eeotWfYuqUNk7r6yWlNlCrOISB2weusu/vjKIob068BnequDWNm3ZLqKWWlmpwBuZg2Iutefl55YIpKpfvWfeVRWOT+7oF/cUSTDJbMH8w2i/sc6E/VMfFyYF5F64s1FG3h+1mq+8Zkj6Np6X9f7iESS2YMxd/9i2pKISEbbtbuSH/17Fj3bNuWbpx8RdxypA5LZg3nLzP5nZiPNrGW6AolIZrr7pYWs3LSL/7voaA2DLDVS4wLj7r2BnwL9gelm9ryZfSltyUQkY7xfvJUH31jCFQO7Mbhnm7jjSB2R7FVkk939BmAgsAl4NC2pRCRjlFdWcdPTs2jbLJ+bP6s79qXmkrnRssDMRpjZC8DbwGqiQiMiWezBN5Yyd/U2bhs+gBaN1RW/1FwyJ/lnAs8QjRr5TnriiEgmWbphB394aSFD+x/G0AGHxR1H6phkCkxPr+nwlyJS51VUVnHDU++Rn5fDbcP7xx1H6qADFhgz+4O7fxcYZ2afKDDuPiwdwUQkXn95fQkzVmzh3iuOp31Bo7jjSB1Ukz2Yx8LP36YziIhkjtmrtnL3hIVccExHhh3bKe44UkcdsMC4+7QweZy7f6xLfjO7HngtHcFEJB6l5ZXc8NR7tG7akF9eOCDuOFKHJXOZ8oi9tF2VohwikiF+978FLFy7nd9cfAwtmzSMO47UYTU5B3MFcCXQw8zGJTzVnOheGBHJEpOWbOTBN5fyxUHdOP2o9nHHkTquJudg9tzz0hb4XUJ7CTArHaFEpPZt3VnODU++x+Gtm/CT8/vGHUeyQE3OwSwHlgMnpz+OiMTB3fnxM++zrqSMp795Ck0aJnMHg8jeJXMn/2Azm2Jm281st5lVmtm2dIYTkdrx9PRV/GfWar53Tm+O7doy7jiSJZI5yX8fcAWwCGgMfA24Px2hRKT2LNuwg1uenc2gHq35xmfUDb+kTrKdXRYBue5e6e6PAEPTE0tEakN5ZRXXP/keuTnG3ZcdR26OxR1JskgyB1p3mllD4D0z+w3Rif+kCpSIZJZ7XlrEzJVbuP/KE+jUsnHccSTLJFMgvgzkAtcCO4CuwBfSEUpE0u/NRRu4f2IRl5zYhfOP6Rh3HMlCNd6DCVeTAewCfpGeOCJSG9ZtK+W7T87gyHbN+IU6spQ0OeAejJm9b2az9vU4lI2bWa6ZzTCz58N8DzN718yKzOzJcEgOM8sP80Xh+e4Jr/Gj0L7AzM5NaB8a2orM7OZDySmSTSqrnO8++R7byyq4/4sn6JJkSZua/GVdkMbtXw/MAwrC/J3A3e4+xsz+DIwEHgg/N7v7kWZ2eVjuMjPrB1xONIxzJ+AlM+sdXut+4BygGJhiZuPcfW4a34tInXDfK0W8vXgjv/nCMfTu0DzuOJLFDrgH4+7L9/c42A2bWRfgfODBMG/AmcDYsMijwIVhejgfDc88FjgrLD8cGOPuZe6+FCgiGmVzIFDk7kvcfTcwJiwrUq+9s3gj97y8kM8f35lLCrvEHUeyXDI3WpaY2bbwKE3BjZZ/AH4IVIX5NsAWd68I88VA5zDdGVgJEJ7fGpb/sL3aOvtq39v7usbMpprZ1PXr1x/C2xHJbOtLyvjOmBl0b9uUX144gOg7mkj61LjAuHtzdy9w9wKiGy2/APzpYDZqZhcA6xKGAoiNu49y90J3L2zXrl3ccUTSorLKuX7MDLbtKuf+K0+gab7Ou0j6HdR9LB55Bjj3QMvuw6nAMDNbRnT46kzgHqClme35y+8CrArTq4guiyY83wLYmNhebZ19tYvUS394aSFvL97I7RcOoG/HggOvIJICyRwiuyjhcbGZ3QGUHsxG3f1H7t7F3bsTnaR/xd2/CLwKXBwWGwE8G6bH8dF4NBeH5T20Xx6uMusB9AImA1OAXuGqtIZhG4lDDYjUGxMXrOOPr0T3u1xa2PXAK4ikSDL7yZ9LmK4AlpH6E+c3AWPM7JfADOCh0P4Q8JiZFRGNQXM5gLvPMbOngLkh07fdvRLAzK4FxhPdHPqwu89JcVaRjPfBll1878n36HNYc24brtEppXZZtCMgAIWFhT516tS4Y4ikRFlFJVeMmsSCNSU8d92n6NmuWdyRJEuZ2TR3L6zeXuM9mHAI6jqge+J67j4sFQFFJHXcnZ/8ezbTV0T9jKm4SBySOUT2DNGhquf46NJiEclAo15fwthpxVx/Vi/1MyaxSabAlLr7vWlLIiIpMWHuWu54cT7nH9OR68/qFXccqceSKTD3mNktwP+Asj2N7j495alE5KDMW72N68fM4OjOLfjtxceSo/FdJEbJFJijibrsP5OPDpF5mBeRmK0rKeVrj06leaM8/vqVQho3zI07ktRzyRSYS4CeoW8vEckgO3dXMHL0VDbt2M1T/+9kOhQ0ijuSSFJ38s8GWqYph4gcpIrKKq77xwzmfLCV+648nqO7tIg7kgiQ3B5MS2C+mU3h4+dgdJmySEzcnVufm8PL89dx+/D+nNW3Q9yRRD6UTIG5JW0pROSgjHp9CX+ftIL/95mefPnk7nHHEfmYZIZMfi2dQUQkOc++t4pfvzCfC47pyE3n9ok7jsgnJHMnfwnRVWMADYEGwI7Qfb+I1KKX563lhqdmMrhna357iS5HlsyUzB7Mh2OrJowmOTgdoURk395ZvJFvPT6d/p0KeHDESTRqoMuRJTPFNR6MiByEWcVb+NqjU+jWugmjrx5IMw0cJhksmUNkFyXM5gCFHOR4MCKSvEVrSxjx8GRaNW3IYyMH0bppw7gjiezXoY4Ho0uURWrBwrUlXPnXSeTl5vD41wZxWAvdSCmZL5kCkwNc7+5bAMysFfA74KtpyCUiwfw127jyr++Sl2M8cc1gDm/TNO5IIjWSzDmYY/YUFwB33wwcn/JEIvKhuR9s44pRk2iQa4y5ZjBHaFwXqUOSKTA5Ya8FADNrTXJ7QCKShNmrtnLlg5No1CCXJ685WYOGSZ2TTIH4HfCOmf0zzF8C/Cr1kURk8tJNjBw9hYLGDXji64Pp1qZJ3JFEkpbMfTB/M7OpfNQ9/0XuPjc9sUTqr5fnreVbj0+nc6vGPDZyEJ1bNo47kshBSeoQVygoKioiafKv6cX8YOws+nUsYPTVJ9GmWX7ckUQOms6hiGSIh99cym3Pz+Xknm3464hC3UQpdZ7+gkViVlXl3Dl+Pn95bQnn9u/APZcfr+5fJCuowIjEaHdFFTc9PYt/z1jFlwZ34xfDBpCrjislS6jAiMRke1kF3/z7NN5YtIEbh/Tm22ccSdSPrEh2UIERicHabaWMfHQK81aX8JuLj+HSwq5xRxJJORUYkVo294NtjHx0Clt3lfPgVwo5o0/7uCOJpIUKjEgtemX+Wq79xwxaNG7A2G+cQr9OGq9PspcKjEgtcHdGv72M25+fS/9OLXhoRCHtC9QjsmQ3FRiRNCstr+Snz8xm7LRihvTrwB8uP44mDfVPT7LfQY1oeajMrKuZvWpmc81sjpldH9pbm9kEM1sUfrYK7WZm95pZkZnNMrMTEl5rRFh+kZmNSGg/0czeD+vca7o8R2KwYuNOLvrT24ydVsx3zurFn790ooqL1BuxFBiiAcu+7+79gMHAt82sH3Az8LK79wJeDvMAnwV6hcc1wAPwYY/OtwCDgIHALQk9Pj8AfD1hvaG18L5EPvTK/LVc8Mc3KN68k0euOokbzulNju5xkXoklgLj7qvdfXqYLgHmAZ2B4cCjYbFHgQvD9HDgbx6ZBLQ0s47AucAEd98UxqeZAAwNzxW4+yR3d+BvCa8lklYVlVXcNX4+Xx09lS6tmvD8dafpSjGpl2LfVzez7kQDl70LdHD31eGpNUCHMN0ZWJmwWnFo21978V7a97b9a4j2iujWrdshvBMRWLO1lO88MYPJyzZxWWFXfjG8v7p9kXor1gJjZs2Ap4Hvuvu2xNMk7u5m5unO4O6jgFEAhYWFad+eZK+JC9Zxw1MzKS2v5O7LjuXzx3eJO5JIrGIrMGbWgKi4PO7u/wrNa82so7uvDoe51oX2VUDirc5dQtsq4PRq7RNDe5e9LC+ScqXllfx+wkJGvb6EPoc1574rT+DI9hp9UiSuq8gMeAiY5+6/T3hqHLDnSrARwLMJ7V8JV5MNBraGQ2njgSFm1iqc3B8CjA/PbTOzwWFbX0l4LZGUmb1qK8Pue5NRry/hi4O68cy3T1VxEQni2oM5Ffgy8L6ZvRfafgzcATxlZiOB5cCl4bn/AucBRcBO4GoAd99kZrcDU8Jyt7n7pjD9LWA00Bh4ITxEUqKisoo/TVzMvS8vok2zhoy++iROP0on8kUSWXSRlUB0Dmbq1Klxx5AMN2/1Nm56ehazircy7NhO3Da8Py2bNIw7lkhszGyauxdWb4/9KjKRumJ3RRX3vVrEn14tokXjBtx/5Qmcf0zHuGOJZCwVGJEaeG/lFm4aO4sFa0u48LhO/Pxz/WndVHstIvujAiOyH9vLKvjd/xbw6NvLaN+8EQ9fVciZfToceEURUYER2ZeX5q7l58/OZvW2Ur406HB+MPQoCho1iDuWSJ2hAiNSzbINO7jzxfm8MHsNvTs0Y+yVp3Di4a0OvKKIfIwKjEiwblsp976yiDGTV5KXa9w4pDfXfPoIGubF1SesSN2mAiP13tad5Yx6YzEPv7mM8soqLh/Yle+c2UsDgokcIhUYqbd2lFXwyFtLGfX6EraVVjDs2E7ccE5vurdtGnc0kaygAiP1Tml5JX+ftJwHJi5m447dnN23PTeccxT9OhXEHU0kq6jASL1RWl7JE5NX8MDExawrKePUI9vw/SFHcUI3ncAXSQcVGMl6peWVjJm8ggdeW8zabWUM7NGaey4/npOPaBN3NJGspgIjWWvD9jL+Pmk5j72znI07djOwR2v+cJkKi0htUYGRrFO0roSH3lzK09NXsbuiirP6tOdrp/VkcM/WJA5qJyLppQIjWcHdeX3RBh5+cymvLVxPfl4OF5/Yha+e2kPjs4jERAVG6rRtpeU8N/MDRr+1jEXrttOueT7fP6c3Vw7qRptm+XHHE6nXVGCkzqmsct5evIGx04p5cfYayiqq6NexgN9dciwXHNuR/LzcuCOKCCowUocsWlvCv2as4pkZq1i9tZSCRnlcUtiFS07syjFdWuj8ikiGUYGRjLZ2Wyn/mbWaf89YxfurtpKbY5zWqy0/Ob8vZ/ftQKMG2lsRyVQqMJJxVm/dxQvvr+GF2auZunwz7tC/UwE/Pb8vw47rRPvm6iNMpC5QgZGMsHZbKf99fzXPz1rNtOWbAehzWHO+d3Zvzjv6MI5s3zzmhCKSLBUYic2KjTt5ad5aXpyzhinLNuEeFZUbh/TmvKM70rOdLi8WqctUYKTWVFRWMW35Zl5ZsI6X562jaN12AHp3aMZ3z+rN+cd01D0rIllEBUbSan1JGa8tXM+rC9bx+sL1lJRW0CDXGNSjDVcO7MZZfdtzeBt1jy+SjVRgJKV2lFUweekm3izawFtFG5i/pgSA9s3zOW9AR87o045Tj2xLc41tL5L1VGDkkJSWVzJt+WbeWbyRd5ZsZObKLVRUOQ3zcjipeyt+OPQoPt2rHf07Feg+FZF6RgVGklJaXsn0FZuZtGQTk5Zs5L0VW9hdWUVujnF05xZ87bSefOrIthR2b6V7VETqORUY2a+KyipmFm/l7aINvLV4A9OXRwUlx2BA5xZcdWp3Tu7ZhsLurXTYS0Q+RgVGPqasopL3i7cyedkmpizdxJRlm9leVgFAv44FjDjlcE4+og2F3VtToIIiIvuhAlPPrd1WyowVW5ixcjMzVmxh5sotlFVUAXBk+2YMO64Tpx7RlpOPaEPrpg1jTisidYkKTD3i7izdsIPJSzcxeekm3l26iVVbdgHQMDeHfp0K+OKgwxnYozUndW+l7u5F5JCowGSx0vJK5nywlWnLNzN9+RamLt/Mhu1lALRt1pCBPVoz8lM9OL5bS/p1KlA39yKSUlldYMxsKHAPkAs86O53xBwpLdyd9SVlLF6/g3mrtzF39TbmfrCNRetKKK90AA5v04TTerXlpO6tGdSzNT3bNtVlwyKSVllbYMwsF7gfOAcoBqaY2Th3nxtvsgNzd8oqqti5u5IdZRWUlFawvayCTTt2s3FHGRtKdrNhexmrt5ayYtMOVmzaSWl51Yfrt23WkH6dWvDp3u04vltLTujWinbNdbhLRGpX1hYYYCBQ5O5LAMxsDDAcSHmB+fG/32fp+h04/mGbe/So8qi1yp2qKqfSnaqqaL68sorySqeisoryKqesvJKyiqoPT7LvT8smDejQvBHdWjfltF7t6Na6Cd3bNqVvx+bqzl5EMkI2F5jOwMqE+WJgUPWFzOwa4BqAbt26HdSGKiudyqpQXBKOOuXmGHlm5JhhBjlm5OZE8zkGDfJyaJBj5OXm0CDXyM/LJT8vh/wG0c+mDXNp1qgBzfJzaZbfgJZNGtC2WT6tmzakYV7OQWUVEakt2VxgasTdRwGjAAoLC/0Ai+/VnRcfk9JMIiLZIJu/Bq8CuibMdwltIiJSC7K5wEwBeplZDzNrCFwOjIs5k4hIvZG1h8jcvcLMrgXGE12m/LC7z4k5lohIvZG1BQbA3f8L/DfuHCIi9VE2HyITEZEYqcCIiEhaqMCIiEhaqMCIiEhamPtB3VuYlcxsPbA8TS/fFtiQptdOJeVMnbqQEZQzlepCRkh9zsPdvV31RhWYWmJmU929MO4cB6KcqVMXMoJyplJdyAi1l1OHyEREJC1UYEREJC1UYGrPqLgD1JBypk5dyAjKmUp1ISPUUk6dgxERkbTQHoyIiKSFCoyIiKSFCkyKmdlQM1tgZkVmdvNenr/BzOaa2Swze9nMDs/QnN8ws/fN7D0ze9PM+mVaxoTlvmBmbmaxXB5ag8/yKjNbHz7L98zsa5mYMyxzafj7nGNm/8i0jGZ2d8LnuNDMttR2xpDjQDm7mdmrZjYj/Fs/L0NzHh7+H5plZhPNrEtKA7i7Hil6EA0LsBjoCTQEZgL9qi1zBtAkTH8TeDJDcxYkTA8DXsy0jGG55sDrwCSgMEM/y6uA++L4m0wyZy9gBtAqzLfPtIzVlr+OaBiOTPwsRwHfDNP9gGUZmvOfwIgwfSbwWCozaA8mtQYCRe6+xN13A2OA4YkLuPur7r4zzE4iGmmzttUk57aE2aZAbV8NcsCMwe3AnUBpbYZLUNOccatJzq8D97v7ZgB3X5eBGRNdATxRK8k+riY5HSgI0y2AD2ox3x41ydkPeCVMv7qX5w+JCkxqdQZWJswXh7Z9GQm8kNZEe1ejnGb2bTNbDPwG+E4tZdvjgBnN7ASgq7v/pzaDVVPT3/kXwmGIsWbWdS/Pp1tNcvYGepvZW2Y2ycyG1lq6SI3//YRDyz346D/H2lSTnLcCXzKzYqIxqa6rnWgfU5OcM4GLwvTngeZm1iZVAVRgYmJmXwIKgbvizrIv7n6/ux8B3AT8NO48icwsB/g98P24s9TAc0B3dz8GmAA8GnOefckjOkx2OtHewV/NrGWcgfbjcmCsu1fGHWQfrgBGu3sX4DzgsfA3m2luBD5jZjOAzwCrgJR9ppn4huuyVUDit9Muoe1jzOxs4CfAMHcvq6VsiWqUM8EY4MJ0BtqLA2VsDgwAJprZMmAwMC6GE/0H/CzdfWPC7/lB4MRaypaoJr/zYmCcu5e7+1JgIVHBqS3J/F1eTjyHx6BmOUcCTwG4+ztAI6IOJmtTTf42P3D3i9z9eKL/k3D3LSlLUNsnnrL5QfQNcAnRrvuek2r9qy1zPNGJt14ZnrNXwvTngKmZlrHa8hOJ5yR/TT7LjgnTnwcmZWjOocCjYbot0eGVNpmUMSzXB1hGuFE8Qz/LF4CrwnRfonMwtZq3hjnbAjlh+lfAbSnNEMcvKJsfRLvDC0MR+Ulou41obwXgJWAt8F54jMvQnPcAc0LGV/f3n3tcGastG0uBqeFn+evwWc4Mn2WfDM1pRIcd5wLvA5dnWsYwfytwRxyfYRKfZT/grfA7fw8YkqE5LwYWhWUeBPJTuX11FSMiImmhczAiIpIWKjAiIpIWKjAiIpIWKjAiIpIWKjAiIpIWKjAiIpIWKjCSMcxstJldfBDrdTezK2tre/XJwX621V7jKjPrlKpM4TUnxjU8g9ScCoxkg+7AIf0nKPvUnUP/bK8CUlpgpG5QgZG0MrOmZvYfM5tpZrPN7DIzO9HMXjOzaWY23sw67mW9vS5jZkea2Uvh9aab2RHAHcBpYRCq75lZrpndZWZTQg/G/y+sa2Z2XxiA6SWg/QGyLzOzX4fXnWpmJ4Qsi83sG2GZZmHApukWDdA2PLSfFLbdKHwGc8xswD62c3p4r8+a2RIzu8PMvmhmk8NrHhGW+5yZvRsGsXrJzDqE9nvM7Odh+lwze31fHSuGPZJX7KMB77qF9o/tzZnZ9jBZ/bO9KuScaGaLzOyWhNednbD+jWZ2a3jNQuDx8BqN95JpqJn9s9rn8XyYfiB89nPM7Bf7eE/bE6YvNrPRYbqdmT0d/g6mmNmpe1tf0ijO7hb0yP4H8AXgrwnzLYC3gXZh/jLCoFHAaKKuKxrsZ5l3gc+H6UZAE6Lef59P2MY1wE/DdD4wlag/pouIejPOJfpGvQW4eD/Zl/HRoFF3A7OIOtlsB6wN7XmEwdmI+nUqgg97yPgl8FvgfuBH+9nO6SFLx5B3FfCL8Nz1wB/CdKuE1/4a8Lsw3YSoK5ozgAXAEfvZ1nN8NMDUV4FnEj/7hOW2J2RL/GyvAlYDbYDGwGyiAtIdmJ2w3I3ArWF6Ivvpxid8hiuApmH+AeBLYbp1+JkbXueY6q+5J2uYvpioF2OAfwCfCtPdgHlx/3uob488RNLrfeB3ZnYn8DywmagX5AlmBtF/HKurrXPU3pYxs+ZAZ3f/N4C7lwKEZRINAY5J+EbegqhX4E8DT3jUxfsHZlaTsUTGJbyPZu5eApSYWZlFXdnvAP7PzD4NVBGNt9EBWEPU59MUosHQDjSezhR3Xx3ez2LgfwnbPSNMdwGeDHtzDYGl4XPYaWZfJxrZ83vuvng/2zmZj8b/eIxorJ9kTXD3jSHrv4BPAc8cxOsA4O4VZvYi8DkzGwucD/wwPH2pmV1DVIQ6EvXxNauGL3020C/h76PAzJq5+/b9rCMppAIjaeXuCy0aGOw8om/0rwBz3P3k/axme1smFJiaMOA6dx9fbf2DGRd9Tzf7VQnTe+bzgC8S7dGc6O7lFg0d0Cgs0wZoRrRH1oioGB1oO9W3tWc7AH8Efu/u48zsdKJOH/c4GtjIwZ/rqCAcMg+H1xruZ9nqHRh64vpBI5IzBrgW2ETUc3eJmfUg2hM6yd03h0Nfe3vdxDyJz+cAg/d8EZHap3MwklYWXT20093/TjS42iCgnZmdHJ5vYGb9q622YG/LhL2HYjO7MLTnm1kToITo0NUe44FvmlmDsFxvM2tK9A3/MovO0XTkoz2DQ9ECWBeKyxnA4QnP/QX4GfA40bDOqdjWnvE8RuxptGh0x+8TDQXxWTMbtJ/XeJtoLBWIiuMbYXoZH41TM4yoKMInP1uAc8ysdTifciFRr8FrgfZm1sbM8oELEpbf22tU9xpwAtGwzWNCWwFRUd4azjd9dh/rrjWzvqEwfj6h/X8kjCRpZscdIIOkmPZgJN2OBu4ysyqgHPgm0bfde82sBdHf4B+IziEA4O67w+GtvS3zZeAvZnZbeL1LiA6ZVJrZTKJzCfcQnROYbtHxkfVE/xH+GziTqDv6FcA7KXh/jwPPmdn7ROd65gOY2VeAcnf/h5nlAm+b2ZnufihD/N4K/NPMNhPtCfYI7+8h4EZ3/8DMRgKjzeykfXxzvw54xMx+QPS5XB3a/wo8Gz7DF/lob6v6Z7sZmAw8TXTI7u/uPjW859vCc6v2fA7BaODPZrYLONndd1UP5e6V4cT+VYTi6e4zLRppcT7R2DRv7eNzuZno8Ot6ot9Bs9D+HeB+M5tF9Df0OvCNfbyGpIG66xeRGjOzq4hOrl8bdxbJfDpEJiIiaaE9GKn3zOzfRJcxJ7qp+kUCKdjO0URXbiUqc/f9nTM52G39hOjwYaJ/uvuvUr2tZNTWZy2ZQQVGRETSQofIREQkLVRgREQkLVRgREQkLVRgREQkLf4/0GtpFfeYRGoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# توزیع تجمعی مقادیر خروجی بیشینه\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cum_values = []\n",
    "values = []\n",
    "\n",
    "interval_number = 100\n",
    "for i in range(18, 92):\n",
    "    cum_values.append(np.where(sofmax_values < ((i+1) / interval_number))[0].shape[0])\n",
    "    values.append(np.where(((i / interval_number) < sofmax_values) & (sofmax_values < ((i+1) / interval_number)))[0].shape[0])\n",
    "\n",
    "# Create a figure and an axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = np.array(range(18, 92)) / interval_number\n",
    "ax.plot(x, cum_values)\n",
    "\n",
    "plt.xlabel('selected_max_max_output_value')\n",
    "plt.ylabel('cumulative_count')\n",
    "\n",
    "plt.savefig('output_cumulative_distribution.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# توزیع مقادیر خروجی بیشینه\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values = []\n",
    "interval_number = 100\n",
    "for i in range(18, 92):\n",
    "    values.append(np.where(((i / interval_number) < sofmax_values) & (sofmax_values < ((i+1) / interval_number)))[0].shape[0])\n",
    "\n",
    "# Create a figure and an axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = np.array(range(18, 92)) / interval_number\n",
    "ax.plot(x, values)\n",
    "\n",
    "plt.xlabel('selected_max_max_output_value')\n",
    "plt.ylabel('count')\n",
    "\n",
    "plt.savefig('max_max_output_distribution.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "# np.where(sofmax_values > 0.95)[0].shape[0] / len(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_count = 0\n",
    "true_count = 0\n",
    "false_values = []\n",
    "true_values = []\n",
    "for i in range(len(testx)):\n",
    "    if softmax_classes[i] != i // 5:\n",
    "        false_count += 1\n",
    "        false_values.append(sofmax_values[i])\n",
    "    else:\n",
    "        true_count += 1\n",
    "        true_values.append(sofmax_values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# توزیع مقادیر خروجی بیشینه صحیح\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t_values = []\n",
    "f_values = []\n",
    "\n",
    "interval_number = 100\n",
    "for i in range(18, 92):\n",
    "    t_values.append(np.where(((i / interval_number) < np.array(true_values)) & (np.array(true_values) < ((i+1) / interval_number)))[0].shape[0])\n",
    "    f_values.append(np.where(((i / interval_number) < np.array(false_values)) & (np.array(false_values) < ((i+1) / interval_number)))[0].shape[0])\n",
    "\n",
    "# Create a figure and an axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the data\n",
    "x = np.array(range(18, 92)) / interval_number\n",
    "ax.plot(x, t_values, 'r--', x, f_values, 'b--')\n",
    "\n",
    "plt.legend(['true_counts', 'false_counts'])\n",
    "\n",
    "plt.xlabel('output_values')\n",
    "plt.ylabel('test_sample_count')\n",
    "\n",
    "plt.savefig('max_max_output_distribution.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEHCAYAAACqbOGYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAArhUlEQVR4nO3debxd873/8dcnEYkhhkikkTkRQ6JIHRGNFokY0prnoikqRWh1cHGr5ZbcUsptywNBQ/xMaVBBiNCYhwgiYqiMmkRIIkSEzJ/fH9+1ZZ+TM6xzzl57rX32+/l4rMdee42fs8P+7PUdzd0RERHJaZZ2ACIiki1KDCIiUokSg4iIVKLEICIilSgxiIhIJZukHUBjtW3b1rt165Z2GCIiJeX1119f4u7tqttX8omhW7duTJkyJe0wRERKipl9WNM+FSWJiEgliSYGM2tlZpPN7C0ze8fM/ifafoeZzTGzqdGyZ7TdzOyvZjbTzKaZ2XeSjE9ERDaWdFHSKmCgu39pZi2AF8zs8Wjfhe4+tsrxhwG9omUf4KboVUREiiTRJwYPvozetoiW2sbgOBIYHZ33CrCNmXVIMkYREaks8ToGM2tuZlOBRcBEd3812jUiKi663sxaRts6AvPyTp8fbat6zWFmNsXMpixevDjJ8EVEyk7iicHd17n7nkAnoJ+Z7QZcAuwC7A20AS6q5zVHunuFu1e0a1dtaysREWmgorVKcvfPgUnAoe6+MCouWgWMAvpFhy0AOued1inaJiIiRZJ0q6R2ZrZNtL4ZMBh4P1dvYGYGHAVMj04ZB/w4ap3UH1jm7guTjFFERCpLulVSB+BOM2tOSEJj3P1RM/uXmbUDDJgKnB0dPx4YAswEvgJOTzi+kuAOq1dDy5Z1Hysi0liJJgZ3nwb0rWb7wBqOd2B4kjGVklWrYNQouOkm2HdfuPnmtCMSkXKgns8Z9tZbcM458PnnMHIkvPZa2hGJSDlQYsiwV6OGvePHQ/v2cO65sG5dujGJSNOnxJBhr74KHTpA797w5z/DlClw221pRyUiTZ0SQ4ZNngz9+oEZnHwyHHAA3HADrF+fdmQi0pSV/LDbTdWyZTBzJpwetcsyg9GjYeutoZnSuYgkSIkho7beGpYsqbytc+fqjxURKST99sywNm3Cku+BB2CnnTZOGiIihaLEkFFXXgm33lr9vhkzYP784sYjIuVDiSGD3OGvf4WXXtp4X6dO4VWJQUSSosSQQR9+CIsXwz7VTFGUq2dQYhCRpCgxZFCuY1u/fhvva98emjdXYhCR5CgxZNDkydCqFXz72xvva94cjj8eunYtflwiUh7UXDWDVq6E/faDFi2q33/vvcWNR0TKixJDBt14Y6iAro176PQmIlJoKkrKqNq+9EeMgO23rzt5iIg0hBJDxowaBf37w2ef1XzMZpuFDm6ff160sESkjCgxZMzzz8OsWbDNNjUfk+vLsECzYYtIApQYMiZ/RNWaqJObiCRJiSFD1q8Pw1306VP7cUoMIpIkJYYMWbIEVq+uexTVDh1g6FDo3r04cYlIeUm0uaqZtQKeA1pG9xrr7peZWXfgPmA74HXgNHdfbWYtgdHAXsCnwInuPjfJGLNkzRo49ljYfffaj2vRAu64oyghiUgZSvqJYRUw0N33APYEDjWz/sDVwPXuviPwGXBmdPyZwGfR9uuj48pGx44wdizsv3/dx7rDl18mH5OIlJ9EE4MHua+vFtHiwEBgbLT9TuCoaP3I6D3R/kFm5dONqz79Ek46KTRrFREptMTrGMysuZlNBRYBE4FZwOfuvjY6ZD7QMVrvCMwDiPYvIxQ3Vb3mMDObYmZTFi9enPBfUDwXXxzqF+IkiG99C+bNSz4mESk/iScGd1/n7nsCnYB+wC4FuOZId69w94p27do19nKZ8Z//wKabxhvqomNH+OKLsIiIFFLRWiW5++fAJGBfYBszy1V8dwJyXbUWAJ0Bov1bEyqhy8K8efHndVYnNxFJSqKJwczamdk20fpmwGDgPUKCOC46bCjwcLQ+LnpPtP9f7uUzIlBDEoP6MohIoSU9umoH4E4za05IQmPc/VEzexe4z8yuBN4Ebo+Ovx24y8xmAkuBkxKOLzPWrQu//uMmhp13hksvjX+8iEhciSYGd58G9K1m+2xCfUPV7SuB45OMKatWrYLzz4cDDoh3fPv2cMUViYYkImVK8zFkxOabw/XX1++cTz8Nk/p07Fj3sSIicWlIjIz46qvwJV8fgwfDsGHJxCMi5UuJISNuvjnMs1DbPAxVde6symcRKTwlhoyYNy8UJ9U2D0NVnTopMYhI4SkxZESuqWp9BgDp1AmWLg3FUCIihaLEkBHz59e/6ak6uYlIEpQYMqI+ndtyBgyAW26BNm2SiUlEypOaq2bEf/1X6LRWHz16qFWSiBSeEkNG/OIXDTtv6lTYckvYcceChiMiZSxWUZKZDYizTRpm2TKYORPWrq372KoGDYLrrit8TCJSvuLWMfwt5jZpgPHjoVcv+OCD+p+rJqsiUmi1FiWZ2b7Ad4F2ZvarvF1bAc2TDKyc5CbcaciAeEoMIlJodT0xbApsSUggrfOWL9gwbLY00rx5sPXW0Lp1/c9V72cRKbRanxjc/VngWTO7w90/LFJMZachTVVzOnWCxYvDOEutWhU2LhEpT3FbJbU0s5FAt/xz3H1gEkGVm3nzNnRWq6/jjoO99oLmKtgTkQKJmxj+AdwM3AasSy6c8nT55Q3/tb/LLmERESmUuIlhrbvflGgkZezwwxt+7rp18Pjj0KUL7L574WISkfIVt7nqI2Z2rpl1MLM2uSXRyMrEsmUwaVJ4bYhmzeCEE+DOOwsbl4iUr7hPDEOj1wvztjnQo7DhlJ833oCBA+Gpp0JntfoyC0NjzJpV+NhEpDzFSgzu3j3pQMpVY/ow5PTsqcQgIoUTKzGY2Y+r2+7uo+s4rzMwGmhPeMIY6e5/MbPLgbOAxdGh/+3u46NzLgHOJFRy/9zdJ8SJsVTl+iA0tFUShMTw1FPgXr/5HEREqhO3KGnvvPVWwCDgDcKXfm3WAr929zfMrDXwuplNjPZd7+7X5h9sZr2Bk4A+wA7AU2a2k7s32ZZQ8+bBdtuF2dsaqmfPMFnPxx9Dhw6Fi01EylPcoqTz89+b2TbAfTHOWwgsjNaXm9l7QMdaTjkSuM/dVwFzzGwm0A94OU6cpagxndtyjj8e9t8f2rUrTEwiUt4aOuz2CqBe9Q5m1g3oC7wKDADOi4qophCeKj4jJI1X8k6bTzWJxMyGAcMAunTp0oDws2PECFi+vHHX2H77sIiIFELcYbcfMbNx0fIY8G/gobg3MbMtgQeAC9z9C+AmoCewJ+GJ4s/1CdrdR7p7hbtXtCvxn8l77AH77df469x6K0xo0rUxIlIscZ8Y8usC1gIfunusodvMrAUhKdzt7g8CuPsneftvBR6N3i4A8gtWOkXbmqSVK2HMmFAM1LVr4641YkSY6vOQQwoTm4iUr1hPDNFgeu8TRlbdFlgd5zwzM+B24D13vy5ve34V6dHA9Gh9HHCSmbU0s+5AL2BynHuVogULYOhQeOaZxl9LTVZFpFDiFiWdQPiCPh44AXjVzOIMuz0AOA0YaGZTo2UI8Ccze9vMpgEHAr8EcPd3gDHAu8ATwPCm3CJp0aLwWoj6ASUGESmUuEVJvwX2dvdFAGbWDngKGFvbSe7+AlBdy/rxtZwzAhgRM66SlksM7ds3/lo9e8KSJfDFF7DVVo2/noiUr7hjJTXLJYXIp/U4V2pQ6CcGgDlzGn8tESlvcZ8YnjCzCcC90fsTgceTCal8fBJVwReiYdWQIfD552EmOBGRxojbwe1CMzsGyDWsHOnusZurSvXOPju0ImrZsvHXakzPaRGRfHHHSuoOjM81NzWzzcysm7vPTTK4pq5t27AUytVXQ5s2cNZZhbumiJSfuPUE/wDW571fF22TRrjnHhhfYzV8/T30ENxX50AlIiK1i5sYNnH3b/ouROubJhNS+bjySrj99sJdT01WRaQQ4iaGxWZ2RO6NmR0JLEkmpPKxaFFhmqrm9OgRBuVbHav7oYhI9eK2SjobuNvMbojezyd0XJMGWrsWPv20sIPf9ewJ69fDhx9Cr16Fu66IlJe4Q2LMcvf+QG+gt7t/192/KbQws6E1ny3VWRI9bxU6MbRuvaEZrIhIQ9Rr2G13/7KGXb8ANB19PeS+vAuZGPbbD5Yt0yxuItI4DZ2PoSp9FdVTnz5hWs9CdkhTQhCRQihUYvACXadsbLIJdKxtLrsGuvTSUH9x1VWFv7aIlIdCjXek36r1NGkSXHEFrFlT2OtOnw6PPlr3cSIiNSlUYnixQNcpGxMmhMSwSaGe2SI9e8Ls2eB6hhORBoo7H0N7M7vdzB6P3vc2szNz+939vKQCbKoWLQoVz4WuF+jRA77+GhYuLOx1RaR8xH1iuAOYAOwQvf8AuCCBeMpGoTu35eSG354xo/DXFpHyEDcxtHX3MUTjJbn7WsJ4SdJAn3xS2KaqORUVsOOOsGJF4a8tIuUhbgn3CjPbjqj1kZn1B5YlFlUZWLwYevcu/HXbttXTgog0TtzE8CtgHNDTzF4E2gFx5nyWGsyaBStXJnf99evDUujKbRFp+uIOifEGsD/wXeBnQB93n1bXeWbW2cwmmdm7ZvaOmf0i2t7GzCaa2Yzoddtou5nZX81spplNM7PvNPxPy7bmzWGLLZK59gcfhPqLhx9O5voi0rTVmhjM7JjcAhwB7AzsBBwebavLWuDX7t4b6A8MN7PewMXA0+7eC3g6eg9wGNArWoYBNzXgb8q8jz4Ks7dNnZrM9bt3DyOsTpiQzPVFpGmrq6Dh8Fr2OfBgbSe7+0JgYbS+3MzeAzoCRwIHRIfdCTwDXBRtH+3uDrxiZtuYWYfoOk3G3Llwyy1w9NHJXL9FCxg0CJ54IvRn0FAZIlIftSYGdz+9UDcys25AX+BVoH3el/3HQK7hZkdgXt5p86NtTSoxLFoUXpNolZRzyCFhRrf334ddd03uPiLS9MTt4LZdVPb/hpm9bmZ/iVopxWJmWwIPABe4+xf5+6Kng3r10zWzYWY2xcymLF68uD6nZkISI6tWdcgh4VXFSSJSX3H7MdwHLAaOJbRGWgzcH+dEM2tBSAp3u3uu6OkTM+sQ7e8ARL+hWQB0zju9U7StEncf6e4V7l7Rrl27mH9CduSeGJIMvVu3MOTGgAHJ3UNEmqa4iaGDu1/h7nOi5Uo2FP/UyMwMuB14z92vy9s1DshN7jMUeDhv+4+j1kn9gWVNrX4BQjPV9u1h04Rnzb70Uth772TvISJNT9zE8KSZnWRmzaLlBMIQGXUZQJgCdKCZTY2WIcBVwGAzmwEcFL0HGA/MBmYCtwLn1uePKRUjRhRnLKN16+DFF9XhTUTqxzzGMJxmthzYgmhIDEJCyQ264O6+VTLh1a2iosKnTJmS1u0z7auvoE0bGD4c/vzntKMRkSwxs9fdvaK6fXE7uLV292buvkm0NIu2tU4zKZSqc86Bm4rQQ2PzzeF731MFtIjUT+wBE8xsd6Bb/jl5lclSD2PGQLNCzYRRh0MOgQsvDNOIdupUnHuKSGmL21z178DfCa2SDo+WHyYYV5O1Zg0sXZpsU9V8uWarTz5ZnPuJSOmL+8TQPxrWQhppyZLwmsRcDNXZbTfYait47TU444zi3FNESlvcxPCymfV293cTjaYMFKNzWz4zePnl0K9BRCSOuIlhNCE5fAysAozQGmn3xCJrolauDF/SO+xQ56EFk8S8DyLSdMVNDLcT+iO8zYYmq9IA/fvDnDnFveecOaEV1Lnn6slBROoWNzEsdvdxiUYiiVm+HK65Bvr2VWIQkbrFbTT5ppndY2YnV5mjQerpr3+FI48s7j133TUMv/Hmm8W9r4iUprhPDJsR6hYOzttW53wMsrE33ij+F3SLFqF1khKDiMQRKzEUcl6GcrdoUfFaJOXr2xf++U9N3CMidYuVGMysFXAm0Adoldvu7moZX09pJoaHHoJPP4W2bYt/fxEpHXHrGO4CvgUcAjxLmCdheVJBNWVpJYaf/Sx0rlNSEJG6xE0MO7r774AV7n4n8ANgn+TCarp22gn69Cn+fTfZREVIIhJP3MrnNdHr52a2G2Ge5hR+95a+p55K797//d+hg91119V9rIiUr7hPDCPNbFvgUsIsa+8Cf0osKknE7NmhnkFEpDZx52O4zd0/c/fn3L2Hu2/v7jcnHVxTM3lyaDaa1rxCffvC3Lnw2Wfp3F9ESkPcYbd/YWZbRXMx32Zmb5jZwXWfKflmzoR33oEttkjn/n37htepU9O5v4iUhrhFSWe4+xeEDm7bEcZNuqr2U6SquXPDa9eu6dx/zz3Dqzq6iUht4iaGXHuWIcBod38nb5vENHduaKq6+ebp3H/77WHgwPTuLyKlIW5ieN3MniQkhglm1poYo6ya2d/NbJGZTc/bdrmZLTCzqdEyJG/fJWY208z+bWaH1PePybq5c9MfxO7pp+Hss9ONQUSyLW5z1TOBPYHZ7v6VmW0HfDNMhpn1iZ4iqroDuIEwn0O+69392vwNZtYbOInQu3oH4Ckz28nd18WMMfP22CMbv9bdw6v6NYhIdeK2Slrv7m+4++fR+0/dfVreIXfVcN5zwNKYsRwJ3Ofuq9x9DjAT6Bfz3JJwzTXwP/+TbgzPPw/t2qXXMkpEsi9uUVJd6vvb8zwzmxYVNW0bbesIzMs7Zn60rUlw3/BLPU0dO4bxktQySURqUqjEUJ+vvJuAnoSiqYXAn+t7MzMbZmZTzGzK4sWL63t6Kl5+GbbZJvxiT1P37rD11koMIlKzQiWG2Nz9E3df5+7rgVvZUFy0AOicd2inaFt11xjp7hXuXtGuXbtkAy6QuXPhiy/SH8TOLIzXNGNGunGISHYVKjGsjnugmXXIe3s0kGuxNA44ycxamll3oBcwuUDxpS7tPgz5evQIw2OIiFQn7nwMT7v7oJq2uXv/Gs67FzgAaGtm84HLgAPMbE9C8dNc4GfRNd4xszGEcZjWAsObUouktPsw5Dv88PSbzYpIdtWaGKIJejYnfLFvy4ZK5q2IUTHs7idXs/n2Wo4fAYyo67qlKAt9GHJOOSXtCEQky+p6YvgZcAGhX8HrbEgMXxD6J0hMhx0GzYpeo1Ozr78OraSy8AQjItliHqMNpZmd7+5/K0I89VZRUeFT1Ci/XubPh86dYeRIOOustKMRkTSY2evuXlHdvri/YT+OhsHAzC41swfN7DsFi7CJW7MmtEjKig4doEULmDUr7UhEJIviJobfuftyM9sPOIhQT3BTcmE1La+9FvoOTJiQdiRB8+ahvkMtk0SkOnETQ6510A+Ake7+GLBpMiE1Pbmmql26pBpGJWqyKiI1iZsYFpjZLcCJwHgza1mPc8telvow5PTsqcQgItWLO7rqCcChwLXu/nnUSe3C5MJqWrLUhyHn2GNDD+h160LRkohITqzEEA21vQjYD5hB6ICmQRViylIfhpyBA8MiIlJV3J7PlwEVwM7AKKAF8P+AAcmF1nScfnr4ZZ4l69aFoqTWreFb30o7GhHJkrj1BEcDRwArANz9I6B1UkE1NSefDKeemnYUlX31VShKuuOOtCMRkayJmxhWe+gJ5wBmtkVyITUtX38N77wDK1emHUllrVuHCXtUAS0iVcVNDGOiVknbmNlZwFPAbcmF1XRMnQq77Qb/+lfakWxMTVZFpDpxK5+vNbPBhDGSdgZ+7+4TE42sicg1Vc1a5TOEJqsvvZR2FCKSNbGeGMzsanef6O4Xuvtv3H2imV2ddHBNQRb7MOT06AH/+U8YskNEJCduUdLgarYdVshAmqq5c0NZ/hYZrJU54QT4xz+yMRe1iGRHXfMxnAOcC/Qws2l5u1oDLyYZWFORxT4MOd/+dlhERPLVVcdwD/A48Efg4rzty919ae6NmW3r7p8lEF/Ju/ji0DIpi9atg2efDaOt7rpr2tGISFbUWpTk7svcfa67n+zuH+YtS6sc+nSCMZa0Aw+EIUPSjqJ6ZmECoVGj0o5ERLKkUAPhWd2HlJ/ly+GJJ+DTT9OOpHrNmkH37mqyKiKVFSoxqPqyGtOnh1/kr76adiQ169FDE/aISGWJDp1tZn83s0VmNj1vWxszm2hmM6LXbaPtZmZ/NbOZZjatKcwQl+U+DDm54bfVMklEcpIuSrqDMFx3vouBp929F6FuIlepfRjQK1qG0QRmiPv3v0NxTZYTQ48eYdrRpVVrjUSkbMVODGa2n5mdHq23M7PuebsHVXeOuz8HVP3KORK4M1q/Ezgqb/toD14hDL/RIW58WTRtGvTqla15GKo64QSYPDmMnSQiAgUadruaVkq1ae/uC6P1j4H20XpHYF7ecfOjbQupwsyGEZ4q6JKl+TKreOst2GuvtKOoXceOYRERyYk7g9vRQF/gDQjDbptZo39jurubWb1Lt919JDASoKKiIrOl4+PGpR1B3dxh9OjQOun73087GhHJgjSG3f4kV0QUvS6Kti8AOucd1ynaVrL69AlLlpmFTnijR6cdiYhkRWOG3b61gfccBwyN1ocCD+dt/3HUOqk/sCyvyKnkvPAC3HpraQxQ16MHfPBB2lGISFbESgzufi0wFniADcNu/62u88zsXuBlYGczm29mZwJXAYPNbAZwUPQeYDwwG5hJSDrn1vNvyZS774YLL4RN4hbWpWjAAHjlFfhMg5qICPErn7cA/hUNt70z4Yu+hbvX+nvY3U+uYddGrZiioqrhceIpBdOmwe67h6KarDvuOLjmmlAnMnRo3ceLSNMWtyjpOaClmXUEngBOI/RRkGqsXw9vvx0SQynYe2/o0gVeey3tSEQkC+IWdJi7fxUVBd3k7n8ys6kJxlXS5s4N4yTtsUfakcRjBm++CW3apB2JiGRB3CcGM7N9gVOAx6JtzZMJqfS9/354LZUnBlBSEJEN4j4xXABcAjzk7u+YWQ9gUmJRlbghQ8KIqqXWm/iXvwxxq+mqSHmL2yrpWXc/wt2vjt7PdvefJxtaaWvTBlq0SDuK+lm/HsaMCcVgIlK+ak0MZvaImY2raSlWkKVm+PAwl3KpOe44WLUKxo9POxIRSVNdRUnXFiWKJmTFCrjpJmjfvu5js2bAgDDN59ixcOKJaUcjImmpNTG4+7PFCqSpmD49jD9UShXPOc2awTHHhKk+V6yALRoz8ImIlKy4Hdx6AX8EegOtctvdvUdCcZWsadPCa6k0Va3qtNNCQli1SolBpFzFbZU0CrgMuB44EDidhGd/K1VvvRVaI3XtmnYkDbPPPmERkfIV98t9M3d/mtDR7UN3vxz4QXJhla61a6F//1AsU8ruvTd0ehOR8hP3iWGVmTUDZpjZeYThsLdMLqzSdfPNaUfQeCtWwEUXhR7RU6ZAu3ZpRyQixVRXc9W7otV/ApsDPwf2IoyVpOHWmqgttoAHH4RPPgmtk0ph6HARKZy6Cjz2MrMdCENhtAC+An4N/BTQCP5VPPkk7LsvzJ6ddiSNV1EBI0fCpElh+HARKR91FSXdDDwN9ABeB4wwi1vuVa2S8rz2WpjXoG3btCMpjB//ONQz/N//wSmnhFFYRaTpszANQh0Hmd3k7ucUIZ56q6io8ClTpqQdBgD77QfLloUht5uKtWvDk9CQIWlHIiKFZGavu3tFdfvijpWUyaSQJbNnw4svhl/WTckmmygpiJSbEm9UmR133x1ef/SjdONIyplnwvnnpx2FiBSDEkOB7LEH/OY3YSa0pujLL+GBB8JwHyLStCkxFMgRR4R5k5uqwYNh4UJ47720IxGRpKWWGMxsrpm9bWZTzWxKtK2NmU00sxnR67ZpxVcfkybBxx+nHUWyBg8OrxMnphuHiCQv7SeGA919z7ya8YuBp929F6GZ7MXphRbPmjVwwgnwi1+kHUmyunaFHXdUYhApB3GHxCiWI4EDovU7gWeAi9IKJo4JE2DJEjj11LQjSd7PfgYrV6YdhYgkLVY/hkRubDYH+IzQUe4Wdx9pZp+7+zbRfgM+y72vcu4wYBhAly5d9vrwww+LFndVJ54ITz8dyt9LbSpPESlfje7HkJD93P07wGHAcDP7fv5ODxmr2qzl7iPdvcLdK9qlOMLbsmUwbhycdFL5JIWVK2Hu3LSjEJEkpVaU5O4LotdFZvYQ0A/4xMw6uPtCM+sALEorvjheeCFMaHPaaWlHUjwHHxx6Q7/0UtqRiEhSUnliMLMtzKx1bh04GJgOjGPDqK1DgYfTiC+uH/wA5s2Dfv3SjqR4vv99mDw5PC2JSNOUVlFSe+AFM3sLmAw85u5PAFcBg81sBnBQ9D7TOnYM8xaUi8GDYd06eOaZtCMRkaSkkhjcfba77xEtfdx9RLT9U3cf5O693P0gd1+aRnxxPPggHHYYLMp0YVfh7bsvbL65mq2KNGVp92MoWQ88EGY32267tCMprk03hf33V2IQacqy1o+hJKxdC48/HobBaN487WiK77LLwphJ7uVVjCZSLvTE0AAvvgiffQaHH552JOnYZx/o3z8Uox1ySHh60vSfIk2HEkMDPPJIKFI5+OC0I0nXrFnw/vtw3HHQrVtovisipU+JoQF23hmGD4fWrdOOJF3f/W6YoGjcuJAohw0LxWwiUtpUx9AAZ52VdgTZ0bx5KFJbvx6OOgoeegiOPz7tqESkMfTEUE+zZsGKFWlHkT1HHBHGjDruuLQjEZHGUmKopzPOgAMPTDuK7DGDgQPDq0ZgFSltSgz1sHRpaJFU7pXOtfnnP6Fz5zBUiIiUJiWGenj88TAcRLk2U42jb19Yvhx++9u0IxGRhlJiqIdx46B9e9h777Qjya6uXeGXv4S77oLXX087GhFpCCWGmJYtC08MRx4JzfSp1eqSS6BduzDj2+rVaUcjIvWlr7iYtt4aHn0U/vjHtCPJvq22gpEj4Y03QkslESkt6sdQh3vuCa1szjgjzEUg8Rx1FLz3XugMKCKlRU8MtRg1Ck49Fe6+O3TgkvrJJYVnny2/4clFSpkSQw0efRTOPDNMTPPII6pXaKglS2DIEPjJT5RcRUqFvu6q8e9/wymnhKaX//xnmJhGGqZtW7j66lBx//vfw1dfpR2RiNRFiaEaEydCq1Zh3J/NNks7mtI3fHgYP2nECOjUCW64Ie2IRKQ2SgzVOO+8UHHapUvakTQNZnD//WGe6MGDQwsvgA8+CAMSXnUV/OMfoRXTsmWphioiqFVSJddfD/36wYAB0KZN2tE0LWZhStD999+wbd680GmwasX0U0/BoEFhrofnnw+TASlJixRP5hKDmR0K/AVoDtzm7lclda/bb4dJk8KIqTNnhorSn/40JAZJ3qBB8MknYQiN3L/BrFnQp0/YP24cXHRRWN9ll5AgfvSjkLxFJDnm7mnH8A0zaw58AAwG5gOvASe7+7s1nVNRUeFTpkxp0P1OOy38Iu3ZMyy77RYmm2nVqkGXkwJzh3ffhSefhAkTQrPXZs1g4cLQiS435/T69fDRRyGxtGkDu+8exrQaN27ja3brFhoVNNb8+aH4a8IEmD59w/anngpJbOrUUBw5eHCogK/O2rXhqWn2bNhkE9hxR+jYsX5xfPVVOP8//wnDkeSSqkhdzOx1d6+obl/Wnhj6ATPdfTaAmd0HHAnUmBgaY/RoTWafZWbhi65PnzD+0vLlYfylXFIYPDgkhDlzNgz1/fOfw1/+EuagPuaYja/5u9+FxLB0afgh0L175R8Cw4eH8+bMCU+PVV14IRx6aKgP+dWvQl+NQYOgRYuwPzer3z33wDXXhL9hjz1Cwtp009A6C8IPkFGjKs9417lz+IIHuOACePvtyvfeZRe48cawPnRoSEIffbRh/w9/GJpWQ5gfo+q8IUOGwK9/HdYPOih8hvmOPRbOPRe+/jpcq6rTTgvNjpcurX4ypmHD4MQTQ9IcOnTj/RdcEAagnDEDzj574/2XXBLimjYt/HtX9Yc/hKf5V16pfpDGa68N/7aTJsGVV268/8Ybw2f42GNw3XUb7x81KhRZjh0LN9208f777w9JfvRouPPOjfc//DBsuSXccguMGbPx/okTww+b668PzeHztWoV4gL43//deMSAbbcNcUH4b/ill8Jn8Yc/bHyfQshaYugI5A/YPB/Yp+pBZjYMGAbQpRGFz0oKpaV1azjggLD+9dfQoUPYNmRI+LXdsyf07h32b7pp+NVe1Q47hNfVq8MX/Ny5lcdzyvW1cK9+nKfc/sGDw7ldu1Yf6x//GCYtmjABnntu4zkqvvtd2G67DU+ra9dWbsq7Zs3G91+zZsN6y5Zh+Pfc392ly4aktH59OLfq+flJaPXqjRPDunWV91eVO7+mzyZ3fkP35z7bXPwN3b9uXfX7c39vTefn9jf0/Jyazq9tf34/qbVra/+3z+3P31ZoWStKOg441N1/Gr0/DdjH3c+r6ZzGFCWJiJSr2oqSstZcdQHQOe99p2ibiIgUSdYSw2tALzPrbmabAicB1VQhiohIUjJVx+Dua83sPGACobnq3939nZTDEhEpK5lKDADuPh4Yn3YcIiLlKmtFSSIikjIlBhERqUSJQUREKlFiEBGRSjLVwa0hzGwx8GGCt2gLLEnw+oVQCjGC4iykUogRFGehFTLOru7errodJZ8YkmZmU2rqHZgVpRAjKM5CKoUYQXEWWrHiVFGSiIhUosQgIiKVKDHUbWTaAcRQCjGC4iykUogRFGehFSVO1TGIiEglemIQEZFKlBhERKQSJQbAzA41s3+b2Uwzu7ia/b8ys3fNbJqZPW1mNczblXqcZ5vZ22Y21cxeMLPeWYwz77hjzczNrOjNBGN8lj8xs8XRZznVzKqZ6DP9OKNjToj++3zHzO4pdoxRDHV9ntfnfZYfmNnnKYQZJ84uZjbJzN6M/n8fksEYu0bfQ9PM7Bkz61TwINy9rBfC8N6zgB7ApsBbQO8qxxwIbB6tnwPcn9E4t8pbPwJ4IotxRse1Bp4DXgEqshYj8BPghjT+m6xnnL2AN4Fto/fbZzHOKsefTxhSP3NxEip3z4nWewNzMxjjP4Ch0fpA4K5Cx6EnBugHzHT32e6+GrgPODL/AHef5O65GXlfIcwsV2xx4vwi7+0WQBotC+qMM3IFcDWwspp9SYsbY9rixHkWcKO7fwbg7ouKHCPU//M8Gbi3KJFVFidOB7aK1rcGPipifBAvxt7Av6L1SdXsbzQlBugIzMt7Pz/aVpMzgccTjah6seI0s+FmNgv4E/DzIsWWr844zew7QGd3f6yYgeWJ+29+bPS4PtbMOlezP2lx4twJ2MnMXjSzV8zs0KJFt0Hs/4eiYtjubPhiK6Y4cV4OnGpm8wnzwpxfnNC+ESfGt4BjovWjgdZmtl0hg1BiqAczOxWoAK5JO5aauPuN7t4TuAi4NO14qjKzZsB1wK/TjqUOjwDd3H13YCJwZ8rx1GQTQnHSAYRf4rea2TZpBlSHk4Cx7r4u7UBqcDJwh7t3AoYAd0X/zWbJb4D9zexNYH9gAVDQzzNrf3AaFgD5vwY7RdsqMbODgN8CR7j7qiLFli9WnHnuA45KMqAa1BVna2A34Bkzmwv0B8YVuQK6zs/S3T/N+3e+DdirSLHli/NvPh8Y5+5r3H0O8AEhURRTff7bPIl0ipEgXpxnAmMA3P1loBVh4LpiifPf5kfufoy79yV8J+Hunxc0imJXAGVtIfzimk14vM1V9vSpckxfQoVQr4zH2Stv/XBgShbjrHL8MxS/8jnOZ9khb/1o4JUsfpbAocCd0XpbQjHEdlmLMzpuF2AuUcfajH6ejwM/idZ3JdQxFC3emDG2BZpF6yOAPxQ8jjT+gbK2EB4ZP4i+/H8bbfsD4ekA4CngE2BqtIzLaJx/Ad6JYpxU2xdymnFWObboiSHmZ/nH6LN8K/osd8niZwkYoWjuXeBt4KQsxhm9vxy4Ko346vF59gZejP7dpwIHZzDG44AZ0TG3AS0LHYOGxBARkUpUxyAiIpUoMYiISCVKDCIiUokSg4iIVKLEICIilSgxiIhIJUoMUvaiIbZ3aMT53czsRwWO6QAze7SQ1xSJS4lBJAyx3eDEAHQDCpoYRNKkxCBNUjS50vRouSD6VT89b/9vzOxyMzuOMDDi3dEkMpuZ2Vwz+1M06dFkM9sxOueO6PjcNb6MVq8Cvhed/8sa4nnFzPrkvX/GzCrMrJ+ZvRxNDPOSme1czbmXm9lv8t5PN7Nu0fqpUYxTzewWM2seLXdEx71dU0wiNVFikCbHzPYCTgf2IQzSdxawbXXHuvtYYApwirvv6e5fR7uWufu3gRuA/6vjlhcDz0fnX1/DMfcDJ0TxdSCMxTQFeB/4nocB0X4P/G+8vxLMbFfgRGCAu+9JGGHzFGBPoKO77xb9DaPiXlMEwoBNIk3NfsBD7r4CwMweBL5Xz2vcm/da05d9fYwBngQuIySIsdH2rYE7zawXYZKYFvW45iDCqK+vmRnAZsAiwpDhPczsb8Bj0X1FYlNikHKxDZWfkFvVcbxXs742d41ojP5N497c3ReY2admtjvhV/7Z0a4rgEnufnRUPPRMNad/c98qsRthZNVLqp5gZnsAh0T3OQE4I26sIipKkqboeeAoM9vczLYgDJv9OLC9mW1nZi2BH+Ydv5wwT0S+E/NeX47W57JhXoYj2PDrvrrzq3M/8F/A1u4+Ldq2NRvG2/9JDefNBb4D38x+1z3a/jRwnJltH+1rE00UnxuW+QHCZE3fiRGbyDf0xCBNjru/YWZ3AJOjTbe5+2tm9odo2wJC2X7OHcDNZvY1sG+0bVszmwasIszqBXAr8LCZvQU8AayItk8D1kXb76ilnmEsYWj0K/K2/YlQlHQpodinOg8APzazd4BXCcMt4+7vRuc9GT3BrAGGA18Do/JmHtvoiUKkNhp2W6SKaGa5CndfknYsImlQUZKIiFSiJwaRAjKzQ4Crq2ye4+5HpxGPSEMoMYiISCUqShIRkUqUGEREpBIlBhERqUSJQUREKvn/04K2/bsSx+IAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a figure and an axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the data\n",
    "x = np.array(range(18, 92)) / interval_number\n",
    "ax.plot(x, f_values, 'b--')\n",
    "\n",
    "plt.xlabel('output_values')\n",
    "plt.ylabel('false_test_sample_count')\n",
    "\n",
    "plt.savefig('false_max_max_output_distribution.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_percent = (np.array(f_values) / np.array(values))\n",
    "t_percent = (np.array(t_values) / np.array(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEHCAYAAACjh0HiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5sUlEQVR4nO3deXxU5dn/8c81WUkIAZIAgQAJ+74GFFmUKoL7vvBoH6l7a63WWrW1tdbqr7baPlYrKm7UuuFSlSoqgiCLIosgS9hCCBhACFsgIWSb6/fHmcQQsszALEnmer9e85qZM2fO+WbQuebc9zn3LaqKMcaY8OUKdQBjjDGhZYXAGGPCnBUCY4wJc1YIjDEmzFkhMMaYMBcZ6gAnIjk5WdPT00MdwxhjmpQVK1bsVdWUmsubZCFIT09n+fLloY5hjDFNiohsq225NQ0ZY0yYs0JgjDFhzgqBMcaEOSsExhgT5qwQGGNMmAtoIRCRl0Rkj4isreN1EZEnRSRbRFaLyLBA5jHGGHO8QB8RTAcm1fP6OUBPz+1m4JkA5zHGGFNDQK8jUNUFIpJezyoXAa+oMxb2EhFpLSKpqrorEHmW5+5n/a5D9a4TExnBxUM7ER1prWaNhrsCDu2EA7lwcBuoQlQLiI537l2RzjJ1OzdqDK3udkN5MZQdde7LS3zPEBENMS0hOsG5j4xxllfuyl0OpYVQWuTclx1xMlXnigBXFEREOZkBKkqhogzcZcevXxtXZLVbhPPZuMt/uJnmr8dZ0DbDr5sM9QVlnYDvqj3P8yw7rhCIyM04Rw106dLlhHb2ydrveWHR1gbXKyot5yej/ftBmxpU4WgBFOVDySEoOezcig/CoR1Q8B0U5MHB75zHFaWhTmxM43DVa82uEHhNVacB0wAyMzNPaDadOyf04tYzute7zs2vLOflxbn876h0IlxyIrsx1VWUw96NsOtb57Z3ExTscL7sSwvreJNAQgdITIMOA6HvBdAm3fmPv3UX59dwWbHz67vsiPOrWATE5dwQ53nV5lwQGescPUTGen7N+/hvW1ECJYVO5pLDzlFF1T4EXC6Ibum5xUNUnCdLJfX8ei/zHAGUO6+7Ij1HCFE11q+NHvvr313hOcqodpTg699lmp7YVn7fZKgLwQ6gc7XnaZ5lAdEyJpKWMfX/yTeN7cZPX/uGOet3M7F/h0BFaf6K9sFHv4RNn0L5UWdZVDyk9IaUXtD9R9Cqo/OFH5sIMQmeWytISIXI6NDmNyaMhLoQzAR+LiJvAqcABYHqH/DW2f07kNamBS8u3FprIVBVROxXV722fQXvXA9H9sHwKZCWCalDIKm78wvWGNOoBLQQiMgbwBlAsojkAX8AogBU9VlgFnAukA0cAX4SyDzeiHAJU05L5+GP1rM67yCD0lpXvVZwpIzJzy+hZ/uW/O2KwURGWIfyMdxuWPwEfP4wtOkKN34GqYNDncoY04BAnzU0uYHXFbgtkBlOxFUjOvPEnM28uGgr/7h6KADlFW5ue/0bNu4+TNauQ0RFuPjrZYNwWT+Co/gAvHsTZH8G/S+FC/4RkLZMY4z/2U/aWiTERnHViM58tHoXuwqKAXj4o/Usyt7Lny8ZyJ1n9eSdFXk8/NF61JtT/pq7vdnwwlmQMx/O+xtc/pIVAWOakFD3ETRaU05L5+XFW/nXl9vo3LYF07/M5cYxGVw5ojOqSkFxGS8t3kpiiyjuOKtnqOOGTvZceOcnzlkv1/0Xuo4KdSJjjI+sENShc9s4Jg3owL+/yqWk3M0ZvVP4zbl9ARARfn9ePw4fLef/5myiTXwU/zsqPbSBg83thqXPwae/hZS+MPkNp1/AGNPkWNNQPW4Yk0FRaQXpyfE8OXnoMdcVuFzCo5cO5Mw+7Xj4o/Xk7i0KYdIgUnVOCZ02Dj65D3qdAzfMtiJgTBNmhaAew7q04ZlrhvHqDafQKjbquNcjI1z8+dKBREe4+MPMdc2/v2DrAnjxbHj9SueiqoufhatedYZcMMY0WVYI6iEinDMwlQ6JsXWu065VLL+c0IsvNuXz6brvg5guyLI+gH9d4FwRfP4T8PPlMGSyc0WtMaZJs/+L/eC6UV3p0yGBh/6bxZHSZjjwlyoseAySe8Ht30DmT5xhEYwxzYIVAj+IjHDx8MUD2FlwlCfnZoc6jv/lzIPv18Bpt0NU3UdHxpimyQqBn2Smt+Xy4Wm8sDCH7D2HQx3HvxY/CS3bw6CrQp3EGBMAVgj86L5z+hAXHcGDM7NCHcV/dq12jghOufWHMfiNMc2KFQI/Sm4Zw8/G92BR9l62NpfTSb980hlaOfP6UCcxxgSIFQI/u2hIRwBmrQnpIKr+cXA7rP2PM4Joi9ahTmOMCRArBH6WmtiC4V3b8OHqZlAIvprqTL5y6k9DncQYE0BWCALg3IGprN91iJz8umbgagKKD8A3r8CAy52ZwowxzZYVggA4d6AzoU2Tbh6a/yiUFcHoX4Q6iTEmwKwQBEBl89BHa5rolcbfvAJfPwsjb4H2/UOdxhgTYFYIAqTJNg/lLoIP73LmFJ74/0KdxhgTBFYIAqRJNg/tz4EZP4Y26XD5yxBho5QbEw6sEARIkzt76GgBvH41qBv+Z4adLmpMGLFCEEDnDkxlw/eH2dIUmodm3QP7t8BV/4ak7qFOY4wJIisEAVTVPFTtqOBIaTlr8goa19wFB7+DNW87w0hkjAt1GmNMkFkjcABVNg+9t2oHFap8mb2Pld8doKxCee7Hw5nYv0OoIzqWPufcn3JLaHMYY0LCjggC7IJBqeTkF/GPuZspLqvg+jEZJMREMn/jnlBHc5QchhX/gn4XQesuoU5jjAkBOyIIsGtO7Uqv9gn069iK1nHRAGzNL2Lh5r2oKiLSwBYCbOWrUHIIRv08tDmMMSFjRwQBFhXh4rQeyVVFAGBsz2TyDhSzbd+RECYD3BWwZCp0PhXShoc2izEmZKwQhMCYnikALMzeG9ogGz50RhgddVtocxhjQsoKQQikJ8XRqXULFm3OD22Qr56G1l2hz3mhzWGMCSkrBCEgIozpkcyXW/ZRXuEOTYjvlsF3X8OpPwNXRGgyGGMaBSsEITKmZzKHj5azekdBaAIseRpiEmHotaHZvzGm0bBCECKjeyQjAos3h6CfwO2G7Lkw4BKIaRn8/RtjGhUrBCHSNj6a/h1bhabD+MBW55TRTnamkDEmCIVARCaJyEYRyRaR+2p5vYuIzBORlSKyWkTODXSmxmJMjxRWbj9AUUl5cHe8c6VznzokuPs1xjRKAS0EIhIBPA2cA/QDJotIvxqr/Q54S1WHAlcDUwOZqTEZ2zOZsgrl6637grvjnSshIgba9Q3ufo0xjVKgjwhGAtmqmqOqpcCbwEU11lGgledxIrAzwJkajeFd2xAT6WJhsPsJdn0LHQZARFRw92uMaZQCXQg6Ad9Ve57nWVbdg8C1IpIHzAJur21DInKziCwXkeX5+SE+/95PYqMiGJnRlkXBLARuN+xcZc1CxpgqjaGzeDIwXVXTgHOBf4vIcblUdZqqZqpqZkpKStBDBsqYHsls3lPI9wVHg7PD/TlQehg6Dg3O/owxjV6gC8EOoHO152meZdXdALwFoKpfAbFAcoBzNRpjejp/6qJgnT20a5Vz33FIcPZnjGn0Al0IlgE9RSRDRKJxOoNn1lhnO3AmgIj0xSkEzaPtxwt9O7SifasYZq/7Pjg7rOwoTukTnP0ZYxq9gBYCVS0Hfg58CqzHOTtonYg8JCIXelb7FXCTiHwLvAFM0UY1fVdguVzCOQNSmb8pn8JgnEa6cxV0GGgdxcaYKgGfj0BVZ+F0Aldf9kC1x1nA6EDnaMzOHZjK9C9zmbt+NxcNqdmX7kdut3PG0OCrArcPY0yT0xg6i8NeZtc2tEuI4eM1AW4e2r/F6Si2M4aMMdVYIWgEnOahDszbuCewVxnvXOXc2xlDxphqrBA0EucOTKWk3M3nGwI4l/HOlRAZax3FxphjWCFoJDLT25KSEMOsNbsCt5Ndq6D9AIiwqaqNMT+wQtBIRLiESf2d5qEjpQFoHqrsKLZmIWNMDVYIGpFzB6ZytMzNvA0BuIxiXzaUFtqFZMaY4/hUCERkjIj8xPM4RUQyAhMrPI3MaEtyywA1D1VeUWxnDBljavC6EIjIH4B7gd94FkUBrwYiVLiKcAmTBrTn8w17KC6t8O/GraPYGFMHX44ILgEuBIoAVHUnkBCIUOHs3IGpFJdVMG+jn88eqrqi2DqKjTHH8qUQlHqGflAAEYkPTKTwdkpGEskto/lkrR8vLisvdTqKrVnIGFMLXwrBWyLyHNBaRG4C5gDPByZW+IpwCeN6pbBwcz4Vbj8NubT5Uygrgh5n+Wd7xphmxetCoKqPA+8A7wK9gQdU9alABQtnp/dK4cCRMtbsKPDPBle+Ci07WCEwxtTKpwZjVf0M+CxAWYzH2J4piMCCTfkM6dz65DZ2aBdsng2j77D+AWNMrXw5a+iwiByqcftORN4TkW6BDBlu2sZHM6hTIl9s8sP1BN++AeqGoT8++W0ZY5olX/oIngB+jTPncBpwN/A6zoT0L/k9WZg7vVcKK7cfoOBI2YlvRNVpFupyGiR19184Y0yz4kshuFBVn1PVw6p6SFWnARNVdQbQJkD5wtbpvVNwKyzechJTWG7/yhl6epgdDRhj6uZLITgiIleKiMtzuxKonHE9bGYUC5bBaa1JiI3ki40n0Ty08lWIToB+F/kvmDGm2fGlEFwD/BjYA+z2PL5WRFrgTEdp/CgywsXYnsks2JzPCc3cWXIY1r0HAy6FaLvkwxhTN69PI1HVHOCCOl5e5J84prrTe6Uwa833bN5TSK/2Pl7EvfY/UHbEOomNMQ3yuhCISCxwA9AfiK1crqrXByCXAcb1SgHgi435vheCla864wqlZQYgmTGmOfGlaejfQAdgIvAFzplDhwMRyjhSE1vQq31LFmz2sZ9g3xbIWwpDrgGRwIQzxjQbvhSCHqr6e6BIVf8FnAecEphYptLpvVL4Ome/b5PV5Mxz7vucF5hQxphmxZdCUHlC+0ERGQAkAu38H8lUN65XCqUVbr7O2e/9m3IXQatO0Nau8zPGNMyXQjBNRNoAvwNmAlnAXwKSylQZkd6W2CiX91cZqzqFIH2sNQsZY7ziy+Azc1X1ALAA6AZgM5QFXmxUBKO6JTF/4x6cfvoG5G+AonxIHxPwbMaY5sGXI4J3a1n2jr+CmLr9qE87cvcdYUt+YcMrb13o3GeMDWwoY0yz0WAhEJE+InIZkCgil1a7TaHaaaQmcH7Utz0Ac9fvbnjl3AWQ2AXapAc2lDGm2fDmiKA3cD7QGueCssrbMOCmgCUzVTq1bkHf1FbMWd/A9JVut9M/YEcDxhgfNNhHoKofAB+IyChV/SoImUwtzuzTjme+2MLBI6W0jouufaU9WVB8wOkoNsYYL/nSR5AtIr8VkWki8lLlLWDJzDHO7NuOCrfWf/ZQrqd/wDqKjTE+8KUQfIBz7cAc4KNqt3qJyCQR2Sgi2SJyXx3rXCkiWSKyTkRe9yFT2Bic1prkltH1Nw9tXej0DbTuHLRcxpimz5fTR+NU9V5fNi4iEcDTwAQgD1gmIjNVNavaOj2B3wCjVfWAiNhFarVwuYTxvdvxybrvKatwExVRo4a7K2DbIuh7YWgCGmOaLF+OCD4UkXN93P5IIFtVc1S1FGc2s5qD498EPO25RgFVbaBHNHyd2bc9h4+Wszz3wPEvfr8GjhZAxrjgBzPGNGm+FII7cIrBUc98xYdF5FAD7+kEfFfteZ5nWXW9gF4islhElojIpNo2JCI3i8hyEVmen++HuXyboLE9k4mOcNV+GmlV/4B1FBtjfON1IVDVBFV1qWqsqrbyPG/lhwyRQE/gDGAy8LyItK5l/9NUNVNVM1NSUvyw26YnPiaSU7snMXdDLQdNWxdCUg9olRr8YMaYJs3rQiCOa0Xk957nnUVkZANv2wFU77lM8yyrLg+YqaplqroV2IRTGEwtzurbjq17i8ipfpVxRbkzP7GdLWSMOQG+dBZPBdzAj4A/AYU4HcEj6nnPMqCnZ0yiHcDVwP/UWOd9nCOBl0UkGaepKMeHXGHlR33a8cAH65i7fg/dUlo6C7//FkoOWbOQ8UpZWRl5eXkcPXq04ZVNkxQbG0taWhpRUVFere9LIThFVYeJyEoAzxk+dVzZ5FDVchH5OfApEAG8pKrrROQhYLmqzvS8draIZAEVwK9VdZ8PucJKWps4+nRIYM763dw0zjPM9M5Vzn3nhg7QjIG8vDwSEhJIT09HbITaZkdV2bdvH3l5eWRkeDcuqC+FoMxzOqgCiEgKzhFCQ6FmAbNqLHug2mMF7vLcjBdO753CCwu3/nAa6YGtEBENrdJCHc00AUePHrUi0IyJCElJSfhyUo0vZw09CbwHtBORR3AmrP9/vkU0/tA9pSUVbmXHgWJnwYFcaN0VXL78c5pwZkWgefP139frIwJVfU1EVgBnAgJcrKrrfYtn/KFr2zgAtu0/QnpyPOzPhbY2NYQx5sT4ctbQqcAOVX1aVf8J7BARm7M4BLomxQOwff8RZ0ayA1uhjRUC03Q8+eST9O3bl2uuuabW1+fPn8/5558f5FSB98QTT3DkyJFQxziOL20Jz+CcKVSp0LPMBFm7hBiiI11s31cER/ZBaaHNP2CalKlTp/LZZ5/x2muvhTpKg8rLy/22rcZaCHzpLBZPxy4AquoWEV/eb/zE5RK6tI1j274jsH+rs9CahswJ+ON/15G1s6EBAnzTr2Mr/nBB3dOq3nrrreTk5HDOOedw7bXX8v7773P06FFatGjByy+/TO/evY9Z/4svvuCOO+4AnLbvBQsWkJCQwGOPPcZbb71FSUkJl1xyCX/84x9r3V9ubi6TJk1i+PDhfPPNN/Tv359XXnmFuLg4VqxYwV133UVhYSHJyclMnz6d1NRUzjjjDIYMGcKiRYuYPHky48aN44477qCoqIiYmBjmzp1LXFwc9913H/Pnz6ekpITbbruNW265hfnz5/Pggw+SnJzM2rVrGT58OK+++ipPPfUUO3fuZPz48SQnJzNv3jx++tOfsmzZMoqLi7n88sur/oZZs2Zx1113ER8fz+jRo8nJyeHDDz+kqKiI22+/nbVr11JWVsaDDz7IRRfVHLXHd758keeIyC/44SjgZ9j5/iHTtW2c0zR04HtngTUNmSbi2Wef5ZNPPmHevHlER0fzq1/9isjISObMmcNvf/tb3n332FlxH3/8cZ5++mlGjx5NYWEhsbGxzJ49m82bN7N06VJUlQsvvJAFCxYwblztY21t3LiRF198kdGjR3P99dczdepU7rjjDm6//XY++OADUlJSmDFjBvfffz8vveSMrl9aWsry5cspLS2lT58+zJgxgxEjRnDo0CFatGjBiy++SGJiIsuWLaOkpITRo0dz9tlnA7By5UrWrVtHx44dGT16NIsXL+YXv/gFf//735k3bx7JyckAPPLII7Rt25aKigrOPPNMVq9eTa9evbjllltYsGABGRkZTJ48uerveOSRR/jRj37ESy+9xMGDBxk5ciRnnXUW8fHxJ/Vv4kshuBXnzKHf4ZxCOhe4+aT2bk5Y57ZxfJWzD92/FQFo0zXUkUwTVN8v92AoKCjguuuuY/PmzYgIZWVlx60zevRo7rrrLq655houvfRS0tLSmD17NrNnz2bo0KEAFBYWsnnz5joLQefOnRk9ejQA1157LU8++SSTJk1i7dq1TJgwAYCKigpSU38YouWqq64CnCKSmprKiBHOtbOtWjkj68yePZvVq1fzzjvvVP0tmzdvJjo6mpEjR5KW5pzOPWTIEHJzcxkz5vgr/9966y2mTZtGeXk5u3btIisrC7fbTbdu3aquAZg8eTLTpk2r2ufMmTN5/PHHAedU4O3bt9O3b1+vP/PaeFUIPNcP/J+qXn1SezN+0zUpjiOlFZTkbyE2IRWiWoQ6kjE++/3vf8/48eN57733yM3N5Ywzzjhunfvuu4/zzjuPWbNmMXr0aD799FNUld/85jfccsstXu2n5umUIoKq0r9/f776qvaJFxv6la2qPPXUU0ycOPGY5fPnzycmJqbqeURERK39DFu3buXxxx9n2bJltGnThilTpjR4tbeq8u677x7XfHayvOosVtUKoGtDVxKb4Oma5JxCWrY3x5qFTJNVUFBAp07OgMTTp0+vdZ0tW7YwcOBA7r33XkaMGMGGDRuYOHEiL730EoWFzvkrO3bsYM+eukew3759e9UX/uuvv86YMWPo3bs3+fn5VcvLyspYt27dce/t3bs3u3btYtmyZQAcPnyY8vJyJk6cyDPPPFN1FLNp0yaKiorq/XsTEhI4fPgwAIcOHSI+Pp7ExER2797Nxx9/XLW/nJwccnNzAZgxY0bV+ydOnMhTTz1FZXftypUr692ft3zqIwAWi8hMoOqvVdW/+yWJ8UkXz7UEkQXboPdZIU5jzIm55557uO6663j44Yc577zzal3niSeeYN68ebhcLvr3788555xDTEwM69evZ9SoUQC0bNmSV199lXbtap/Xqnfv3jz99NNcf/319OvXj5/+9KdER0fzzjvv8Itf/IKCggLKy8u588476d//2Oay6OhoZsyYwe23305xcTEtWrRgzpw53HjjjeTm5jJs2DBUlZSUFN5///16/96bb76ZSZMm0bFjR+bNm8fQoUPp06fPMU1XLVq0YOrUqUyaNIn4+PiqJilwjqDuvPNOBg0ahNvtJiMjgw8//NDbj7tOUu1EoPpXFPlDbctVtfau+gDKzMzU5cuXB3u3jcrRsgoG/34mG2OnwPj74fR7Qh3JNBHr168/6TblpiQ3N5fzzz+ftWvXhjqK1woLC2nZsiWqym233UbPnj355S9/6dM2avt3FpEVqppZc11friz+o2dDcara+E6EDTOxUREMTSiAMqxpyJhm5vnnn+df//oXpaWlDB061Ou+kBPldSEQkVHAi0BLoIuIDAZuUdWfBSqcqd/QlgfgAHYxmTHAvn37OPPMM49bPnfu3CZ1NADwy1/+0ucjgJPhSx/BE8BEYCaAqn4rIjZBbgj1idnrPLCLyYwhKSmJVatWhTpGk+TTcJWq+l2NRRV+zGJ8lO7Kp1BjKY5sHeooxpgmzJdC8J2InAaoiESJyN2AjT4aQu0rdrFd27O9cjhqY4w5Ab4UgluB24BOwE5giOe5CZHWR3ewTds5Q00YY8wJ8roQqOpeVb1GVduraoqqXmtTSoaQ201M4Xds13Zs21f/RSzGNCYHDx5k6tSpoY4RFKtWrWLWrFkNrxhivsxH0E1E/isi+SKyR0Q+EJFugQxn6nF4J1JRyu6IVDsiME1KXYXAn8M9nwx/5mh2hQB4HXgLSAU6Am8DbwQilPHCgVwAShK6WiEwTcp9993Hli1bGDJkCCNGjGDs2LFceOGF9OvXj9zcXAYMGFC17uOPP86DDz4IOENNVA4nPXbsWDZs2FDnPqZMmcKtt95KZmYmvXr1qrr6tqKigl//+teMGDGCQYMG8dxzzwHO+EDVc1RUVHD33XczYMAABg0axFNPPQXAihUrOP300xk+fDgTJ05k165dAJxxxhnce++9jBw5kl69erFw4UJKS0t54IEHmDFjBkOGDGHGjBksXbqUUaNGMXToUE477TQ2btwIwJEjR7jyyivp168fl1xyCaeccgqVF83Onj2bUaNGMWzYMK644oqqYTX8yZfTR+NU9d/Vnr8qIr/2dyDjJc88BK6kdLbvs0JgTtDH98H3a/y7zQ4D4ZxH63z50UcfZe3ataxatYr58+dz3nnnsXbtWjIyMqrG16nNzTffzLPPPkvPnj35+uuv+dnPfsbnn39e5/q5ubksXbqULVu2MH78eLKzs3nllVfqHDr6m2++qcrxzDPPkJuby6pVq4iMjGT//v2UlZXVO2x1eXk5S5cuZdasWfzxj39kzpw5PPTQQyxfvpx//vOfgDO+0MKFC48bdnvq1Km0adOGrKws1q5dy5AhQwDYu3cvDz/8MHPmzCE+Pp6//OUv/P3vf+eBBx7w8R+lfr4Ugo9F5D7gTZxhqK8CZolIWwBV3e/XZKZ+B7aCRNCyXQbfZX9HhVuJcNmE5KbpGTlyZNWQy3UpLCzkyy+/5IorrqhaVlJSUu97rrzySlwuFz179qRbt25s2LChwaGjK3PMmTOHW2+9lchI5yuybdu2rF27tt5hqy+99FIAhg8fXmdBq2vY7UWLFlVNvlN5FAKwZMkSsrKyqsYhKi0trRpfyZ98KQRXeu5rXut8NU5hsP6CYDqQC607k5bcirIK5ftDR+nU2oaiNj6q55d7sFQf7jkyMhK32131vHJYZrfbTevWrX26YKyuoafrGjram2Gn6xu2unLo6bqGnQbvht2uuc8JEybwxhuBbYX35ayhjHpu3URkQiCDmhr2b4U26XRt6/zHa2cOmaai+lDMNbVv3549e/awb98+SkpKqtr2W7VqRUZGBm+//TbgfEF+++239e7n7bffxu12s2XLFnJycujdu7fXQ0dPmDCB5557ruoLff/+/V4PW13f31rXsNujR4/mrbfeAiArK4s1a5zmulNPPZXFixeTnZ0NQFFREZs2bap3nyfCpyuLG/AXP27LNOTAVmiTUTUvgfUTmKYiKSmJ0aNHM2DAAH7962O7GaOionjggQcYOXIkEyZMoE+fPlWvvfbaa7z44osMHjyY/v3788EHH9S7ny5dujBy5EjOOeccnn32WWJjY7nxxhvp168fw4YNY8CAAdxyyy21/nq/8cYb6dKlC4MGDWLw4MG8/vrrVcNW33vvvQwePJghQ4bw5Zdf1pth/PjxZGVlVXUW33PPPfzmN79h6NChx+z3Zz/7Gfn5+fTr14/f/e539O/fn8TERFJSUpg+fTqTJ09m0KBBjBo1qt5O8hPl9TDUDW5IZKWqDvXLxhoQ9sNQFx+Ev3SFCQ9Rfurt9Pn9J9w8rhv3TOrT4FuNCYdhqKdMmcL555/P5ZdfHuooXqmoqKCsrIzY2Fi2bNnCWWedxcaNG4mOPvG5wAIyDLUX/FNRTMM8p47SJp3ICBed2rRgm51CakyTdeTIEcaPH09ZWRmqytSpU0+qCPjKn4XABEtVIXDOcOjSNs6ahkxYeuSRR6r6DSpdccUVdU572VglJCQQylYOfxaCXD9uy9Rnf45z75mHoEvbOFbn7QpdHmNC5P777+f+++8PdYwmr8FCICKX1ve6qv7Hc1/vesaPti2Gtt0gthXgTGRfUFxGwZEyEuOiQhzOGNPUeHNEcEE9rynwHz9lMd4oKYStC2DETVWLerZLAGDdzgJO65EcqmSmCVHV486zN82HrycBNVgIVPUnJ5wGEJFJwD+ACOAFVa31ChYRuQx4BxihqmF8SlADcuZDRSn0nlS1aHh6G1wCS3L2WSEwDYqNjWXfvn0kJSVZMWiGVJV9+/YRGxvr9Xt86iMQkfOA/kDVHlT1oXrWjwCeBiYAecAyEZmpqlk11ksA7gC+9iVPWNr0McQkQpcfLjNvFRvFwE6JfJVjo4KbhqWlpZGXl0d+fn6oo5gAiY2NJS0tzev1fZm8/lkgDhgPvABcDixt4G0jgWxVzfFs403gIiCrxnp/wrkgzQaxq4/bDZtmQ48zIeLYvoBTuyfx0qKtFJdW0CI6IkQBTVMQFRXV4Ng+Jrz4cmXxaar6v8ABVf0jMAro1cB7OgHV5znO8yyrIiLDgM6q+lF9GxKRm0VkuYgsD9tfMjtXQtEe6H3OcS+N6pZEWYWyfJuN/WeM8Y0vhaByYtwjItIRKMOZm+CEiYgL+Dvwq4bWVdVpqpqpqpkpKSkns9uma9PHIBHQ46zjXspMb0uES1hizUPGGB/50kfwoYi0Bh4DvsE5Y+j5Bt6zA+hc7XmaZ1mlBGAAMN/TadUBmCkiF1qHcS02fgJdToW4tse91DImkkFpiXy1xQqBMcY3vow++idVPaiq7wJdgT6q2tDsCMuAniKSISLROENWz6y2zQJVTVbVdFVNB5YAVgRqU5AHu9dAr4l1rjKqWxKr8wooKmkcU/4ZY5oGX+YsXi0ivxWR7qpaoqoFDb1HVcuBnwOfAuuBt1R1nYg8JCIXnnjsMLTpE+e+1/H9A5VGdU+i3K0sy7V+AmOM93xpGroAZ1ayt0TEDczA+WLfXt+bVHUWMKvGslqPJFT1DB/yhJeNnzhXEyf3rHOVzK5tiYoQvsrZxxm92wUxnDGmKfOlaWibqv5VVYcD/wMMArYGLJn5QWmRczVxr0lQzwVALaIjGNK5NUty7IjAGOM9nyamEZGuInIPzrzFfYB7ApLKHCtnPlSUOIWgAaO6JbF2RwGHj5YFPpcxplnwpY/ga+A9z3uuUNWRqvq3gCUzP9j0iXM1cdfTGlz11G5JVFg/gTHGB14VAs/5/v9R1WGq+mjllcImCFRh8xzofsZxVxPXZljXNkRHuOw0UmOM17wqBKrqBq4IcBZTmz3r4fDOWi8iq01sVARDu7S2cYeMMV7zpY9gjojcLSKdRaRt5S1gyYxjy1znvvuZXr9lVPck1u08REGx9RMYYxrmSyG4CrgNWACs8Nzswq9Ay54D7fpBYqeG1/UY1S0JVVicvTeAwYwxzYUvp49m1HLrFshwYa+0CLZ9Cd1/5NPbhndtQ/tWMby7Ii9AwYwxzYkvZw3FicjvRGSa53lPETk/cNEMuYudSWi87B+oFBnh4rJhaczbuIfdh44GKJwxprnwpWnoZaAUqDyHcQfwsN8TmR9kz4GouGMmofHWlZmdcSu8+40dFRhj6udLIeiuqn/FGX4aVT0C2Dx3gZQ9B9LHQJT3U85VSk+OZ2RGW95enufz/KXGmPDiSyEoFZEWOMNPIyLdgZKApDKwfyvs3+Jzs1B1V2Z2ZuveIpblHvBjMGNMc+NLIXgQ+AToLCKvAXOBewMRyvDDaaMnUQjOHdiBljGRvLX8u4ZXNsaELV/OGpoNXApMAd4AMlV1XoBymey50LqrM+LoCYqLjuSCwal8tHqXjT1kjKmTL2cNzVXVfar6kap+qKp7RWRuIMOFrfJSZ7TRHmfVO9qoN67I7ExxWQUfrd7lp3DGmOamwUIgIrGeK4iTRaRNtauK06kxEb3xk+++htJC6OH91cR1Gdq5NT3atWSGNQ8ZY+rgzRHBLThXEffhhyuKVwAfAP8MXLQwlj0HXJGQMe6kNyUiXJXZmZXbD7J592E/hDPGNDcNFgJV/YeqZgB3q2q3alcVD1ZVKwSBkD0HOp8KMQl+2dwlwzrhEvjQmoeMMbXweqpKVX1KRE4D0qu/T1VfCUCu8LVvC+xeCxP/n982mdwyhh7tWrI676DftmmMaT68LgQi8m+gO7AKqPAsVsAKgT9lve/c973Qr5sd0CmRBZv2oqrISXZAG2OaF18mr88E+qldphpYWR9Ap0xo3dmvmx3UKZH/fLOD7w8dJTWxhV+3bYxp2ny5oGwt0CFQQQzO1cS7voX+F/t90wPTEgFYk1fg920bY5o2X44IkoEsEVlKtaElVNW/bRjhrLJZqN9Fft90v9REXAJrdxRwdn+r58aYH/hSCB4MVAjjse596DQcWnfx+6ZbREfQs10Cq3fYEYEx5li+nDX0RSCDhL39W2HXKpjwUMB2MaBTIl9s2mMdxsaYY3hzZfEiz/1hETlU7XZYRA4FPmKYyPrAuQ9As1ClQWmJ7C0s5XubrMYYU02DRwSqOsZz75+rm0ztsj6AjkOhTXrAdjGgk9NhvDqvwM4cMsZU8eWsIRMoB7bBzm+g38UB3U2/1FZEuIS11k9gjKnGCkFjEIRmIajsMG7JGisExphqrBA0BlnvQ+pgaJsR8F0N6JTImrwCm77SGFMl4IVARCaJyEYRyRaR+2p5/S4RyRKR1SIyV0S6BjpTo3L4e9ixwu9DStRlUFoi+4pK2VVgHcbGGEdAC4GIRABPA+cA/YDJItKvxmorcWY7GwS8A/w1kJkanZz5zv1JTEnpi8oOY2seMsZUCvQRwUggW1VzVLUUeBM4piFcVeep6hHP0yVAWoAzNS5b5kFcEnQYFJTdVXYY21ATxphKgS4EnYDqU2PlUf+sZjcAH9f2gojcLCLLRWR5fn6+HyOGkKpzRJBxOriC010TG2UdxsaYYzWazmIRuRZnhNPHantdVaepaqaqZqakpAQ3XKDsWQ+F30P38UHd7cBOiazZYR3GxhhHoAvBDqD6eMppnmXHEJGzgPuBC1W1pObrzVZl/0C34BaCQWmJ7C8qZad1GBtjCHwhWAb0FJEMEYkGrgZmVl9BRIYCz+EUgT0BztO45MyDpB5+n3ugIVUdxtZPYIwhwIVAVcuBnwOfAuuBt1R1nYg8JCKV50s+BrQE3haRVSIys47NNS/lpZC7OOhHAwB9U1sR6RLW7DgY9H0bYxofX4ahPiGqOguYVWPZA9UeB+e8ycYmbymUFUG3M4K+69ioCAZ0SuSzrN3cfXZvG4nUmDDXaDqLw86WeSARkDE2JLu/9tSubNpdyMLNe0Oyf2NM42GFIFRy5jmT0MQmhmT3FwxOJSUhhhcWbQ3J/o0xjYcVglAoPgA7Vwb9tNHqYiIjuG5UVxZsymfj94dDlsMYE3pWCEJh60JQd0g6iqv7n1O6Ehvl4iU7KjAmrFkhCIWceRCdAGmZIY3RNj6ay4al8d6qHeQfDp/LN4wxx7JCEGhbF8BjPeDVy2Dxk7DrW6ejOH0MRESFOh3Xj8mgtNzNq0u2hTqKMSZErBAE2orpUHYUDm6Hz34Pz42DA1tD2j9QXfeUlpzVtx2vLtnG0bKKUMcxxoSAFYJAKiuGjZ/AwMvg58vgrvVwyXNw2u0w8IpQp6tyw5hu7Csq5f2Vx43+YYwJAwG/oCysbf7MuWisci7iVh1h8NUhjVSbU7u1pX/HVjz1eTZn9+9A2/joUEcyxgSRHREEUtb7zlwD6aG5aMxbIsLDFw8gv7CEW19dQWm5O9SRjDFBZIUgUCqbhfpeABGN/8BraJc2PHb5IJZu3c/v3l9jQ1QbE0Ya/zdUU1XZLNT/klAn8dpFQzqxZU8hT36eTa/2Cdw4tluoIxljgsAKQaCse89pFuo6JtRJfHLnWb3Izi/kkVnryUiO58y+7UMdyRgTYNY0FAhlxbDpU+h7YZNoFqrO5RL+dsUQBnRM5M43V1FwpCzUkYwxAWaFIBCqmoUuDnWSE9IiOoK/XDaIwyXlvPq1XWhmTHNnhSAQmmizUHX9OrZiXK8Upn+ZaxeaGdPMWSHwtybcLFTTLeO6kX+4xC40M6aZs0Lgb028Wai607on0b9jK6YtzMHtttNJjWmurBD4k7sCFv4NEjo26WahSiLCLad3Jye/iDnrd4c6jjEmQKwQ+NOKl2HXKjj7T02+WajSuQM6kNamBdMW5IQ6ijEmQKwQ+EthPsx9CDLGwYDLQp3GbyIjXNw4JoPl2w6wYtv+UMcxxgSAFQJ/mfMHKD0C5z4OIqFO41dXjuhM67gonvvCjgqMaY6sEPjD9iWw6jUYdRuk9A51Gr+Li47kx6d25bP1u3lw5jr2HD4a6kjGGD9qHg3ZgVRaBNHxdb9eUQ4f/QpapcHp9wQvV5Ddenp39haW8O8l23hz2XauOy2dW8d1p40NWW1Mk2dHBPU5/D38rS/8906oazTOr5+B3Wth0p/rLxhNXHxMJH++dBBz7zqdcwakMm1BDmP/Oo8PVtk1BsY0dVYI6rP0eSgpcM4G+vrZ41/PmgmfPQC9z3WGmw4D6cnx/N9VQ/j0znH0TU3gjjdX8dinG+w6A2OaMCsEdSkrhuUvOV/yfc6HT38L2XN+eH3LPHj3Bug0HC59vtl1EDekV/sEXrvxVK4e0Zmn523hlldXUFRSHupYxpgTYIWgLt++CcX7YdTPnXmG2/WDt6+H/E3w3VJ48xpI6gnXvA0xLUOdNiSiI138+dKB/OGCfsxdv5vLnvmS7/YfCXUsY4yPrBDUxu2GJVMhdQh0Pc35op/8BkRGw2uXO7eE9vDj96BFm1CnDSkR4SejM5j+k5HsPFjMeU8u5NN134c6ljHGB1YIarNlLuzd5JwOWtnk07oLXPUaHN4FUfHw4/edYmAAGNcrhQ9vH0t6cjy3/HsFf/zvOpv72Jgmwk4frc1X/3TGC+p38bHLu5wCN30OccnQKjUk0RqzLklxvH3rKB79eAMvL87lm20HeOCCfvTvmEhsVESo4xlj6hDwQiAik4B/ABHAC6r6aI3XY4BXgOHAPuAqVc0NdK467V4HOfPhzD84TUE1dRgY9EhNSUxkBH+4oD+nZCTx63e+5bJnviLSJfRsn8CAjq3o3SGB1MQWpLaOJTUxlnYJsUS4wquj3ZjGJqCFQEQigKeBCUAesExEZqpqVrXVbgAOqGoPEbka+AtwVSBz1WvJVIiKg+FTQhahOZg0oAMj0tuwLHc/a3YUsHbHIT7fsIe3V+Qdt258dAQtYyNpGRNJqxZRpCbG0jGxBR1bO7fObVvQpW0cCbFRIfhLjGn+An1EMBLIVtUcABF5E7gIqF4ILgIe9Dx+B/iniIhqXVdwnYRlL0DWB/Wvs30JDPtfiGvr992Hm6SWMUwakMqkAU4zmqpyqLicnQXF7CooZufBo+QfLqGopJzCknIOl5RzqLiMjd8fZt6GfIprzIzWNj6azm1a4HIJxaUVFJdVcKS0ArdbEXE6rl0CghxzNq9LhJgoF7GREcRGuYiOdOHy4XRfVSitcFflLCopP6b/o3K/8TGRzi06ghbREcfsQxUq3EqZ2015hVJW4cYlQlSEEBnhItIlDR4ZuVWpcCvlbue+wq1EeN4X6RIiXa5wO4s5LP1yQi9GpPv3+ynQhaAT8F2153nAKXWto6rlIlIAJAF7q68kIjcDNwN06dLlxNK43VDRwGTsXU+D0Xec2PZNvUSExLgoEuOi6Jvaqt51VZWDR8rYcbCY7fuPVN0qT09NbhlDXHQEcdERRLgEtzpftqqKu8ZviHK3UlLupqSsgqNlbkrKK6jAt98ZsVEukuLjaOn5so+OdCFQtZXyCjdHSp3CVFhSTnFpBRUc21ke4RJaRkU6X9oRLlShrMJNudtNmac41Pv54XzZx0ZJVeGoLAzlFUq52zrnw0EAfiI3nc5iVZ0GTAPIzMw8sY/ilJudm2n0RIQ28dG0iY9mQKfEUMcxplkL9OmjO4DO1Z6neZbVuo6IRAKJOJ3GxhhjgiDQhWAZ0FNEMkQkGrgamFljnZnAdZ7HlwOfB6R/wBhjTK0C2jTkafP/OfApzumjL6nqOhF5CFiuqjOBF4F/i0g2sB+nWBhjjAmSgPcRqOosYFaNZQ9Ue3wUuCLQOYwxxtTOhpgwxpgwZ4XAGGPCnBUCY4wJc1YIjDEmzElTPFNTRPKBbQHcRTI1rmxuhJpCRrCc/tQUMoLl9Cd/Z+yqqik1FzbJQhBoIrJcVTNDnaM+TSEjWE5/agoZwXL6U7AyWtOQMcaEOSsExhgT5qwQ1G5aqAN4oSlkBMvpT00hI1hOfwpKRusjMMaYMGdHBMYYE+asEBhjTJgL20IgIpNEZKOIZIvIfbW8fpeIZInIahGZKyJdG2nOW0VkjYisEpFFItKvMeastt5lIqIiEvTT9rz4LKeISL7ns1wlIjcGO6M3OT3rXOn573OdiLwe7IyeDA19nv9X7bPcJCIHG2HGLiIyT0RWev5fPzfYGb3M2dXzPbRaROaLSJpfA6hq2N1whsTeAnQDooFvgX411hkPxHke/xSY0Uhztqr2+ELgk8aY07NeArAAWAJkNraMwBTgn6H4b9LHnD2BlUAbz/N2jTFnjfVvxxmGvlFlxOmM/anncT8gtzF+lsDbwHWexz8C/u3PDOF6RDASyFbVHFUtBd4ELqq+gqrOU9UjnqdLcGZXCzZvch6q9jQefJyM1z8azOnxJ+AvwNFghvPwNmOoeZPzJuBpVT0AoKp7gpwRfP88JwNvBCXZD7zJqEDlBNqJwM4g5qvkTc5+wOeex/Nqef2khGsh6AR8V+15nmdZXW4APg5ootp5lVNEbhORLcBfgV8EKVt1DeYUkWFAZ1X9KJjBqvH23/wyz+H3OyLSuZbXA82bnL2AXiKyWESWiMikoKX7gdf/D3maVTP44YssWLzJ+CBwrYjk4cybcntwoh3Dm5zfApd6Hl8CJIhIkr8ChGsh8JqIXAtkAo+FOktdVPVpVe0O3Av8LtR5ahIRF/B34FehztKA/wLpqjoI+Az4V4jz1CUSp3noDJxf2s+LSOtQBmrA1cA7qloR6iC1mAxMV9U04Fyc2RIb4/fi3cDpIrISOB1nrne/fZ6N8Q8Ohh1A9V97aZ5lxxCRs4D7gQtVtSRI2arzKmc1bwIXBzJQHRrKmQAMAOaLSC5wKjAzyB3GDX6Wqrqv2r/zC8DwIGWrzpt/8zxgpqqWqepWYBNOYQgmX/7bvJrgNwuBdxlvAN4CUNWvgFicgd6CyZv/Nneq6qWqOhTnOwlVPei3BMHuGGkMN5xfVDk4h6uVnTP9a6wzFKcDp2cjz9mz2uMLcOaCbnQ5a6w/n+B3FnvzWaZWe3wJsKQxfpbAJOBfnsfJOM0KSY0tp2e9PkAunotXG1tGnCbfKZ7HfXH6CIKa1cucyYDL8/gR4CG/Zgj2P05jueEcBm7yfNnf71n2EM6vf4A5wG5glec2s5Hm/AewzpNxXn1fwKHMWWPdoBcCLz/LP3s+y289n2WfxvhZAoLT1JYFrAGubow5Pc8fBB4NRT4vP8t+wGLPv/kq4OxGmvNyYLNnnReAGH/u34aYMMaYMBeufQTGGGM8rBAYY0yYs0JgjDFhzgqBMcaEOSsExhgT5qwQGGNMmLNCYMKSZ8jpjifx/nQR+R8/ZzpDRD705zaN8YYVAhOupgAnXAiAdMCvhcCYULFCYJoNz2RCaz23Oz2/2tdWe/1uEXlQRC7HGUjwNc+kKS1EJFdE/uqZ5GepiPTwvGe6Z/3KbRR6Hj4KjPW8/5d15FkiIv2rPZ8vIpkiMlJEvvJMhvKliPSu5b0Pisjd1Z6vFZF0z+NrPRlXichzIhLhuU33rLemrkzG1MYKgWkWRGQ48BPgFJxB7W4C2tS2rqq+AywHrlHVIapa7HmpQFUHAv8Enmhgl/cBCz3v/7861pkBXOnJl4ozltFyYAMwVp0BxB4A/p93fyWISF/gKmC0qg7BGYHyGmAI0ElVB3j+hpe93aYxkaEOYIyfjAHeU9UiABH5DzDWx228Ue2+ri93X7wFzAb+gFMQ3vEsTwT+JSI9cSZGifJhm2fijIq6TEQAWgB7cIbQ7iYiTwEfefZrjFesEJjmrDXHHvXGNrC+1vK4vHIbnnHqo73duaruEJF9IjII51f8rZ6X/gTMU9VLPM0982t5e9V+a2QXnJFHf1PzDSIyGJjo2c+VwPXeZjXhzZqGTHOxELhYROJEJB5nGOmPgXYikiQiMcD51dY/jDNPQnVXVbv/yvM4lx/mJbiQH3691/b+2swA7gESVXW1Z1kiP4w3P6WO9+UCw6BqdrcMz/K5wOUi0s7zWlvPxOaVwxS/izM50TAvshkD2BGBaSZU9RsRmQ4s9Sx6QVWXichDnmU7cNrmK00HnhWRYmCUZ1kbEVkNlODMXAXwPPCBiHwLfAIUeZavBio8y6fX00/wDs5Q4X+qtuyvOE1Dv8NpxqnNu8D/isg64Guc4YdR1SzP+2Z7jlDKgNuAYuDlarNrHXfEYExdbBhqYwDPzGmZqro31FmMCTZrGjLGmDBnRwTGnCQRmQj8pcbirap6SSjyGOMrKwTGGBPmrGnIGGPCnBUCY4wJc1YIjDEmzFkhMMaYMPf/AbRzjPeKBjP0AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# توزیع مقادیر خروجی بیشینه غلط\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# f_p = []\n",
    "# interval_number = 100\n",
    "# for i in range(18, 92):\n",
    "#     f_p.append(np.where(((i / interval_number) < np.array(f_percent)) & (np.array(f_percent) < ((i+1) / interval_number)))[0].shape[0])\n",
    "\n",
    "# Create a figure and an axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = np.array(range(18, 92)) / interval_number\n",
    "ax.plot(x, f_percent, x, t_percent)\n",
    "\n",
    "plt.legend(['false_percentage', 'true_percentage'])\n",
    "\n",
    "plt.xlabel('output_values')\n",
    "plt.ylabel('interval_percentage')\n",
    "\n",
    "plt.savefig('percent_max_max_output_distribution.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(values), len(f_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9787912323787324, 0.9718900000000001, 0.9717739254219554)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_max_precision_array = np.divide(max_max_true_prec, (max_max_true_prec + max_max_false_prec), out=np.zeros_like(max_max_true_prec), where=(max_max_false_prec + max_max_true_prec)!=0)\n",
    "max_max_recall_array = np.divide(max_max_true_recall, (max_max_true_recall + max_max_false_recall), out=np.zeros_like(max_max_true_recall), where=(max_max_false_recall + max_max_true_recall)!=0)\n",
    "\n",
    "max_max_f_score_x = 2 * max_max_precision_array * max_max_recall_array\n",
    "max_max_f_score_y = max_max_precision_array + max_max_recall_array\n",
    "max_max_fscore_array =  np.divide(max_max_f_score_x, max_max_f_score_y, out=np.zeros_like(max_max_f_score_x), where=(max_max_f_score_y)!=0)\n",
    "\n",
    "max_max_precision = np.sum(max_max_precision_array) / n_classes\n",
    "max_max_recall = np.sum(max_max_recall_array) / n_classes\n",
    "max_max_fscore = np.sum(max_max_fscore_array) / n_classes\n",
    "\n",
    "max_max_precision, max_max_recall, max_max_fscore\n",
    "\n",
    "max_max_report = metrics.classification_report(testl, softmax_classes, output_dict=True, zero_division=0)\n",
    "print(max_max_report['accuracy'], max_max_report['macro avg'], max_max_report['weighted avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97189 {'precision': 0.9787912323787324, 'recall': 0.9718900000000001, 'f1-score': 0.9717739254219554, 'support': 100000} {'precision': 0.9787912323787322, 'recall': 0.97189, 'f1-score': 0.9717739254219554, 'support': 100000}\n"
     ]
    }
   ],
   "source": [
    "max_max_report = metrics.classification_report(testl, softmax_classes, output_dict=True, zero_division=0)\n",
    "print(max_max_report['accuracy'], max_max_report['macro avg'], max_max_report['weighted avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for idx, s in enumerate(softmax_classes):\n",
    "    if (idx // 5) != s:\n",
    "        pairs.append([idx, int(s.item())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1127"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "942.9999999999994"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1- 0.98114)*50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7039, 0.6940, 0.7845,  ..., 0.6685, 0.5220, 0.5741])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sofmax_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24, 4, 747],\n",
       " [47, 9, 1422],\n",
       " [48, 9, 1014],\n",
       " [96, 19, 7303],\n",
       " [98, 19, 3576],\n",
       " [116, 23, 8988],\n",
       " [228, 45, 3526],\n",
       " [229, 45, 5708],\n",
       " [256, 51, 4882],\n",
       " [349, 69, 5790],\n",
       " [411, 82, 2916],\n",
       " [440, 88, 5705],\n",
       " [444, 88, 9935],\n",
       " [500, 100, 6107],\n",
       " [563, 112, 8561],\n",
       " [642, 128, 890],\n",
       " [648, 129, 1371],\n",
       " [715, 143, 3389],\n",
       " [728, 145, 1184],\n",
       " [768, 153, 8599],\n",
       " [829, 165, 9225],\n",
       " [934, 186, 7949],\n",
       " [1113, 222, 1880],\n",
       " [1190, 238, 4728],\n",
       " [1218, 243, 5159],\n",
       " [1273, 254, 7420],\n",
       " [1298, 259, 1534],\n",
       " [1299, 259, 2197],\n",
       " [1361, 272, 2543],\n",
       " [1383, 276, 5400],\n",
       " [1391, 278, 2531],\n",
       " [1578, 315, 4060],\n",
       " [1712, 342, 1581],\n",
       " [1800, 360, 1392],\n",
       " [1829, 365, 6545],\n",
       " [1900, 380, 9641],\n",
       " [1901, 380, 3034],\n",
       " [1929, 385, 6544],\n",
       " [1956, 391, 2222],\n",
       " [1993, 398, 2075],\n",
       " [1999, 399, 3805],\n",
       " [2010, 402, 4998],\n",
       " [2077, 415, 1820],\n",
       " [2084, 416, 1610],\n",
       " [2099, 419, 2916],\n",
       " [2104, 420, 5924],\n",
       " [2117, 423, 6984],\n",
       " [2125, 425, 693],\n",
       " [2141, 428, 1098],\n",
       " [2142, 428, 7970],\n",
       " [2153, 430, 7075],\n",
       " [2199, 439, 2044],\n",
       " [2205, 441, 2091],\n",
       " [2227, 445, 1214],\n",
       " [2322, 464, 1493],\n",
       " [2402, 480, 9782],\n",
       " [2412, 482, 3827],\n",
       " [2413, 482, 2018],\n",
       " [2414, 482, 2829],\n",
       " [2438, 487, 4342],\n",
       " [2482, 496, 2484],\n",
       " [2623, 524, 4562],\n",
       " [2627, 525, 4638],\n",
       " [2690, 538, 8881],\n",
       " [2691, 538, 1933],\n",
       " [2782, 556, 8856],\n",
       " [2839, 567, 6776],\n",
       " [2902, 580, 1807],\n",
       " [3011, 602, 4246],\n",
       " [3050, 610, 8372],\n",
       " [3074, 614, 2028],\n",
       " [3129, 625, 160],\n",
       " [3167, 633, 4587],\n",
       " [3171, 634, 1134],\n",
       " [3178, 635, 5368],\n",
       " [3187, 637, 2059],\n",
       " [3197, 639, 8384],\n",
       " [3217, 643, 642],\n",
       " [3233, 646, 9102],\n",
       " [3276, 655, 1441],\n",
       " [3303, 660, 903],\n",
       " [3317, 663, 3408],\n",
       " [3387, 677, 6980],\n",
       " [3393, 678, 4702],\n",
       " [3435, 687, 5514],\n",
       " [3451, 690, 6043],\n",
       " [3452, 690, 6043],\n",
       " [3453, 690, 7884],\n",
       " [3454, 690, 5573],\n",
       " [3544, 708, 2197],\n",
       " [3554, 710, 564],\n",
       " [3569, 713, 5358],\n",
       " [3720, 744, 1534],\n",
       " [3876, 775, 3661],\n",
       " [3989, 797, 6374],\n",
       " [4119, 823, 3401],\n",
       " [4128, 825, 590],\n",
       " [4148, 829, 6584],\n",
       " [4155, 831, 4674],\n",
       " [4162, 832, 2891],\n",
       " [4163, 832, 591],\n",
       " [4164, 832, 7360],\n",
       " [4223, 844, 9669],\n",
       " [4272, 854, 2075],\n",
       " [4285, 857, 5573],\n",
       " [4292, 858, 7198],\n",
       " [4334, 866, 64],\n",
       " [4394, 878, 2590],\n",
       " [4433, 886, 2316],\n",
       " [4488, 897, 1445],\n",
       " [4523, 904, 7423],\n",
       " [4535, 907, 1308],\n",
       " [4549, 909, 9875],\n",
       " [4572, 914, 3261],\n",
       " [4640, 928, 890],\n",
       " [4642, 928, 5939],\n",
       " [4746, 949, 2075],\n",
       " [4760, 952, 3350],\n",
       " [4769, 953, 1134],\n",
       " [4834, 966, 9065],\n",
       " [4937, 987, 8263],\n",
       " [5004, 1000, 6528],\n",
       " [5098, 1019, 3806],\n",
       " [5127, 1025, 4959],\n",
       " [5168, 1033, 7677],\n",
       " [5198, 1039, 7331],\n",
       " [5604, 1120, 405],\n",
       " [5617, 1123, 4271],\n",
       " [5619, 1123, 4202],\n",
       " [5625, 1125, 2381],\n",
       " [5788, 1157, 352],\n",
       " [5792, 1158, 6933],\n",
       " [5837, 1167, 242],\n",
       " [5864, 1172, 2428],\n",
       " [5927, 1185, 2064],\n",
       " [5939, 1187, 3278],\n",
       " [5947, 1189, 3606],\n",
       " [5949, 1189, 1493],\n",
       " [5962, 1192, 3242],\n",
       " [6024, 1204, 20],\n",
       " [6052, 1210, 3856],\n",
       " [6082, 1216, 4728],\n",
       " [6173, 1234, 532],\n",
       " [6197, 1239, 4589],\n",
       " [6241, 1248, 539],\n",
       " [6253, 1250, 6302],\n",
       " [6258, 1251, 2018],\n",
       " [6372, 1274, 1632],\n",
       " [6373, 1274, 7533],\n",
       " [6384, 1276, 8588],\n",
       " [6464, 1292, 7222],\n",
       " [6465, 1293, 7642],\n",
       " [6603, 1320, 9017],\n",
       " [6689, 1337, 3577],\n",
       " [6725, 1345, 6361],\n",
       " [6727, 1345, 8731],\n",
       " [6773, 1354, 9047],\n",
       " [6777, 1355, 1861],\n",
       " [6779, 1355, 1346],\n",
       " [6797, 1359, 7540],\n",
       " [6819, 1363, 9110],\n",
       " [6852, 1370, 1357],\n",
       " [6906, 1381, 8185],\n",
       " [6926, 1385, 3881],\n",
       " [6953, 1390, 6022],\n",
       " [6993, 1398, 667],\n",
       " [6994, 1398, 2124],\n",
       " [7009, 1401, 101],\n",
       " [7039, 1407, 8561],\n",
       " [7094, 1418, 3399],\n",
       " [7104, 1420, 5734],\n",
       " [7109, 1421, 2793],\n",
       " [7367, 1473, 2863],\n",
       " [7397, 1479, 6479],\n",
       " [7409, 1481, 352],\n",
       " [7522, 1504, 4865],\n",
       " [7532, 1506, 1445],\n",
       " [7558, 1511, 587],\n",
       " [7578, 1515, 2059],\n",
       " [7583, 1516, 8306],\n",
       " [7611, 1522, 1239],\n",
       " [7661, 1532, 7120],\n",
       " [7664, 1532, 7989],\n",
       " [7679, 1535, 4406],\n",
       " [7770, 1554, 4359],\n",
       " [7912, 1582, 8469],\n",
       " [7913, 1582, 1844],\n",
       " [7914, 1582, 870],\n",
       " [7948, 1589, 2507],\n",
       " [7977, 1595, 7036],\n",
       " [7989, 1597, 967],\n",
       " [8073, 1614, 9035],\n",
       " [8103, 1620, 1347],\n",
       " [8251, 1650, 7530],\n",
       " [8299, 1659, 2666],\n",
       " [8308, 1661, 1234],\n",
       " [8314, 1662, 1573],\n",
       " [8369, 1673, 5739],\n",
       " [8406, 1681, 1501],\n",
       " [8587, 1717, 5938],\n",
       " [8604, 1720, 3409],\n",
       " [8607, 1721, 8234],\n",
       " [8628, 1725, 4007],\n",
       " [8651, 1730, 8290],\n",
       " [8784, 1756, 2199],\n",
       " [8844, 1768, 4117],\n",
       " [8872, 1774, 7926],\n",
       " [8909, 1781, 4674],\n",
       " [8933, 1786, 3606],\n",
       " [8974, 1794, 9749],\n",
       " [9051, 1810, 8111],\n",
       " [9058, 1811, 7331],\n",
       " [9149, 1829, 9212],\n",
       " [9152, 1830, 7438],\n",
       " [9320, 1864, 1700],\n",
       " [9383, 1876, 5062],\n",
       " [9436, 1887, 3283],\n",
       " [9488, 1897, 6682],\n",
       " [9535, 1907, 7097],\n",
       " [9593, 1918, 8935],\n",
       " [9879, 1975, 3024],\n",
       " [9962, 1992, 3929],\n",
       " [9972, 1994, 7863],\n",
       " [10136, 2027, 2655],\n",
       " [10235, 2047, 2124],\n",
       " [10310, 2062, 2380],\n",
       " [10342, 2068, 8599],\n",
       " [10442, 2088, 9138],\n",
       " [10527, 2105, 4379],\n",
       " [10649, 2129, 1738],\n",
       " [10693, 2138, 6305],\n",
       " [10725, 2145, 796],\n",
       " [10880, 2176, 1475],\n",
       " [11011, 2202, 5767],\n",
       " [11027, 2205, 8183],\n",
       " [11086, 2217, 7613],\n",
       " [11099, 2219, 4601],\n",
       " [11135, 2227, 4401],\n",
       " [11137, 2227, 736],\n",
       " [11204, 2240, 741],\n",
       " [11258, 2251, 8834],\n",
       " [11292, 2258, 2698],\n",
       " [11345, 2269, 9796],\n",
       " [11361, 2272, 3903],\n",
       " [11379, 2275, 9495],\n",
       " [11388, 2277, 914],\n",
       " [11423, 2284, 352],\n",
       " [11467, 2293, 6116],\n",
       " [11532, 2306, 6049],\n",
       " [11572, 2314, 4036],\n",
       " [11575, 2315, 4589],\n",
       " [11602, 2320, 4530],\n",
       " [11603, 2320, 6508],\n",
       " [11639, 2327, 901],\n",
       " [11656, 2331, 6895],\n",
       " [11790, 2358, 3796],\n",
       " [11987, 2397, 894],\n",
       " [11994, 2398, 3640],\n",
       " [12014, 2402, 4473],\n",
       " [12154, 2430, 3880],\n",
       " [12155, 2431, 5497],\n",
       " [12198, 2439, 6946],\n",
       " [12215, 2443, 1214],\n",
       " [12319, 2463, 1485],\n",
       " [12397, 2479, 340],\n",
       " [12430, 2486, 2028],\n",
       " [12562, 2512, 1610],\n",
       " [12587, 2517, 3439],\n",
       " [12629, 2525, 2247],\n",
       " [12646, 2529, 1214],\n",
       " [12677, 2535, 3930],\n",
       " [12750, 2550, 5484],\n",
       " [12789, 2557, 2623],\n",
       " [12825, 2565, 5083],\n",
       " [12828, 2565, 5083],\n",
       " [12849, 2569, 4779],\n",
       " [12927, 2585, 3750],\n",
       " [12965, 2593, 9749],\n",
       " [12968, 2593, 242],\n",
       " [13011, 2602, 7809],\n",
       " [13012, 2602, 7809],\n",
       " [13013, 2602, 7809],\n",
       " [13037, 2607, 1600],\n",
       " [13133, 2626, 4882],\n",
       " [13145, 2629, 4916],\n",
       " [13146, 2629, 4916],\n",
       " [13149, 2629, 4528],\n",
       " [13199, 2639, 4728],\n",
       " [13214, 2642, 9201],\n",
       " [13386, 2677, 1747],\n",
       " [13482, 2696, 1315],\n",
       " [13546, 2709, 5640],\n",
       " [13589, 2717, 3215],\n",
       " [13594, 2718, 155],\n",
       " [13644, 2728, 7441],\n",
       " [13794, 2758, 1371],\n",
       " [13813, 2762, 4156],\n",
       " [13892, 2778, 8274],\n",
       " [13904, 2780, 3931],\n",
       " [13927, 2785, 1617],\n",
       " [13944, 2788, 8834],\n",
       " [13974, 2794, 7703],\n",
       " [14097, 2819, 7530],\n",
       " [14121, 2824, 2343],\n",
       " [14157, 2831, 1610],\n",
       " [14274, 2854, 6534],\n",
       " [14369, 2873, 8582],\n",
       " [14370, 2874, 3713],\n",
       " [14393, 2878, 6584],\n",
       " [14476, 2895, 6420],\n",
       " [14513, 2902, 7410],\n",
       " [14565, 2913, 5005],\n",
       " [14568, 2913, 9771],\n",
       " [14569, 2913, 9949],\n",
       " [14681, 2936, 7515],\n",
       " [14682, 2936, 7229],\n",
       " [14683, 2936, 7515],\n",
       " [14684, 2936, 7515],\n",
       " [14693, 2938, 8496],\n",
       " [14705, 2941, 9507],\n",
       " [14768, 2953, 1503],\n",
       " [15120, 3024, 918],\n",
       " [15121, 3024, 918],\n",
       " [15124, 3024, 8493],\n",
       " [15132, 3026, 9574],\n",
       " [15142, 3028, 64],\n",
       " [15157, 3031, 4841],\n",
       " [15178, 3035, 6490],\n",
       " [15180, 3036, 3819],\n",
       " [15183, 3036, 5141],\n",
       " [15206, 3041, 930],\n",
       " [15303, 3060, 4489],\n",
       " [15309, 3061, 9955],\n",
       " [15318, 3063, 2114],\n",
       " [15332, 3066, 3424],\n",
       " [15395, 3079, 3523],\n",
       " [15417, 3083, 1346],\n",
       " [15452, 3090, 7553],\n",
       " [15473, 3094, 1277],\n",
       " [15548, 3109, 5810],\n",
       " [15557, 3111, 4244],\n",
       " [15558, 3111, 8260],\n",
       " [15561, 3112, 8306],\n",
       " [15567, 3113, 7222],\n",
       " [15616, 3123, 7420],\n",
       " [15657, 3131, 4007],\n",
       " [15690, 3138, 2680],\n",
       " [15764, 3152, 6387],\n",
       " [15822, 3164, 9424],\n",
       " [15989, 3197, 2091],\n",
       " [16027, 3205, 2805],\n",
       " [16083, 3216, 5570],\n",
       " [16112, 3222, 2997],\n",
       " [16180, 3236, 3096],\n",
       " [16324, 3264, 4726],\n",
       " [16452, 3290, 2991],\n",
       " [16517, 3303, 5383],\n",
       " [16520, 3304, 7764],\n",
       " [16522, 3304, 7705],\n",
       " [16523, 3304, 3149],\n",
       " [16553, 3310, 1799],\n",
       " [16578, 3315, 8384],\n",
       " [16655, 3331, 2770],\n",
       " [16661, 3332, 1882],\n",
       " [16664, 3332, 7420],\n",
       " [16757, 3351, 2997],\n",
       " [16784, 3356, 8793],\n",
       " [16891, 3378, 3581],\n",
       " [16892, 3378, 886],\n",
       " [16894, 3378, 8731],\n",
       " [16933, 3386, 5368],\n",
       " [16934, 3386, 2103],\n",
       " [16989, 3397, 1497],\n",
       " [17040, 3408, 4557],\n",
       " [17046, 3409, 3931],\n",
       " [17188, 3437, 5303],\n",
       " [17198, 3439, 7487],\n",
       " [17267, 3453, 1445],\n",
       " [17294, 3458, 6180],\n",
       " [17342, 3468, 7573],\n",
       " [17425, 3485, 7863],\n",
       " [17429, 3485, 4627],\n",
       " [17467, 3493, 9524],\n",
       " [17468, 3493, 7542],\n",
       " [17470, 3494, 5408],\n",
       " [17481, 3496, 8550],\n",
       " [17587, 3517, 4244],\n",
       " [17619, 3523, 2246],\n",
       " [17697, 3539, 1184],\n",
       " [17703, 3540, 4743],\n",
       " [17778, 3555, 8168],\n",
       " [17789, 3557, 4627],\n",
       " [17871, 3574, 9496],\n",
       " [17927, 3585, 9474],\n",
       " [17944, 3588, 3951],\n",
       " [17959, 3591, 1862],\n",
       " [18062, 3612, 8844],\n",
       " [18064, 3612, 579],\n",
       " [18101, 3620, 1933],\n",
       " [18102, 3620, 1933],\n",
       " [18152, 3630, 1963],\n",
       " [18157, 3631, 9282],\n",
       " [18168, 3633, 6092],\n",
       " [18198, 3639, 867],\n",
       " [18297, 3659, 12],\n",
       " [18356, 3671, 3184],\n",
       " [18495, 3699, 1399],\n",
       " [18526, 3705, 5561],\n",
       " [18528, 3705, 6480],\n",
       " [18649, 3729, 3506],\n",
       " [18729, 3745, 1236],\n",
       " [18776, 3755, 9167],\n",
       " [18826, 3765, 2581],\n",
       " [18827, 3765, 6320],\n",
       " [18887, 3777, 6570],\n",
       " [18926, 3785, 7681],\n",
       " [18932, 3786, 4931],\n",
       " [18972, 3794, 742],\n",
       " [19118, 3823, 5556],\n",
       " [19154, 3830, 3294],\n",
       " [19225, 3845, 2679],\n",
       " [19228, 3845, 6241],\n",
       " [19229, 3845, 2679],\n",
       " [19373, 3874, 1610],\n",
       " [19394, 3878, 5516],\n",
       " [19470, 3894, 470],\n",
       " [19490, 3898, 7949],\n",
       " [19491, 3898, 7949],\n",
       " [19494, 3898, 7949],\n",
       " [19563, 3912, 8354],\n",
       " [19723, 3944, 2063],\n",
       " [19821, 3964, 4057],\n",
       " [19822, 3964, 1347],\n",
       " [19823, 3964, 9663],\n",
       " [19842, 3968, 6205],\n",
       " [19878, 3975, 691],\n",
       " [19888, 3977, 4562],\n",
       " [19894, 3978, 1445],\n",
       " [19918, 3983, 3752],\n",
       " [19943, 3988, 4438],\n",
       " [19985, 3997, 8306],\n",
       " [20078, 4015, 894],\n",
       " [20090, 4018, 607],\n",
       " [20185, 4037, 1862],\n",
       " [20189, 4037, 3620],\n",
       " [20239, 4047, 9209],\n",
       " [20256, 4051, 8067],\n",
       " [20263, 4052, 7939],\n",
       " [20394, 4078, 5631],\n",
       " [20463, 4092, 9138],\n",
       " [20467, 4093, 4683],\n",
       " [20543, 4108, 8582],\n",
       " [20599, 4119, 8259],\n",
       " [20614, 4122, 2858],\n",
       " [20693, 4138, 8826],\n",
       " [20698, 4139, 7361],\n",
       " [20700, 4140, 2829],\n",
       " [20704, 4140, 2059],\n",
       " [20733, 4146, 8801],\n",
       " [20778, 4155, 8885],\n",
       " [20804, 4160, 9678],\n",
       " [20922, 4184, 3518],\n",
       " [20937, 4187, 1444],\n",
       " [21067, 4213, 1443],\n",
       " [21138, 4227, 3242],\n",
       " [21158, 4231, 8801],\n",
       " [21243, 4248, 6127],\n",
       " [21369, 4273, 2691],\n",
       " [21389, 4277, 5525],\n",
       " [21422, 4284, 6194],\n",
       " [21424, 4284, 9683],\n",
       " [21440, 4288, 1308],\n",
       " [21481, 4296, 9317],\n",
       " [21654, 4330, 9579],\n",
       " [21722, 4344, 8026],\n",
       " [21872, 4374, 4614],\n",
       " [21883, 4376, 1008],\n",
       " [21889, 4377, 8801],\n",
       " [21973, 4394, 6018],\n",
       " [21974, 4394, 1066],\n",
       " [21996, 4399, 9493],\n",
       " [21997, 4399, 6419],\n",
       " [22048, 4409, 1174],\n",
       " [22124, 4424, 3150],\n",
       " [22174, 4434, 3546],\n",
       " [22286, 4457, 3743],\n",
       " [22288, 4457, 3986],\n",
       " [22299, 4459, 2316],\n",
       " [22337, 4467, 2124],\n",
       " [22354, 4470, 7638],\n",
       " [22446, 4489, 4292],\n",
       " [22489, 4497, 3359],\n",
       " [22539, 4507, 8834],\n",
       " [22611, 4522, 2829],\n",
       " [22678, 4535, 9251],\n",
       " [22679, 4535, 6716],\n",
       " [22688, 4537, 9583],\n",
       " [22857, 4571, 6346],\n",
       " [22864, 4572, 951],\n",
       " [23102, 4620, 8168],\n",
       " [23105, 4621, 2410],\n",
       " [23108, 4621, 2410],\n",
       " [23147, 4629, 6078],\n",
       " [23159, 4631, 2997],\n",
       " [23222, 4644, 3977],\n",
       " [23235, 4647, 6897],\n",
       " [23266, 4653, 2141],\n",
       " [23320, 4664, 4544],\n",
       " [23340, 4668, 1526],\n",
       " [23344, 4668, 1526],\n",
       " [23497, 4699, 9882],\n",
       " [23679, 4735, 1572],\n",
       " [23764, 4752, 4985],\n",
       " [23855, 4771, 1593],\n",
       " [23913, 4782, 98],\n",
       " [23949, 4789, 3242],\n",
       " [23958, 4791, 2084],\n",
       " [23982, 4796, 7670],\n",
       " [24090, 4818, 8640],\n",
       " [24162, 4832, 352],\n",
       " [24230, 4846, 8354],\n",
       " [24280, 4856, 6018],\n",
       " [24310, 4862, 8312],\n",
       " [24357, 4871, 8582],\n",
       " [24376, 4875, 7956],\n",
       " [24547, 4909, 4451],\n",
       " [24549, 4909, 8599],\n",
       " [24622, 4924, 1308],\n",
       " [24674, 4934, 2713],\n",
       " [24769, 4953, 7011],\n",
       " [24866, 4973, 6360],\n",
       " [24894, 4978, 7031],\n",
       " [24909, 4981, 8569],\n",
       " [25094, 5018, 3425],\n",
       " [25137, 5027, 2091],\n",
       " [25139, 5027, 3278],\n",
       " [25244, 5048, 4562],\n",
       " [25278, 5055, 3038],\n",
       " [25284, 5056, 1789],\n",
       " [25302, 5060, 2381],\n",
       " [25338, 5067, 2091],\n",
       " [25385, 5077, 2611],\n",
       " [25449, 5089, 2316],\n",
       " [25609, 5121, 7965],\n",
       " [25637, 5127, 4562],\n",
       " [25639, 5127, 4391],\n",
       " [25654, 5130, 6894],\n",
       " [25657, 5131, 2834],\n",
       " [25659, 5131, 5117],\n",
       " [25674, 5134, 8821],\n",
       " [25720, 5144, 94],\n",
       " [25721, 5144, 6819],\n",
       " [25815, 5163, 5484],\n",
       " [25818, 5163, 806],\n",
       " [25828, 5165, 4373],\n",
       " [25848, 5169, 9552],\n",
       " [25890, 5178, 5556],\n",
       " [25919, 5183, 1550],\n",
       " [25959, 5191, 3215],\n",
       " [25973, 5194, 6642],\n",
       " [25985, 5197, 8959],\n",
       " [25987, 5197, 2091],\n",
       " [25989, 5197, 8959],\n",
       " [26017, 5203, 5356],\n",
       " [26036, 5207, 6083],\n",
       " [26038, 5207, 9259],\n",
       " [26059, 5211, 242],\n",
       " [26084, 5216, 9371],\n",
       " [26106, 5221, 896],\n",
       " [26107, 5221, 896],\n",
       " [26253, 5250, 4711],\n",
       " [26254, 5250, 2754],\n",
       " [26259, 5251, 999],\n",
       " [26262, 5252, 9627],\n",
       " [26294, 5258, 9949],\n",
       " [26305, 5261, 8883],\n",
       " [26307, 5261, 8883],\n",
       " [26308, 5261, 8883],\n",
       " [26309, 5261, 8883],\n",
       " [26383, 5276, 7256],\n",
       " [26449, 5289, 6275],\n",
       " [26504, 5300, 6768],\n",
       " [26533, 5306, 4342],\n",
       " [26552, 5310, 2284],\n",
       " [26612, 5322, 7109],\n",
       " [26659, 5331, 8664],\n",
       " [26669, 5333, 7794],\n",
       " [26811, 5362, 2091],\n",
       " [26900, 5380, 2919],\n",
       " [26902, 5380, 420],\n",
       " [26903, 5380, 2091],\n",
       " [26904, 5380, 2091],\n",
       " [27064, 5412, 9105],\n",
       " [27067, 5413, 1141],\n",
       " [27084, 5416, 9934],\n",
       " [27221, 5444, 375],\n",
       " [27224, 5444, 2180],\n",
       " [27228, 5445, 6428],\n",
       " [27269, 5453, 5031],\n",
       " [27354, 5470, 690],\n",
       " [27374, 5474, 2916],\n",
       " [27435, 5487, 9493],\n",
       " [27490, 5498, 4589],\n",
       " [27511, 5502, 249],\n",
       " [27537, 5507, 2698],\n",
       " [27566, 5513, 3683],\n",
       " [27578, 5515, 3931],\n",
       " [27688, 5537, 8572],\n",
       " [27722, 5544, 2091],\n",
       " [27793, 5558, 2804],\n",
       " [27836, 5567, 2623],\n",
       " [27837, 5567, 3360],\n",
       " [27923, 5584, 6002],\n",
       " [27988, 5597, 3650],\n",
       " [28094, 5618, 2725],\n",
       " [28194, 5638, 2770],\n",
       " [28292, 5658, 4174],\n",
       " [28308, 5661, 1188],\n",
       " [28313, 5662, 5585],\n",
       " [28315, 5663, 4342],\n",
       " [28343, 5668, 8234],\n",
       " [28389, 5677, 5497],\n",
       " [28422, 5684, 7493],\n",
       " [28424, 5684, 6018],\n",
       " [28443, 5688, 9487],\n",
       " [28444, 5688, 5955],\n",
       " [28492, 5698, 361],\n",
       " [28518, 5703, 8034],\n",
       " [28524, 5704, 5477],\n",
       " [28534, 5706, 4944],\n",
       " [28629, 5725, 4042],\n",
       " [28696, 5739, 8731],\n",
       " [28717, 5743, 4104],\n",
       " [28723, 5744, 7528],\n",
       " [28828, 5765, 2091],\n",
       " [28829, 5765, 2091],\n",
       " [28858, 5771, 5820],\n",
       " [28861, 5772, 352],\n",
       " [28863, 5772, 352],\n",
       " [28867, 5773, 7120],\n",
       " [28969, 5793, 5811],\n",
       " [29070, 5814, 2930],\n",
       " [29071, 5814, 4227],\n",
       " [29211, 5842, 638],\n",
       " [29282, 5856, 7794],\n",
       " [29291, 5858, 7863],\n",
       " [29357, 5871, 394],\n",
       " [29398, 5879, 242],\n",
       " [29473, 5894, 3871],\n",
       " [29519, 5903, 508],\n",
       " [29535, 5907, 3712],\n",
       " [29592, 5918, 2829],\n",
       " [29593, 5918, 951],\n",
       " [29639, 5927, 4280],\n",
       " [29663, 5932, 8281],\n",
       " [29742, 5948, 242],\n",
       " [29787, 5957, 8312],\n",
       " [29832, 5966, 6018],\n",
       " [29848, 5969, 4224],\n",
       " [29849, 5969, 3854],\n",
       " [29909, 5981, 7962],\n",
       " [29932, 5986, 7031],\n",
       " [29940, 5988, 6587],\n",
       " [29997, 5999, 1836],\n",
       " [30015, 6003, 4277],\n",
       " [30071, 6014, 2770],\n",
       " [30123, 6024, 896],\n",
       " [30211, 6042, 2084],\n",
       " [30274, 6054, 7843],\n",
       " [30372, 6074, 7264],\n",
       " [30483, 6096, 9824],\n",
       " [30488, 6097, 886],\n",
       " [30518, 6103, 6420],\n",
       " [30572, 6114, 4179],\n",
       " [30573, 6114, 5709],\n",
       " [30645, 6129, 9136],\n",
       " [30647, 6129, 9136],\n",
       " [30728, 6145, 1855],\n",
       " [30736, 6147, 9673],\n",
       " [30754, 6150, 747],\n",
       " [30777, 6155, 1336],\n",
       " [30883, 6176, 1247],\n",
       " [30912, 6182, 8953],\n",
       " [30982, 6196, 5368],\n",
       " [31026, 6205, 2881],\n",
       " [31027, 6205, 489],\n",
       " [31097, 6219, 8032],\n",
       " [31099, 6219, 9167],\n",
       " [31128, 6225, 3566],\n",
       " [31164, 6232, 1120],\n",
       " [31178, 6235, 4728],\n",
       " [31184, 6236, 484],\n",
       " [31227, 6245, 996],\n",
       " [31244, 6248, 7364],\n",
       " [31341, 6268, 8007],\n",
       " [31367, 6273, 2829],\n",
       " [31378, 6275, 1765],\n",
       " [31392, 6278, 9403],\n",
       " [31428, 6285, 506],\n",
       " [31512, 6302, 5734],\n",
       " [31553, 6310, 7168],\n",
       " [31731, 6346, 1593],\n",
       " [31758, 6351, 9229],\n",
       " [31927, 6385, 6334],\n",
       " [32120, 6424, 4342],\n",
       " [32228, 6445, 7210],\n",
       " [32259, 6451, 5589],\n",
       " [32278, 6455, 7670],\n",
       " [32283, 6456, 5159],\n",
       " [32356, 6471, 8505],\n",
       " [32411, 6482, 5531],\n",
       " [32414, 6482, 3247],\n",
       " [32455, 6491, 3910],\n",
       " [32457, 6491, 1700],\n",
       " [32597, 6519, 5684],\n",
       " [32598, 6519, 4612],\n",
       " [32601, 6520, 8912],\n",
       " [32789, 6557, 5767],\n",
       " [32808, 6561, 5775],\n",
       " [32910, 6582, 3988],\n",
       " [33002, 6600, 1247],\n",
       " [33065, 6613, 5303],\n",
       " [33069, 6613, 5194],\n",
       " [33081, 6616, 852],\n",
       " [33197, 6639, 8259],\n",
       " [33343, 6668, 2999],\n",
       " [33373, 6674, 6181],\n",
       " [33399, 6679, 8988],\n",
       " [33498, 6699, 608],\n",
       " [33555, 6711, 3497],\n",
       " [33557, 6711, 4082],\n",
       " [33621, 6724, 3688],\n",
       " [33625, 6725, 9274],\n",
       " [33627, 6725, 5477],\n",
       " [33629, 6725, 5767],\n",
       " [33652, 6730, 33],\n",
       " [33698, 6739, 1121],\n",
       " [33748, 6749, 5284],\n",
       " [33789, 6757, 3696],\n",
       " [33794, 6758, 2248],\n",
       " [33988, 6797, 1565],\n",
       " [34037, 6807, 9144],\n",
       " [34042, 6808, 540],\n",
       " [34044, 6808, 780],\n",
       " [34104, 6820, 2091],\n",
       " [34135, 6827, 7553],\n",
       " [34157, 6831, 3793],\n",
       " [34290, 6858, 3901],\n",
       " [34344, 6868, 8372],\n",
       " [34354, 6870, 61],\n",
       " [34358, 6871, 9398],\n",
       " [34388, 6877, 8464],\n",
       " [34497, 6899, 2634],\n",
       " [34498, 6899, 1379],\n",
       " [34502, 6900, 7236],\n",
       " [34507, 6901, 2529],\n",
       " [34508, 6901, 6243],\n",
       " [34584, 6916, 1991],\n",
       " [34757, 6951, 9311],\n",
       " [34845, 6969, 2315],\n",
       " [34863, 6972, 7699],\n",
       " [34942, 6988, 3931],\n",
       " [35007, 7001, 9926],\n",
       " [35053, 7010, 1354],\n",
       " [35086, 7017, 7987],\n",
       " [35147, 7029, 2666],\n",
       " [35217, 7043, 216],\n",
       " [35306, 7061, 7530],\n",
       " [35307, 7061, 6065],\n",
       " [35319, 7063, 9040],\n",
       " [35342, 7068, 1755],\n",
       " [35344, 7068, 775],\n",
       " [35362, 7072, 8629],\n",
       " [35396, 7079, 6120],\n",
       " [35398, 7079, 2316],\n",
       " [35536, 7107, 2807],\n",
       " [35544, 7108, 6324],\n",
       " [35643, 7128, 4066],\n",
       " [35691, 7138, 5176],\n",
       " [35703, 7140, 2916],\n",
       " [35746, 7149, 8071],\n",
       " [35749, 7149, 7528],\n",
       " [35767, 7153, 508],\n",
       " [35898, 7179, 4912],\n",
       " [35977, 7195, 8735],\n",
       " [36041, 7208, 7379],\n",
       " [36042, 7208, 7987],\n",
       " [36043, 7208, 8168],\n",
       " [36059, 7211, 5802],\n",
       " [36104, 7220, 195],\n",
       " [36117, 7223, 9151],\n",
       " [36120, 7224, 5358],\n",
       " [36132, 7226, 6601],\n",
       " [36179, 7235, 4804],\n",
       " [36238, 7247, 1591],\n",
       " [36246, 7249, 8200],\n",
       " [36289, 7257, 630],\n",
       " [36431, 7286, 3240],\n",
       " [36435, 7287, 2398],\n",
       " [36466, 7293, 7816],\n",
       " [36669, 7333, 1247],\n",
       " [36897, 7379, 4587],\n",
       " [36903, 7380, 5673],\n",
       " [36904, 7380, 5673],\n",
       " [36925, 7385, 6971],\n",
       " [36926, 7385, 6971],\n",
       " [36927, 7385, 5981],\n",
       " [36929, 7385, 2952],\n",
       " [36968, 7393, 8317],\n",
       " [37028, 7405, 4083],\n",
       " [37099, 7419, 2075],\n",
       " [37139, 7427, 7482],\n",
       " [37164, 7432, 4754],\n",
       " [37184, 7436, 7987],\n",
       " [37303, 7460, 930],\n",
       " [37311, 7462, 6279],\n",
       " [37313, 7462, 5062],\n",
       " [37314, 7462, 1550],\n",
       " [37321, 7464, 1421],\n",
       " [37334, 7466, 9508],\n",
       " [37387, 7477, 640],\n",
       " [37429, 7485, 2091],\n",
       " [37436, 7487, 1487],\n",
       " [37444, 7488, 7540],\n",
       " [37598, 7519, 5710],\n",
       " [37641, 7528, 1137],\n",
       " [37644, 7528, 5368],\n",
       " [37653, 7530, 7613],\n",
       " [37842, 7568, 9197],\n",
       " [37929, 7585, 9509],\n",
       " [37982, 7596, 7842],\n",
       " [38034, 7606, 1339],\n",
       " [38137, 7627, 164],\n",
       " [38268, 7653, 2768],\n",
       " [38309, 7661, 7619],\n",
       " [38326, 7665, 7088],\n",
       " [38579, 7715, 5016],\n",
       " [38597, 7719, 2316],\n",
       " [38601, 7720, 6607],\n",
       " [38624, 7724, 3624],\n",
       " [38714, 7742, 5770],\n",
       " [38754, 7750, 3165],\n",
       " [38906, 7781, 5848],\n",
       " [38913, 7782, 8697],\n",
       " [39013, 7802, 5962],\n",
       " [39058, 7811, 5962],\n",
       " [39148, 7829, 7282],\n",
       " [39259, 7851, 2997],\n",
       " [39272, 7854, 4784],\n",
       " [39374, 7874, 6723],\n",
       " [39391, 7878, 9016],\n",
       " [39408, 7881, 2916],\n",
       " [39498, 7899, 5116],\n",
       " [39719, 7943, 9888],\n",
       " [39757, 7951, 5176],\n",
       " [39758, 7951, 5176],\n",
       " [39773, 7954, 9901],\n",
       " [39881, 7976, 4589],\n",
       " [39893, 7978, 7644],\n",
       " [39932, 7986, 2885],\n",
       " [39948, 7989, 2520],\n",
       " [39963, 7992, 5293],\n",
       " [39968, 7993, 3003],\n",
       " [39980, 7996, 3113],\n",
       " [40007, 8001, 901],\n",
       " [40014, 8002, 9870],\n",
       " [40076, 8015, 1350],\n",
       " [40085, 8017, 7354],\n",
       " [40183, 8036, 2714],\n",
       " [40198, 8039, 3264],\n",
       " [40213, 8042, 9356],\n",
       " [40308, 8061, 6008],\n",
       " [40345, 8069, 6992],\n",
       " [40349, 8069, 3735],\n",
       " [40407, 8081, 8179],\n",
       " [40484, 8096, 4680],\n",
       " [40707, 8141, 6590],\n",
       " [40716, 8143, 8482],\n",
       " [40717, 8143, 1819],\n",
       " [40797, 8159, 7644],\n",
       " [40802, 8160, 8312],\n",
       " [40818, 8163, 3578],\n",
       " [40824, 8164, 6108],\n",
       " [41127, 8225, 699],\n",
       " [41128, 8225, 3884],\n",
       " [41164, 8232, 2754],\n",
       " [41276, 8255, 8534],\n",
       " [41328, 8265, 242],\n",
       " [41373, 8274, 4702],\n",
       " [41507, 8301, 9123],\n",
       " [41537, 8307, 8443],\n",
       " [41619, 8323, 4862],\n",
       " [41657, 8331, 5219],\n",
       " [41701, 8340, 4342],\n",
       " [41777, 8355, 2091],\n",
       " [41789, 8357, 9732],\n",
       " [41858, 8371, 1006],\n",
       " [41861, 8372, 2091],\n",
       " [41871, 8374, 917],\n",
       " [41893, 8378, 7195],\n",
       " [41958, 8391, 5999],\n",
       " [41968, 8393, 803],\n",
       " [41988, 8397, 511],\n",
       " [42018, 8403, 8618],\n",
       " [42049, 8409, 9018],\n",
       " [42079, 8415, 8372],\n",
       " [42161, 8432, 8958],\n",
       " [42249, 8449, 8372],\n",
       " [42250, 8450, 3958],\n",
       " [42387, 8477, 3528],\n",
       " [42388, 8477, 2377],\n",
       " [42393, 8478, 2998],\n",
       " [42410, 8482, 1534],\n",
       " [42523, 8504, 8355],\n",
       " [42651, 8530, 6716],\n",
       " [42652, 8530, 5492],\n",
       " [42680, 8536, 8834],\n",
       " [42698, 8539, 7949],\n",
       " [42703, 8540, 3576],\n",
       " [42748, 8549, 4267],\n",
       " [42796, 8559, 5074],\n",
       " [42824, 8564, 1112],\n",
       " [42870, 8574, 375],\n",
       " [42979, 8595, 1737],\n",
       " [43087, 8617, 2491],\n",
       " [43089, 8617, 6240],\n",
       " [43103, 8620, 2329],\n",
       " [43104, 8620, 7703],\n",
       " [43144, 8628, 7535],\n",
       " [43175, 8635, 339],\n",
       " [43179, 8635, 339],\n",
       " [43184, 8636, 9136],\n",
       " [43260, 8652, 4292],\n",
       " [43297, 8659, 202],\n",
       " [43304, 8660, 7],\n",
       " [43318, 8663, 2916],\n",
       " [43319, 8663, 3102],\n",
       " [43495, 8699, 4342],\n",
       " [43501, 8700, 2587],\n",
       " [43509, 8701, 4562],\n",
       " [43561, 8712, 2724],\n",
       " [43563, 8712, 4207],\n",
       " [43579, 8715, 2829],\n",
       " [43618, 8723, 1090],\n",
       " [43717, 8743, 1227],\n",
       " [43718, 8743, 1227],\n",
       " [43734, 8746, 3554],\n",
       " [43774, 8754, 8018],\n",
       " [43825, 8765, 7317],\n",
       " [43904, 8780, 9443],\n",
       " [43944, 8788, 9266],\n",
       " [44004, 8800, 8168],\n",
       " [44028, 8805, 7302],\n",
       " [44057, 8811, 982],\n",
       " [44081, 8816, 4350],\n",
       " [44100, 8820, 1460],\n",
       " [44125, 8825, 6980],\n",
       " [44129, 8825, 6104],\n",
       " [44351, 8870, 9141],\n",
       " [44462, 8892, 8648],\n",
       " [44532, 8906, 9998],\n",
       " [44539, 8907, 6862],\n",
       " [44556, 8911, 8881],\n",
       " [44604, 8920, 7926],\n",
       " [44719, 8943, 9832],\n",
       " [44742, 8948, 9518],\n",
       " [44767, 8953, 8534],\n",
       " [44783, 8956, 7774],\n",
       " [44800, 8960, 2097],\n",
       " [44820, 8964, 6348],\n",
       " [44823, 8964, 6836],\n",
       " [44836, 8967, 7192],\n",
       " [44837, 8967, 5711],\n",
       " [44901, 8980, 2624],\n",
       " [44903, 8980, 6630],\n",
       " [44926, 8985, 1958],\n",
       " [44945, 8989, 6319],\n",
       " [44985, 8997, 2380],\n",
       " [44988, 8997, 6208],\n",
       " [44989, 8997, 6938],\n",
       " [45010, 9002, 2263],\n",
       " [45076, 9015, 5734],\n",
       " [45181, 9036, 7388],\n",
       " [45234, 9046, 3711],\n",
       " [45272, 9054, 7413],\n",
       " [45306, 9061, 7777],\n",
       " [45328, 9065, 2091],\n",
       " [45347, 9069, 2997],\n",
       " [45391, 9078, 3854],\n",
       " [45454, 9090, 5882],\n",
       " [45519, 9103, 930],\n",
       " [45547, 9109, 5861],\n",
       " [45573, 9114, 3871],\n",
       " [45639, 9127, 2623],\n",
       " [45682, 9136, 775],\n",
       " [45698, 9139, 9496],\n",
       " [45735, 9147, 8288],\n",
       " [45801, 9160, 8236],\n",
       " [45803, 9160, 8236],\n",
       " [45843, 9168, 1422],\n",
       " ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[p[0], p[0] // 5, p[1]] for p in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3194])\n",
      "tensor([0.4292])\n",
      "tensor([0.3341])\n",
      "tensor([0.3127])\n",
      "tensor([0.3013])\n",
      "tensor([0.2394])\n",
      "tensor([0.3463])\n",
      "tensor([0.3208])\n",
      "tensor([0.2430])\n",
      "tensor([0.2894])\n",
      "tensor([0.2881])\n",
      "tensor([0.2645])\n",
      "tensor([0.2695])\n",
      "tensor([0.3285])\n",
      "tensor([0.3847])\n",
      "tensor([0.2774])\n",
      "tensor([0.2923])\n",
      "tensor([0.2677])\n",
      "tensor([0.3260])\n",
      "tensor([0.3890])\n",
      "tensor([0.2798])\n",
      "tensor([0.2461])\n",
      "tensor([0.2777])\n",
      "tensor([0.2906])\n",
      "tensor([0.4307])\n",
      "tensor([0.3062])\n",
      "tensor([0.3286])\n",
      "tensor([0.2787])\n",
      "tensor([0.3194])\n",
      "tensor([0.2631])\n",
      "tensor([0.4182])\n",
      "tensor([0.3123])\n",
      "tensor([0.2536])\n",
      "tensor([0.3861])\n",
      "tensor([0.2700])\n",
      "tensor([0.2949])\n",
      "tensor([0.2824])\n",
      "tensor([0.3364])\n",
      "tensor([0.4109])\n",
      "tensor([0.2567])\n",
      "tensor([0.3801])\n",
      "tensor([0.3281])\n",
      "tensor([0.3089])\n",
      "tensor([0.2746])\n",
      "tensor([0.2650])\n",
      "tensor([0.3261])\n",
      "tensor([0.3264])\n",
      "tensor([0.2149])\n",
      "tensor([0.2769])\n",
      "tensor([0.2141])\n",
      "tensor([0.2110])\n",
      "tensor([0.3102])\n",
      "tensor([0.3731])\n",
      "tensor([0.2610])\n",
      "tensor([0.3515])\n",
      "tensor([0.2336])\n",
      "tensor([0.3052])\n",
      "tensor([0.3635])\n",
      "tensor([0.2491])\n",
      "tensor([0.2387])\n",
      "tensor([0.3524])\n",
      "tensor([0.3293])\n",
      "tensor([0.3906])\n",
      "tensor([0.2883])\n",
      "tensor([0.3697])\n",
      "tensor([0.3815])\n",
      "tensor([0.2954])\n",
      "tensor([0.2533])\n",
      "tensor([0.4965])\n",
      "tensor([0.2297])\n",
      "tensor([0.3268])\n",
      "tensor([0.2755])\n",
      "tensor([0.2477])\n",
      "tensor([0.3774])\n",
      "tensor([0.3852])\n",
      "tensor([0.3006])\n",
      "tensor([0.3552])\n",
      "tensor([0.2332])\n",
      "tensor([0.2513])\n",
      "tensor([0.3189])\n",
      "tensor([0.2616])\n",
      "tensor([0.2874])\n",
      "tensor([0.2743])\n",
      "tensor([0.3752])\n",
      "tensor([0.2615])\n",
      "tensor([0.3207])\n",
      "tensor([0.3197])\n",
      "tensor([0.2390])\n",
      "tensor([0.2643])\n",
      "tensor([0.2885])\n",
      "tensor([0.2455])\n",
      "tensor([0.2331])\n",
      "tensor([0.3208])\n",
      "tensor([0.3796])\n",
      "tensor([0.3640])\n",
      "tensor([0.3194])\n",
      "tensor([0.2811])\n",
      "tensor([0.2910])\n",
      "tensor([0.3190])\n",
      "tensor([0.3284])\n",
      "tensor([0.3563])\n",
      "tensor([0.2581])\n",
      "tensor([0.3346])\n",
      "tensor([0.3023])\n",
      "tensor([0.3251])\n",
      "tensor([0.4092])\n",
      "tensor([0.2494])\n",
      "tensor([0.2553])\n",
      "tensor([0.3308])\n",
      "tensor([0.3082])\n",
      "tensor([0.2623])\n",
      "tensor([0.2838])\n",
      "tensor([0.1480])\n",
      "tensor([0.3333])\n",
      "tensor([0.2591])\n",
      "tensor([0.2824])\n",
      "tensor([0.2700])\n",
      "tensor([0.3768])\n",
      "tensor([0.3898])\n",
      "tensor([0.2133])\n",
      "tensor([0.3465])\n",
      "tensor([0.3391])\n",
      "tensor([0.2728])\n",
      "tensor([0.3160])\n",
      "tensor([0.2907])\n",
      "tensor([0.2620])\n",
      "tensor([0.3008])\n",
      "tensor([0.2942])\n",
      "tensor([0.3409])\n",
      "tensor([0.4119])\n",
      "tensor([0.3011])\n",
      "tensor([0.3645])\n",
      "tensor([0.2734])\n",
      "tensor([0.3330])\n",
      "tensor([0.3499])\n",
      "tensor([0.2903])\n",
      "tensor([0.2517])\n",
      "tensor([0.3458])\n",
      "tensor([0.2705])\n",
      "tensor([0.2647])\n",
      "tensor([0.3166])\n",
      "tensor([0.2759])\n",
      "tensor([0.2153])\n",
      "tensor([0.3267])\n",
      "tensor([0.3221])\n",
      "tensor([0.2915])\n",
      "tensor([0.3474])\n",
      "tensor([0.4074])\n",
      "tensor([0.3363])\n",
      "tensor([0.4367])\n",
      "tensor([0.3074])\n",
      "tensor([0.3190])\n",
      "tensor([0.2667])\n",
      "tensor([0.2881])\n",
      "tensor([0.3137])\n",
      "tensor([0.1816])\n",
      "tensor([0.2186])\n",
      "tensor([0.3252])\n",
      "tensor([0.2804])\n",
      "tensor([0.3845])\n",
      "tensor([0.3259])\n",
      "tensor([0.3442])\n",
      "tensor([0.2884])\n",
      "tensor([0.3043])\n",
      "tensor([0.3584])\n",
      "tensor([0.3192])\n",
      "tensor([0.2603])\n",
      "tensor([0.3273])\n",
      "tensor([0.3190])\n",
      "tensor([0.2990])\n",
      "tensor([0.2331])\n",
      "tensor([0.2453])\n",
      "tensor([0.2740])\n",
      "tensor([0.3178])\n",
      "tensor([0.3459])\n",
      "tensor([0.2719])\n",
      "tensor([0.2227])\n",
      "tensor([0.3203])\n",
      "tensor([0.2793])\n",
      "tensor([0.3920])\n",
      "tensor([0.2823])\n",
      "tensor([0.2794])\n",
      "tensor([0.2770])\n",
      "tensor([0.3274])\n",
      "tensor([0.2350])\n",
      "tensor([0.3781])\n",
      "tensor([0.3031])\n",
      "tensor([0.2958])\n",
      "tensor([0.3168])\n",
      "tensor([0.3480])\n",
      "tensor([0.3817])\n",
      "tensor([0.3295])\n",
      "tensor([0.3941])\n",
      "tensor([0.3653])\n",
      "tensor([0.3469])\n",
      "tensor([0.2215])\n",
      "tensor([0.3245])\n",
      "tensor([0.3264])\n",
      "tensor([0.2270])\n",
      "tensor([0.2944])\n",
      "tensor([0.2576])\n",
      "tensor([0.3263])\n",
      "tensor([0.3972])\n",
      "tensor([0.2424])\n",
      "tensor([0.2573])\n",
      "tensor([0.2555])\n",
      "tensor([0.2860])\n",
      "tensor([0.2526])\n",
      "tensor([0.2729])\n",
      "tensor([0.3117])\n",
      "tensor([0.2442])\n",
      "tensor([0.3822])\n",
      "tensor([0.3079])\n",
      "tensor([0.2406])\n",
      "tensor([0.2839])\n",
      "tensor([0.3066])\n",
      "tensor([0.3363])\n",
      "tensor([0.3175])\n",
      "tensor([0.3430])\n",
      "tensor([0.3113])\n",
      "tensor([0.5065])\n",
      "tensor([0.4056])\n",
      "tensor([0.3382])\n",
      "tensor([0.3611])\n",
      "tensor([0.2935])\n",
      "tensor([0.3921])\n",
      "tensor([0.3190])\n",
      "tensor([0.3099])\n",
      "tensor([0.3233])\n",
      "tensor([0.3035])\n",
      "tensor([0.2630])\n",
      "tensor([0.3655])\n",
      "tensor([0.2867])\n",
      "tensor([0.3433])\n",
      "tensor([0.2878])\n",
      "tensor([0.2076])\n",
      "tensor([0.3019])\n",
      "tensor([0.3982])\n",
      "tensor([0.3698])\n",
      "tensor([0.3362])\n",
      "tensor([0.2928])\n",
      "tensor([0.4027])\n",
      "tensor([0.3156])\n",
      "tensor([0.2761])\n",
      "tensor([0.2497])\n",
      "tensor([0.3790])\n",
      "tensor([0.2440])\n",
      "tensor([0.2554])\n",
      "tensor([0.3529])\n",
      "tensor([0.2320])\n",
      "tensor([0.3457])\n",
      "tensor([0.3317])\n",
      "tensor([0.3446])\n",
      "tensor([0.3112])\n",
      "tensor([0.2839])\n",
      "tensor([0.3941])\n",
      "tensor([0.2856])\n",
      "tensor([0.2717])\n",
      "tensor([0.3508])\n",
      "tensor([0.3394])\n",
      "tensor([0.2938])\n",
      "tensor([0.3935])\n",
      "tensor([0.2884])\n",
      "tensor([0.3402])\n",
      "tensor([0.2702])\n",
      "tensor([0.3689])\n",
      "tensor([0.2782])\n",
      "tensor([0.2588])\n",
      "tensor([0.3256])\n",
      "tensor([0.2362])\n",
      "tensor([0.3444])\n",
      "tensor([0.3450])\n",
      "tensor([0.3012])\n",
      "tensor([0.3017])\n",
      "tensor([0.3100])\n",
      "tensor([0.3081])\n",
      "tensor([0.2257])\n",
      "tensor([0.2592])\n",
      "tensor([0.3055])\n",
      "tensor([0.5182])\n",
      "tensor([0.3761])\n",
      "tensor([0.4374])\n",
      "tensor([0.3429])\n",
      "tensor([0.2880])\n",
      "tensor([0.3941])\n",
      "tensor([0.3777])\n",
      "tensor([0.3318])\n",
      "tensor([0.2755])\n",
      "tensor([0.2689])\n",
      "tensor([0.5171])\n",
      "tensor([0.2658])\n",
      "tensor([0.3411])\n",
      "tensor([0.3117])\n",
      "tensor([0.2902])\n",
      "tensor([0.3255])\n",
      "tensor([0.3875])\n",
      "tensor([0.3768])\n",
      "tensor([0.4351])\n",
      "tensor([0.3303])\n",
      "tensor([0.3089])\n",
      "tensor([0.4073])\n",
      "tensor([0.2926])\n",
      "tensor([0.2881])\n",
      "tensor([0.2713])\n",
      "tensor([0.1920])\n",
      "tensor([0.3073])\n",
      "tensor([0.3585])\n",
      "tensor([0.3541])\n",
      "tensor([0.3555])\n",
      "tensor([0.3331])\n",
      "tensor([0.4810])\n",
      "tensor([0.3036])\n",
      "tensor([0.3195])\n",
      "tensor([0.3489])\n",
      "tensor([0.3068])\n",
      "tensor([0.2243])\n",
      "tensor([0.3004])\n",
      "tensor([0.3100])\n",
      "tensor([0.2303])\n",
      "tensor([0.2571])\n",
      "tensor([0.2546])\n",
      "tensor([0.3988])\n",
      "tensor([0.3599])\n",
      "tensor([0.3306])\n",
      "tensor([0.4193])\n",
      "tensor([0.4241])\n",
      "tensor([0.3198])\n",
      "tensor([0.3035])\n",
      "tensor([0.3417])\n",
      "tensor([0.2916])\n",
      "tensor([0.2858])\n",
      "tensor([0.3322])\n",
      "tensor([0.3932])\n",
      "tensor([0.3227])\n",
      "tensor([0.2852])\n",
      "tensor([0.2249])\n",
      "tensor([0.2604])\n",
      "tensor([0.4581])\n",
      "tensor([0.3431])\n",
      "tensor([0.3193])\n",
      "tensor([0.4167])\n",
      "tensor([0.3092])\n",
      "tensor([0.2484])\n",
      "tensor([0.3001])\n",
      "tensor([0.2289])\n",
      "tensor([0.3589])\n",
      "tensor([0.3394])\n",
      "tensor([0.2531])\n",
      "tensor([0.3763])\n",
      "tensor([0.2611])\n",
      "tensor([0.2627])\n",
      "tensor([0.3506])\n",
      "tensor([0.4505])\n",
      "tensor([0.2526])\n",
      "tensor([0.3150])\n",
      "tensor([0.2995])\n",
      "tensor([0.3197])\n",
      "tensor([0.2941])\n",
      "tensor([0.3169])\n",
      "tensor([0.3033])\n",
      "tensor([0.2468])\n",
      "tensor([0.2713])\n",
      "tensor([0.2669])\n",
      "tensor([0.3116])\n",
      "tensor([0.2806])\n",
      "tensor([0.2326])\n",
      "tensor([0.3411])\n",
      "tensor([0.2974])\n",
      "tensor([0.3478])\n",
      "tensor([0.1869])\n",
      "tensor([0.2678])\n",
      "tensor([0.4199])\n",
      "tensor([0.3295])\n",
      "tensor([0.3132])\n",
      "tensor([0.4155])\n",
      "tensor([0.3362])\n",
      "tensor([0.2651])\n",
      "tensor([0.2928])\n",
      "tensor([0.2406])\n",
      "tensor([0.3428])\n",
      "tensor([0.2890])\n",
      "tensor([0.2444])\n",
      "tensor([0.3194])\n",
      "tensor([0.4179])\n",
      "tensor([0.2508])\n",
      "tensor([0.2114])\n",
      "tensor([0.2985])\n",
      "tensor([0.3206])\n",
      "tensor([0.2993])\n",
      "tensor([0.3498])\n",
      "tensor([0.2607])\n",
      "tensor([0.2800])\n",
      "tensor([0.3017])\n",
      "tensor([0.3454])\n",
      "tensor([0.2910])\n",
      "tensor([0.3055])\n",
      "tensor([0.3091])\n",
      "tensor([0.3827])\n",
      "tensor([0.2836])\n",
      "tensor([0.3415])\n",
      "tensor([0.3520])\n",
      "tensor([0.2139])\n",
      "tensor([0.2420])\n",
      "tensor([0.3406])\n",
      "tensor([0.3271])\n",
      "tensor([0.2704])\n",
      "tensor([0.3352])\n",
      "tensor([0.4385])\n",
      "tensor([0.3419])\n",
      "tensor([0.2888])\n",
      "tensor([0.2668])\n",
      "tensor([0.2352])\n",
      "tensor([0.2884])\n",
      "tensor([0.2453])\n",
      "tensor([0.2533])\n",
      "tensor([0.2892])\n",
      "tensor([0.2470])\n",
      "tensor([0.3189])\n",
      "tensor([0.3122])\n",
      "tensor([0.3093])\n",
      "tensor([0.2710])\n",
      "tensor([0.4021])\n",
      "tensor([0.2907])\n",
      "tensor([0.2798])\n",
      "tensor([0.3281])\n",
      "tensor([0.2707])\n",
      "tensor([0.4624])\n",
      "tensor([0.4281])\n",
      "tensor([0.4569])\n",
      "tensor([0.2621])\n",
      "tensor([0.2551])\n",
      "tensor([0.3498])\n",
      "tensor([0.3420])\n",
      "tensor([0.2894])\n",
      "tensor([0.3729])\n",
      "tensor([0.3178])\n",
      "tensor([0.3623])\n",
      "tensor([0.3285])\n",
      "tensor([0.2425])\n",
      "tensor([0.2665])\n",
      "tensor([0.3135])\n",
      "tensor([0.2685])\n",
      "tensor([0.2797])\n",
      "tensor([0.2660])\n",
      "tensor([0.2757])\n",
      "tensor([0.2222])\n",
      "tensor([0.3858])\n",
      "tensor([0.3220])\n",
      "tensor([0.3203])\n",
      "tensor([0.4368])\n",
      "tensor([0.2411])\n",
      "tensor([0.2886])\n",
      "tensor([0.3093])\n",
      "tensor([0.3257])\n",
      "tensor([0.1872])\n",
      "tensor([0.2481])\n",
      "tensor([0.2976])\n",
      "tensor([0.2220])\n",
      "tensor([0.3333])\n",
      "tensor([0.3565])\n",
      "tensor([0.3725])\n",
      "tensor([0.2830])\n",
      "tensor([0.2436])\n",
      "tensor([0.2533])\n",
      "tensor([0.3413])\n",
      "tensor([0.2929])\n",
      "tensor([0.4577])\n",
      "tensor([0.2788])\n",
      "tensor([0.3376])\n",
      "tensor([0.2783])\n",
      "tensor([0.2630])\n",
      "tensor([0.3080])\n",
      "tensor([0.3270])\n",
      "tensor([0.2840])\n",
      "tensor([0.3189])\n",
      "tensor([0.3894])\n",
      "tensor([0.3716])\n",
      "tensor([0.2877])\n",
      "tensor([0.0230])\n",
      "tensor([0.3142])\n",
      "tensor([0.1513])\n",
      "tensor([0.3056])\n",
      "tensor([0.2645])\n",
      "tensor([0.2385])\n",
      "tensor([0.3244])\n",
      "tensor([0.2408])\n",
      "tensor([0.3768])\n",
      "tensor([0.3676])\n",
      "tensor([0.2715])\n",
      "tensor([0.4026])\n",
      "tensor([0.3337])\n",
      "tensor([0.2659])\n",
      "tensor([0.2169])\n",
      "tensor([0.3105])\n",
      "tensor([0.4352])\n",
      "tensor([0.3725])\n",
      "tensor([0.4755])\n",
      "tensor([0.2656])\n",
      "tensor([0.3156])\n",
      "tensor([0.2697])\n",
      "tensor([0.3851])\n",
      "tensor([0.3845])\n",
      "tensor([0.2741])\n",
      "tensor([0.2747])\n",
      "tensor([0.3391])\n",
      "tensor([0.3598])\n",
      "tensor([0.2865])\n",
      "tensor([0.2410])\n",
      "tensor([0.2877])\n",
      "tensor([0.2989])\n",
      "tensor([0.3952])\n",
      "tensor([0.3215])\n",
      "tensor([0.2817])\n",
      "tensor([0.3730])\n",
      "tensor([0.3123])\n",
      "tensor([0.3206])\n",
      "tensor([0.3591])\n",
      "tensor([0.2814])\n",
      "tensor([0.3036])\n",
      "tensor([0.3056])\n",
      "tensor([0.3793])\n",
      "tensor([0.0040])\n",
      "tensor([0.3484])\n",
      "tensor([0.3045])\n",
      "tensor([0.3500])\n",
      "tensor([0.4137])\n",
      "tensor([0.3308])\n",
      "tensor([0.4231])\n",
      "tensor([0.3908])\n",
      "tensor([0.2541])\n",
      "tensor([0.4197])\n",
      "tensor([0.3033])\n",
      "tensor([0.4761])\n",
      "tensor([0.3228])\n",
      "tensor([0.3629])\n",
      "tensor([0.2945])\n",
      "tensor([0.3470])\n",
      "tensor([0.4252])\n",
      "tensor([0.3237])\n",
      "tensor([0.4867])\n",
      "tensor([0.3999])\n",
      "tensor([0.3982])\n",
      "tensor([0.3190])\n",
      "tensor([0.2950])\n",
      "tensor([0.2331])\n",
      "tensor([0.2888])\n",
      "tensor([0.2539])\n",
      "tensor([0.3292])\n",
      "tensor([0.3815])\n",
      "tensor([0.3707])\n",
      "tensor([0.2810])\n",
      "tensor([0.2797])\n",
      "tensor([0.3480])\n",
      "tensor([0.3794])\n",
      "tensor([0.1665])\n",
      "tensor([0.3572])\n",
      "tensor([0.2570])\n",
      "tensor([0.3014])\n",
      "tensor([0.2930])\n",
      "tensor([0.3355])\n",
      "tensor([0.3004])\n",
      "tensor([0.3331])\n",
      "tensor([0.2664])\n",
      "tensor([0.2875])\n",
      "tensor([0.3393])\n",
      "tensor([0.2612])\n",
      "tensor([0.2103])\n",
      "tensor([0.3479])\n",
      "tensor([0.3417])\n",
      "tensor([0.3438])\n",
      "tensor([0.3406])\n",
      "tensor([0.2999])\n",
      "tensor([0.3540])\n",
      "tensor([0.3097])\n",
      "tensor([0.4269])\n",
      "tensor([0.3470])\n",
      "tensor([0.3656])\n",
      "tensor([0.3261])\n",
      "tensor([0.3482])\n",
      "tensor([0.2021])\n",
      "tensor([0.3269])\n",
      "tensor([0.2446])\n",
      "tensor([0.2940])\n",
      "tensor([0.2470])\n",
      "tensor([0.4537])\n",
      "tensor([0.3384])\n",
      "tensor([0.2060])\n",
      "tensor([0.3098])\n",
      "tensor([0.2896])\n",
      "tensor([0.3109])\n",
      "tensor([0.4256])\n",
      "tensor([0.3197])\n",
      "tensor([0.3245])\n",
      "tensor([0.3667])\n",
      "tensor([0.4086])\n",
      "tensor([0.3237])\n",
      "tensor([0.3089])\n",
      "tensor([0.3603])\n",
      "tensor([0.2979])\n",
      "tensor([0.4786])\n",
      "tensor([0.3796])\n",
      "tensor([0.2858])\n",
      "tensor([0.3482])\n",
      "tensor([0.2925])\n",
      "tensor([0.3140])\n",
      "tensor([0.3093])\n",
      "tensor([0.3232])\n",
      "tensor([0.2979])\n",
      "tensor([0.2231])\n",
      "tensor([0.3492])\n",
      "tensor([0.2947])\n",
      "tensor([0.2578])\n",
      "tensor([0.3059])\n",
      "tensor([0.3445])\n",
      "tensor([0.3229])\n",
      "tensor([0.3206])\n",
      "tensor([0.4080])\n",
      "tensor([0.2457])\n",
      "tensor([0.3050])\n",
      "tensor([0.2766])\n",
      "tensor([0.2348])\n",
      "tensor([0.3363])\n",
      "tensor([0.3383])\n",
      "tensor([0.0438])\n",
      "tensor([0.3132])\n",
      "tensor([0.2631])\n",
      "tensor([0.3049])\n",
      "tensor([0.2079])\n",
      "tensor([0.2638])\n",
      "tensor([0.3138])\n",
      "tensor([0.3409])\n",
      "tensor([0.1265])\n",
      "tensor([0.4119])\n",
      "tensor([0.3463])\n",
      "tensor([0.3872])\n",
      "tensor([0.3578])\n",
      "tensor([0.3397])\n",
      "tensor([0.2856])\n",
      "tensor([0.2984])\n",
      "tensor([0.2940])\n",
      "tensor([0.3495])\n",
      "tensor([0.2760])\n",
      "tensor([0.2641])\n",
      "tensor([0.3686])\n",
      "tensor([0.1448])\n",
      "tensor([0.2479])\n",
      "tensor([0.2959])\n",
      "tensor([0.2901])\n",
      "tensor([0.2868])\n",
      "tensor([0.3141])\n",
      "tensor([0.2937])\n",
      "tensor([0.3591])\n",
      "tensor([0.2537])\n",
      "tensor([0.2877])\n",
      "tensor([0.3794])\n",
      "tensor([0.3036])\n",
      "tensor([0.2694])\n",
      "tensor([0.0658])\n",
      "tensor([0.3633])\n",
      "tensor([0.2859])\n",
      "tensor([0.5166])\n",
      "tensor([0.3384])\n",
      "tensor([0.3180])\n",
      "tensor([0.4975])\n",
      "tensor([0.3404])\n",
      "tensor([0.3817])\n",
      "tensor([0.3582])\n",
      "tensor([0.2898])\n",
      "tensor([0.3078])\n",
      "tensor([0.3315])\n",
      "tensor([0.2951])\n",
      "tensor([0.3706])\n",
      "tensor([0.4194])\n",
      "tensor([0.4113])\n",
      "tensor([0.3188])\n",
      "tensor([0.3564])\n",
      "tensor([0.2983])\n",
      "tensor([0.4225])\n",
      "tensor([0.1693])\n",
      "tensor([0.3278])\n",
      "tensor([0.2507])\n",
      "tensor([0.2356])\n",
      "tensor([0.2458])\n",
      "tensor([0.2821])\n",
      "tensor([0.3457])\n",
      "tensor([0.3015])\n",
      "tensor([0.3565])\n",
      "tensor([0.2214])\n",
      "tensor([0.3650])\n",
      "tensor([0.2999])\n",
      "tensor([0.2862])\n",
      "tensor([0.2768])\n",
      "tensor([0.2969])\n",
      "tensor([0.3792])\n",
      "tensor([0.3508])\n",
      "tensor([0.2929])\n",
      "tensor([0.3254])\n",
      "tensor([0.3727])\n",
      "tensor([0.4456])\n",
      "tensor([0.2502])\n",
      "tensor([0.3597])\n",
      "tensor([0.3197])\n",
      "tensor([0.3974])\n",
      "tensor([0.2719])\n",
      "tensor([0.3212])\n",
      "tensor([0.3150])\n",
      "tensor([0.3924])\n",
      "tensor([0.3677])\n",
      "tensor([0.3209])\n",
      "tensor([0.2973])\n",
      "tensor([0.3168])\n",
      "tensor([0.2791])\n",
      "tensor([0.4150])\n",
      "tensor([0.3011])\n",
      "tensor([0.3822])\n",
      "tensor([0.3910])\n",
      "tensor([0.2913])\n",
      "tensor([0.3330])\n",
      "tensor([0.2945])\n",
      "tensor([0.3152])\n",
      "tensor([0.2172])\n",
      "tensor([0.2829])\n",
      "tensor([0.3685])\n",
      "tensor([0.2761])\n",
      "tensor([0.3280])\n",
      "tensor([0.2398])\n",
      "tensor([0.2880])\n",
      "tensor([0.3744])\n",
      "tensor([0.3027])\n",
      "tensor([0.2457])\n",
      "tensor([0.2712])\n",
      "tensor([0.3081])\n",
      "tensor([0.3292])\n",
      "tensor([0.3870])\n",
      "tensor([0.3558])\n",
      "tensor([0.3449])\n",
      "tensor([0.3265])\n",
      "tensor([0.3212])\n",
      "tensor([0.2253])\n",
      "tensor([0.2006])\n",
      "tensor([0.3334])\n",
      "tensor([0.4186])\n",
      "tensor([0.2819])\n",
      "tensor([0.3397])\n",
      "tensor([0.3214])\n",
      "tensor([0.3453])\n",
      "tensor([0.2842])\n",
      "tensor([0.3143])\n",
      "tensor([0.2891])\n",
      "tensor([0.2971])\n",
      "tensor([0.3916])\n",
      "tensor([0.2690])\n",
      "tensor([0.3373])\n",
      "tensor([0.3724])\n",
      "tensor([0.3322])\n",
      "tensor([0.2942])\n",
      "tensor([0.3900])\n",
      "tensor([0.3336])\n",
      "tensor([0.3057])\n",
      "tensor([0.3056])\n",
      "tensor([0.3649])\n",
      "tensor([0.3229])\n",
      "tensor([0.2258])\n",
      "tensor([0.2913])\n",
      "tensor([0.2177])\n",
      "tensor([0.3177])\n",
      "tensor([0.4014])\n",
      "tensor([0.3180])\n",
      "tensor([0.2968])\n",
      "tensor([0.3492])\n",
      "tensor([0.3023])\n",
      "tensor([0.2433])\n",
      "tensor([0.3649])\n",
      "tensor([0.2916])\n",
      "tensor([0.3651])\n",
      "tensor([0.2176])\n",
      "tensor([0.4097])\n",
      "tensor([0.3620])\n",
      "tensor([0.2669])\n",
      "tensor([0.2332])\n",
      "tensor([0.3505])\n",
      "tensor([0.3756])\n",
      "tensor([0.4642])\n",
      "tensor([0.2888])\n",
      "tensor([0.3655])\n",
      "tensor([0.2183])\n",
      "tensor([0.1934])\n",
      "tensor([0.2024])\n",
      "tensor([0.2789])\n",
      "tensor([0.3325])\n",
      "tensor([0.3310])\n",
      "tensor([0.2552])\n",
      "tensor([0.3196])\n",
      "tensor([0.2491])\n",
      "tensor([0.3353])\n",
      "tensor([0.3032])\n",
      "tensor([0.2642])\n",
      "tensor([0.3028])\n",
      "tensor([0.3299])\n",
      "tensor([0.3430])\n",
      "tensor([0.3297])\n",
      "tensor([0.3606])\n",
      "tensor([0.3961])\n",
      "tensor([0.4015])\n",
      "tensor([0.3366])\n",
      "tensor([0.3112])\n",
      "tensor([0.2846])\n",
      "tensor([0.3217])\n",
      "tensor([0.2938])\n",
      "tensor([0.4014])\n",
      "tensor([0.3007])\n",
      "tensor([0.2977])\n",
      "tensor([0.2578])\n",
      "tensor([0.3924])\n",
      "tensor([0.2912])\n",
      "tensor([0.3376])\n",
      "tensor([0.3492])\n",
      "tensor([0.2668])\n",
      "tensor([0.2873])\n",
      "tensor([0.3098])\n",
      "tensor([0.3713])\n",
      "tensor([0.3527])\n",
      "tensor([0.3283])\n",
      "tensor([0.3747])\n",
      "tensor([0.3368])\n",
      "tensor([0.3824])\n",
      "tensor([0.2633])\n",
      "tensor([0.2263])\n",
      "tensor([0.2845])\n",
      "tensor([0.2580])\n",
      "tensor([0.3567])\n",
      "tensor([0.2491])\n",
      "tensor([0.3079])\n",
      "tensor([0.3502])\n",
      "tensor([0.3948])\n",
      "tensor([0.3084])\n",
      "tensor([0.2722])\n",
      "tensor([0.3520])\n",
      "tensor([0.4850])\n",
      "tensor([0.4107])\n",
      "tensor([0.3349])\n",
      "tensor([0.3826])\n",
      "tensor([0.2925])\n",
      "tensor([0.2523])\n",
      "tensor([0.3158])\n",
      "tensor([0.2624])\n",
      "tensor([0.2384])\n",
      "tensor([0.2296])\n",
      "tensor([0.2916])\n",
      "tensor([0.2421])\n",
      "tensor([0.4409])\n",
      "tensor([0.3409])\n",
      "tensor([0.3132])\n",
      "tensor([0.2839])\n",
      "tensor([0.3296])\n",
      "tensor([0.3904])\n",
      "tensor([0.2843])\n",
      "tensor([0.3556])\n",
      "tensor([0.3197])\n",
      "tensor([0.2697])\n",
      "tensor([0.3270])\n",
      "tensor([0.2059])\n",
      "tensor([0.2812])\n",
      "tensor([0.2937])\n",
      "tensor([0.3315])\n",
      "tensor([0.2518])\n",
      "tensor([0.3037])\n",
      "tensor([0.3147])\n",
      "tensor([0.3556])\n",
      "tensor([0.3376])\n",
      "tensor([0.3793])\n",
      "tensor([0.2578])\n",
      "tensor([0.2131])\n",
      "tensor([0.3394])\n",
      "tensor([0.2741])\n",
      "tensor([0.3103])\n",
      "tensor([0.3245])\n",
      "tensor([0.2930])\n",
      "tensor([0.2996])\n",
      "tensor([0.2559])\n",
      "tensor([0.2789])\n",
      "tensor([0.3774])\n",
      "tensor([0.2909])\n",
      "tensor([0.2966])\n",
      "tensor([0.2551])\n",
      "tensor([0.2050])\n",
      "tensor([0.2165])\n",
      "tensor([0.2220])\n",
      "tensor([0.2868])\n",
      "tensor([0.3155])\n",
      "tensor([0.2223])\n",
      "tensor([0.3371])\n",
      "tensor([0.3714])\n",
      "tensor([0.2785])\n",
      "tensor([0.2999])\n",
      "tensor([0.3101])\n",
      "tensor([0.3764])\n",
      "tensor([0.2480])\n",
      "tensor([0.3101])\n",
      "tensor([0.2696])\n",
      "tensor([0.3070])\n",
      "tensor([0.3844])\n",
      "tensor([0.3457])\n",
      "tensor([0.2591])\n",
      "tensor([0.2809])\n",
      "tensor([0.2774])\n",
      "tensor([0.3278])\n",
      "tensor([0.3516])\n",
      "tensor([0.2754])\n",
      "tensor([0.3644])\n",
      "tensor([0.3592])\n",
      "tensor([0.3395])\n",
      "tensor([0.2256])\n",
      "tensor([0.3351])\n",
      "tensor([0.3664])\n",
      "tensor([0.4216])\n",
      "tensor([0.3166])\n",
      "tensor([0.2047])\n",
      "tensor([0.2554])\n",
      "tensor([0.2270])\n",
      "tensor([0.3402])\n",
      "tensor([0.3269])\n",
      "tensor([0.3076])\n",
      "tensor([0.3799])\n",
      "tensor([0.3475])\n",
      "tensor([0.3745])\n",
      "tensor([0.3975])\n",
      "tensor([0.3621])\n",
      "tensor([0.2959])\n",
      "tensor([0.3354])\n",
      "tensor([0.3121])\n",
      "tensor([0.2755])\n",
      "tensor([0.2912])\n",
      "tensor([0.3194])\n",
      "tensor([0.2846])\n",
      "tensor([0.2774])\n",
      "tensor([0.2343])\n",
      "tensor([0.3128])\n",
      "tensor([0.3176])\n",
      "tensor([0.3785])\n",
      "tensor([0.3592])\n",
      "tensor([0.3071])\n",
      "tensor([0.2826])\n",
      "tensor([0.3023])\n",
      "tensor([0.3092])\n",
      "tensor([0.3093])\n",
      "tensor([0.2529])\n",
      "tensor([0.3709])\n",
      "tensor([0.4142])\n",
      "tensor([0.2566])\n",
      "tensor([0.2762])\n",
      "tensor([0.3565])\n",
      "tensor([0.2451])\n",
      "tensor([0.3209])\n",
      "tensor([0.3207])\n",
      "tensor([0.3753])\n",
      "tensor([0.3338])\n",
      "tensor([0.2813])\n",
      "tensor([0.3768])\n",
      "tensor([0.3437])\n",
      "tensor([0.2281])\n",
      "tensor([0.2066])\n",
      "tensor([0.2797])\n",
      "tensor([0.2710])\n",
      "tensor([0.2803])\n",
      "tensor([0.3612])\n",
      "tensor([0.2070])\n",
      "tensor([0.3808])\n",
      "tensor([0.3071])\n",
      "tensor([0.3861])\n",
      "tensor([0.3461])\n",
      "tensor([0.4271])\n",
      "tensor([0.3680])\n",
      "tensor([0.2325])\n",
      "tensor([0.2050])\n",
      "tensor([0.3662])\n",
      "tensor([0.3130])\n",
      "tensor([0.4823])\n",
      "tensor([0.2912])\n",
      "tensor([0.3356])\n",
      "tensor([0.3157])\n",
      "tensor([0.2309])\n",
      "tensor([0.3006])\n",
      "tensor([0.2972])\n",
      "tensor([0.3353])\n",
      "tensor([0.2782])\n",
      "tensor([0.3426])\n",
      "tensor([0.2206])\n",
      "tensor([0.2864])\n",
      "tensor([0.3643])\n",
      "tensor([0.3015])\n",
      "tensor([0.3272])\n",
      "tensor([0.3066])\n",
      "tensor([0.3490])\n",
      "tensor([0.2804])\n",
      "tensor([0.2671])\n",
      "tensor([0.3032])\n",
      "tensor([0.3555])\n",
      "tensor([0.3496])\n",
      "tensor([0.4093])\n",
      "tensor([0.3009])\n",
      "tensor([0.2908])\n",
      "tensor([0.2841])\n",
      "tensor([0.3596])\n",
      "tensor([0.2546])\n",
      "tensor([0.2792])\n",
      "tensor([0.3202])\n",
      "tensor([0.3278])\n",
      "tensor([0.3302])\n",
      "tensor([0.2080])\n",
      "tensor([0.3179])\n",
      "tensor([0.3068])\n",
      "tensor([0.2461])\n",
      "tensor([0.3515])\n",
      "tensor([0.3971])\n",
      "tensor([0.3066])\n",
      "tensor([0.3485])\n",
      "tensor([0.3385])\n",
      "tensor([0.2907])\n",
      "tensor([0.1959])\n",
      "tensor([0.2020])\n",
      "tensor([0.2507])\n",
      "tensor([0.2857])\n",
      "tensor([0.3175])\n",
      "tensor([0.3537])\n",
      "tensor([0.3619])\n",
      "tensor([0.3165])\n",
      "tensor([0.2869])\n",
      "tensor([0.2223])\n",
      "tensor([0.3555])\n",
      "tensor([0.4082])\n",
      "tensor([0.4112])\n",
      "tensor([0.4521])\n",
      "tensor([0.2846])\n",
      "tensor([0.2540])\n",
      "tensor([0.2987])\n",
      "tensor([0.3495])\n",
      "tensor([0.3452])\n",
      "tensor([0.4480])\n",
      "tensor([0.3124])\n",
      "tensor([0.2842])\n",
      "tensor([0.3798])\n",
      "tensor([0.2476])\n",
      "tensor([0.2820])\n",
      "tensor([0.2966])\n",
      "tensor([0.3847])\n",
      "tensor([0.3689])\n",
      "tensor([0.3227])\n",
      "tensor([0.3009])\n",
      "tensor([0.3156])\n",
      "tensor([0.2835])\n",
      "tensor([0.2506])\n",
      "tensor([0.2554])\n",
      "tensor([0.3043])\n",
      "tensor([0.2716])\n",
      "tensor([0.3113])\n",
      "tensor([0.2128])\n",
      "tensor([0.4620])\n",
      "tensor([0.3473])\n",
      "tensor([0.2981])\n",
      "tensor([0.3663])\n",
      "tensor([0.2765])\n",
      "tensor([0.3025])\n",
      "tensor([0.3874])\n",
      "tensor([0.2795])\n",
      "tensor([0.2171])\n",
      "tensor([0.3217])\n",
      "tensor([0.2983])\n",
      "tensor([0.2901])\n",
      "tensor([0.3918])\n",
      "tensor([0.2319])\n",
      "tensor([0.2669])\n",
      "tensor([0.3065])\n",
      "tensor([0.3747])\n",
      "tensor([0.2777])\n",
      "tensor([0.1995])\n",
      "tensor([0.2148])\n",
      "tensor([0.3178])\n",
      "tensor([0.3857])\n",
      "tensor([0.3256])\n",
      "tensor([0.2447])\n",
      "tensor([0.2868])\n",
      "tensor([0.2780])\n",
      "tensor([0.2683])\n",
      "tensor([0.2443])\n",
      "tensor([0.3718])\n",
      "tensor([0.3089])\n",
      "tensor([0.2417])\n",
      "tensor([0.3596])\n",
      "tensor([0.3860])\n",
      "tensor([0.2468])\n",
      "tensor([0.2969])\n",
      "tensor([0.3551])\n",
      "tensor([0.2281])\n",
      "tensor([0.2242])\n",
      "tensor([0.3488])\n",
      "tensor([0.2943])\n",
      "tensor([0.4273])\n",
      "tensor([0.3424])\n",
      "tensor([0.3201])\n",
      "tensor([0.2463])\n",
      "tensor([0.3114])\n",
      "tensor([0.2789])\n",
      "tensor([0.2719])\n",
      "tensor([0.2986])\n",
      "tensor([0.2524])\n",
      "tensor([0.3799])\n",
      "tensor([0.2238])\n",
      "tensor([0.2487])\n",
      "tensor([0.4976])\n",
      "tensor([0.3068])\n",
      "tensor([0.3810])\n",
      "tensor([0.2471])\n",
      "tensor([0.3292])\n",
      "tensor([0.2274])\n",
      "tensor([0.3330])\n",
      "tensor([0.2747])\n",
      "tensor([0.3339])\n",
      "tensor([0.3220])\n",
      "tensor([0.2856])\n",
      "tensor([0.2785])\n",
      "tensor([0.2940])\n",
      "tensor([0.2067])\n",
      "tensor([0.2671])\n",
      "tensor([0.3429])\n",
      "tensor([0.2302])\n",
      "tensor([0.2647])\n"
     ]
    }
   ],
   "source": [
    "xs = []\n",
    "for p in pairs:\n",
    "    x = utility_functions.cos_sim(torch.Tensor(np.array([testx[p[0]]])), torch.Tensor(np.array([traincenterx[p[1]]])))[0]\n",
    "    # x = sofmax_values[p[0]]\n",
    "    print(x)\n",
    "    # if 0.2 < x < 0.3:\n",
    "    #     print(p, x)\n",
    "    #     xs.append(x)\n",
    "# np.min(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "47\n",
      "48\n",
      "88\n",
      "96\n",
      "98\n",
      "116\n",
      "220\n",
      "228\n",
      "229\n",
      "256\n",
      "349\n",
      "411\n",
      "434\n",
      "440\n",
      "444\n",
      "500\n",
      "563\n",
      "642\n",
      "648\n",
      "691\n",
      "715\n",
      "728\n",
      "740\n",
      "745\n",
      "768\n",
      "829\n",
      "899\n",
      "934\n",
      "1029\n",
      "1113\n",
      "1149\n",
      "1160\n",
      "1169\n",
      "1190\n",
      "1218\n",
      "1273\n",
      "1298\n",
      "1299\n",
      "1361\n",
      "1383\n",
      "1391\n",
      "1486\n",
      "1578\n",
      "1712\n",
      "1800\n",
      "1829\n",
      "1900\n",
      "1901\n",
      "1929\n",
      "1956\n",
      "1993\n",
      "1999\n",
      "2010\n",
      "2077\n",
      "2084\n",
      "2099\n",
      "2104\n",
      "2117\n",
      "2125\n",
      "2141\n",
      "2142\n",
      "2153\n",
      "2199\n",
      "2205\n",
      "2227\n",
      "2322\n",
      "2402\n",
      "2412\n",
      "2413\n",
      "2414\n",
      "2438\n",
      "2439\n",
      "2482\n",
      "2623\n",
      "2627\n",
      "2690\n",
      "2691\n",
      "2772\n",
      "2782\n",
      "2839\n",
      "2884\n",
      "2902\n",
      "3011\n",
      "3031\n",
      "3050\n",
      "3074\n",
      "3129\n",
      "3164\n",
      "3167\n",
      "3171\n",
      "3178\n",
      "3187\n",
      "3197\n",
      "3217\n",
      "3233\n",
      "3276\n",
      "3303\n",
      "3317\n",
      "3387\n",
      "3393\n",
      "3435\n",
      "3446\n",
      "3451\n",
      "3452\n",
      "3453\n",
      "3454\n",
      "3544\n",
      "3554\n",
      "3569\n",
      "3716\n",
      "3720\n",
      "3876\n",
      "3989\n",
      "4018\n",
      "4097\n",
      "4119\n",
      "4128\n",
      "4143\n",
      "4148\n",
      "4155\n",
      "4162\n",
      "4163\n",
      "4164\n",
      "4223\n",
      "4272\n",
      "4285\n",
      "4292\n",
      "4334\n",
      "4394\n",
      "4433\n",
      "4488\n",
      "4523\n",
      "4535\n",
      "4549\n",
      "4572\n",
      "4640\n",
      "4642\n",
      "4746\n",
      "4760\n",
      "4769\n",
      "4834\n",
      "4937\n",
      "5004\n",
      "5098\n",
      "5127\n",
      "5156\n",
      "5165\n",
      "5168\n",
      "5198\n",
      "5246\n",
      "5479\n",
      "5494\n",
      "5604\n",
      "5617\n",
      "5619\n",
      "5625\n",
      "5788\n",
      "5792\n",
      "5837\n",
      "5864\n",
      "5893\n",
      "5927\n",
      "5939\n",
      "5947\n",
      "5949\n",
      "5962\n",
      "5966\n",
      "6015\n",
      "6024\n",
      "6052\n",
      "6082\n",
      "6094\n",
      "6173\n",
      "6197\n",
      "6241\n",
      "6253\n",
      "6258\n",
      "6372\n",
      "6373\n",
      "6384\n",
      "6464\n",
      "6465\n",
      "6603\n",
      "6689\n",
      "6725\n",
      "6727\n",
      "6773\n",
      "6777\n",
      "6779\n",
      "6797\n",
      "6804\n",
      "6819\n",
      "6852\n",
      "6906\n",
      "6926\n",
      "6953\n",
      "6993\n",
      "6994\n",
      "6998\n",
      "7009\n",
      "7039\n",
      "7040\n",
      "7079\n",
      "7094\n",
      "7104\n",
      "7109\n",
      "7367\n",
      "7368\n",
      "7397\n",
      "7409\n",
      "7522\n",
      "7532\n",
      "7558\n",
      "7578\n",
      "7583\n",
      "7611\n",
      "7661\n",
      "7664\n",
      "7679\n",
      "7770\n",
      "7849\n",
      "7912\n",
      "7913\n",
      "7914\n",
      "7948\n",
      "7977\n",
      "7989\n",
      "8073\n",
      "8103\n",
      "8251\n",
      "8278\n",
      "8299\n",
      "8308\n",
      "8314\n",
      "8369\n",
      "8406\n",
      "8531\n",
      "8587\n",
      "8604\n",
      "8607\n",
      "8628\n",
      "8651\n",
      "8784\n",
      "8844\n",
      "8872\n",
      "8909\n",
      "8933\n",
      "8974\n",
      "9051\n",
      "9058\n",
      "9149\n",
      "9152\n",
      "9220\n",
      "9320\n",
      "9353\n",
      "9383\n",
      "9436\n",
      "9479\n",
      "9488\n",
      "9535\n",
      "9569\n",
      "9593\n",
      "9705\n",
      "9717\n",
      "9807\n",
      "9879\n",
      "9962\n",
      "9971\n",
      "9972\n",
      "10136\n",
      "10235\n",
      "10310\n",
      "10342\n",
      "10350\n",
      "10442\n",
      "10527\n",
      "10649\n",
      "10693\n",
      "10725\n",
      "10880\n",
      "10996\n",
      "11011\n",
      "11027\n",
      "11086\n",
      "11099\n",
      "11135\n",
      "11137\n",
      "11204\n",
      "11258\n",
      "11292\n",
      "11345\n",
      "11361\n",
      "11379\n",
      "11386\n",
      "11388\n",
      "11423\n",
      "11467\n",
      "11532\n",
      "11572\n",
      "11575\n",
      "11579\n",
      "11602\n",
      "11603\n",
      "11624\n",
      "11639\n",
      "11656\n",
      "11715\n",
      "11737\n",
      "11790\n",
      "11878\n",
      "11987\n",
      "11994\n",
      "12014\n",
      "12154\n",
      "12155\n",
      "12198\n",
      "12215\n",
      "12222\n",
      "12319\n",
      "12397\n",
      "12430\n",
      "12536\n",
      "12562\n",
      "12587\n",
      "12629\n",
      "12646\n",
      "12677\n",
      "12736\n",
      "12750\n",
      "12789\n",
      "12825\n",
      "12828\n",
      "12849\n",
      "12860\n",
      "12927\n",
      "12928\n",
      "12965\n",
      "12968\n",
      "13011\n",
      "13012\n",
      "13013\n",
      "13037\n",
      "13044\n",
      "13118\n",
      "13133\n",
      "13145\n",
      "13146\n",
      "13149\n",
      "13151\n",
      "13199\n",
      "13214\n",
      "13268\n",
      "13386\n",
      "13482\n",
      "13546\n",
      "13589\n",
      "13594\n",
      "13640\n",
      "13644\n",
      "13744\n",
      "13794\n",
      "13813\n",
      "13892\n",
      "13904\n",
      "13927\n",
      "13944\n",
      "13974\n",
      "14072\n",
      "14097\n",
      "14121\n",
      "14157\n",
      "14264\n",
      "14274\n",
      "14369\n",
      "14370\n",
      "14393\n",
      "14476\n",
      "14513\n",
      "14565\n",
      "14568\n",
      "14569\n",
      "14583\n",
      "14681\n",
      "14682\n",
      "14683\n",
      "14684\n",
      "14693\n",
      "14705\n",
      "14768\n",
      "14811\n",
      "14856\n",
      "14864\n",
      "14888\n",
      "14981\n",
      "15120\n",
      "15121\n",
      "15124\n",
      "15132\n",
      "15142\n",
      "15157\n",
      "15178\n",
      "15180\n",
      "15183\n",
      "15206\n",
      "15303\n",
      "15309\n",
      "15318\n",
      "15332\n",
      "15395\n",
      "15417\n",
      "15452\n",
      "15473\n",
      "15548\n",
      "15557\n",
      "15558\n",
      "15561\n",
      "15567\n",
      "15616\n",
      "15657\n",
      "15690\n",
      "15764\n",
      "15822\n",
      "15989\n",
      "16027\n",
      "16083\n",
      "16112\n",
      "16180\n",
      "16324\n",
      "16452\n",
      "16517\n",
      "16520\n",
      "16522\n",
      "16523\n",
      "16553\n",
      "16578\n",
      "16638\n",
      "16655\n",
      "16661\n",
      "16664\n",
      "16757\n",
      "16784\n",
      "16891\n",
      "16892\n",
      "16894\n",
      "16933\n",
      "16934\n",
      "16989\n",
      "17040\n",
      "17046\n",
      "17188\n",
      "17198\n",
      "17267\n",
      "17294\n",
      "17342\n",
      "17425\n",
      "17429\n",
      "17467\n",
      "17468\n",
      "17470\n",
      "17481\n",
      "17587\n",
      "17619\n",
      "17677\n",
      "17697\n",
      "17703\n",
      "17778\n",
      "17789\n",
      "17871\n",
      "17927\n",
      "17944\n",
      "17959\n",
      "18062\n",
      "18064\n",
      "18101\n",
      "18102\n",
      "18152\n",
      "18157\n",
      "18168\n",
      "18198\n",
      "18297\n",
      "18301\n",
      "18313\n",
      "18356\n",
      "18495\n",
      "18502\n",
      "18526\n",
      "18528\n",
      "18649\n",
      "18729\n",
      "18775\n",
      "18776\n",
      "18826\n",
      "18827\n",
      "18887\n",
      "18926\n",
      "18932\n",
      "18972\n",
      "19102\n",
      "19118\n",
      "19144\n",
      "19154\n",
      "19225\n",
      "19228\n",
      "19229\n",
      "19352\n",
      "19373\n",
      "19394\n",
      "19400\n",
      "19470\n",
      "19490\n",
      "19491\n",
      "19494\n",
      "19563\n",
      "19723\n",
      "19821\n",
      "19822\n",
      "19823\n",
      "19842\n",
      "19878\n",
      "19884\n",
      "19888\n",
      "19894\n",
      "19896\n",
      "19918\n",
      "19943\n",
      "19985\n",
      "20078\n",
      "20090\n",
      "20185\n",
      "20189\n",
      "20239\n",
      "20256\n",
      "20263\n",
      "20340\n",
      "20394\n",
      "20463\n",
      "20467\n",
      "20543\n",
      "20599\n",
      "20614\n",
      "20693\n",
      "20698\n",
      "20700\n",
      "20704\n",
      "20733\n",
      "20778\n",
      "20804\n",
      "20922\n",
      "20924\n",
      "20937\n",
      "21037\n",
      "21067\n",
      "21138\n",
      "21158\n",
      "21243\n",
      "21369\n",
      "21389\n",
      "21422\n",
      "21424\n",
      "21440\n",
      "21481\n",
      "21654\n",
      "21722\n",
      "21753\n",
      "21872\n",
      "21883\n",
      "21889\n",
      "21934\n",
      "21973\n",
      "21974\n",
      "21996\n",
      "21997\n",
      "22048\n",
      "22124\n",
      "22174\n",
      "22286\n",
      "22288\n",
      "22299\n",
      "22337\n",
      "22354\n",
      "22446\n",
      "22489\n",
      "22506\n",
      "22513\n",
      "22527\n",
      "22539\n",
      "22611\n",
      "22678\n",
      "22679\n",
      "22688\n",
      "22777\n",
      "22857\n",
      "22864\n",
      "23102\n",
      "23105\n",
      "23108\n",
      "23147\n",
      "23159\n",
      "23222\n",
      "23235\n",
      "23266\n",
      "23292\n",
      "23320\n",
      "23328\n",
      "23340\n",
      "23344\n",
      "23391\n",
      "23497\n",
      "23506\n",
      "23679\n",
      "23764\n",
      "23769\n",
      "23855\n",
      "23913\n",
      "23923\n",
      "23949\n",
      "23958\n",
      "23982\n",
      "24090\n",
      "24162\n",
      "24230\n",
      "24280\n",
      "24310\n",
      "24357\n",
      "24376\n",
      "24456\n",
      "24485\n",
      "24547\n",
      "24549\n",
      "24622\n",
      "24628\n",
      "24674\n",
      "24769\n",
      "24866\n",
      "24894\n",
      "24909\n",
      "24941\n",
      "25067\n",
      "25094\n",
      "25137\n",
      "25139\n",
      "25244\n",
      "25278\n",
      "25284\n",
      "25302\n",
      "25338\n",
      "25385\n",
      "25449\n",
      "25456\n",
      "25609\n",
      "25637\n",
      "25639\n",
      "25654\n",
      "25657\n",
      "25659\n",
      "25674\n",
      "25720\n",
      "25721\n",
      "25815\n",
      "25818\n",
      "25827\n",
      "25828\n",
      "25848\n",
      "25890\n",
      "25919\n",
      "25959\n",
      "25973\n",
      "25985\n",
      "25987\n",
      "25989\n",
      "26017\n",
      "26036\n",
      "26038\n",
      "26058\n",
      "26059\n",
      "26081\n",
      "26084\n",
      "26106\n",
      "26107\n",
      "26176\n",
      "26178\n",
      "26253\n",
      "26254\n",
      "26259\n",
      "26262\n",
      "26294\n",
      "26305\n",
      "26307\n",
      "26308\n",
      "26309\n",
      "26383\n",
      "26394\n",
      "26449\n",
      "26504\n",
      "26533\n",
      "26552\n",
      "26612\n",
      "26659\n",
      "26669\n",
      "26811\n",
      "26841\n",
      "26900\n",
      "26902\n",
      "26903\n",
      "26904\n",
      "26981\n",
      "27064\n",
      "27067\n",
      "27070\n",
      "27084\n",
      "27171\n",
      "27192\n",
      "27194\n",
      "27221\n",
      "27224\n",
      "27228\n",
      "27268\n",
      "27269\n",
      "27301\n",
      "27354\n",
      "27374\n",
      "27435\n",
      "27459\n",
      "27490\n",
      "27511\n",
      "27537\n",
      "27566\n",
      "27578\n",
      "27688\n",
      "27722\n",
      "27793\n",
      "27836\n",
      "27837\n",
      "27923\n",
      "27982\n",
      "27988\n",
      "27989\n",
      "28020\n",
      "28094\n",
      "28146\n",
      "28194\n",
      "28235\n",
      "28292\n",
      "28308\n",
      "28313\n",
      "28315\n",
      "28343\n",
      "28389\n",
      "28422\n",
      "28423\n",
      "28424\n",
      "28443\n",
      "28444\n",
      "28492\n",
      "28518\n",
      "28524\n",
      "28534\n",
      "28629\n",
      "28696\n",
      "28717\n",
      "28723\n",
      "28828\n",
      "28829\n",
      "28858\n",
      "28861\n",
      "28863\n",
      "28867\n",
      "28969\n",
      "29070\n",
      "29071\n",
      "29209\n",
      "29211\n",
      "29282\n",
      "29291\n",
      "29357\n",
      "29359\n",
      "29398\n",
      "29473\n",
      "29519\n",
      "29535\n",
      "29592\n",
      "29593\n",
      "29594\n",
      "29639\n",
      "29663\n",
      "29742\n",
      "29787\n",
      "29832\n",
      "29848\n",
      "29849\n",
      "29909\n",
      "29932\n",
      "29940\n",
      "29997\n",
      "30015\n",
      "30071\n",
      "30086\n",
      "30123\n",
      "30211\n",
      "30274\n",
      "30372\n",
      "30400\n",
      "30483\n",
      "30488\n",
      "30518\n",
      "30572\n",
      "30573\n",
      "30645\n",
      "30647\n",
      "30655\n",
      "30728\n",
      "30736\n",
      "30754\n",
      "30777\n",
      "30883\n",
      "30912\n",
      "30981\n",
      "30982\n",
      "31026\n",
      "31027\n",
      "31073\n",
      "31097\n",
      "31099\n",
      "31128\n",
      "31164\n",
      "31178\n",
      "31184\n",
      "31227\n",
      "31244\n",
      "31341\n",
      "31367\n",
      "31378\n",
      "31391\n",
      "31392\n",
      "31428\n",
      "31512\n",
      "31529\n",
      "31553\n",
      "31694\n",
      "31731\n",
      "31758\n",
      "31867\n",
      "31869\n",
      "31927\n",
      "32120\n",
      "32228\n",
      "32259\n",
      "32278\n",
      "32283\n",
      "32356\n",
      "32411\n",
      "32414\n",
      "32455\n",
      "32457\n",
      "32597\n",
      "32598\n",
      "32601\n",
      "32700\n",
      "32789\n",
      "32808\n",
      "32910\n",
      "33002\n",
      "33065\n",
      "33067\n",
      "33069\n",
      "33081\n",
      "33197\n",
      "33212\n",
      "33343\n",
      "33373\n",
      "33393\n",
      "33399\n",
      "33413\n",
      "33498\n",
      "33509\n",
      "33555\n",
      "33557\n",
      "33621\n",
      "33625\n",
      "33627\n",
      "33629\n",
      "33652\n",
      "33698\n",
      "33748\n",
      "33789\n",
      "33794\n",
      "33988\n",
      "34035\n",
      "34037\n",
      "34042\n",
      "34044\n",
      "34053\n",
      "34104\n",
      "34135\n",
      "34138\n",
      "34157\n",
      "34271\n",
      "34290\n",
      "34344\n",
      "34354\n",
      "34358\n",
      "34388\n",
      "34402\n",
      "34497\n",
      "34498\n",
      "34502\n",
      "34507\n",
      "34508\n",
      "34544\n",
      "34584\n",
      "34658\n",
      "34757\n",
      "34845\n",
      "34863\n",
      "34942\n",
      "35007\n",
      "35053\n",
      "35086\n",
      "35147\n",
      "35217\n",
      "35306\n",
      "35307\n",
      "35319\n",
      "35342\n",
      "35344\n",
      "35362\n",
      "35396\n",
      "35398\n",
      "35536\n",
      "35544\n",
      "35643\n",
      "35691\n",
      "35703\n",
      "35707\n",
      "35728\n",
      "35746\n",
      "35749\n",
      "35767\n",
      "35898\n",
      "35977\n",
      "36041\n",
      "36042\n",
      "36043\n",
      "36059\n",
      "36104\n",
      "36117\n",
      "36120\n",
      "36132\n",
      "36179\n",
      "36184\n",
      "36238\n",
      "36246\n",
      "36289\n",
      "36300\n",
      "36431\n",
      "36435\n",
      "36466\n",
      "36669\n",
      "36803\n",
      "36897\n",
      "36903\n",
      "36904\n",
      "36925\n",
      "36926\n",
      "36927\n",
      "36929\n",
      "36968\n",
      "37028\n",
      "37099\n",
      "37103\n",
      "37139\n",
      "37164\n",
      "37184\n",
      "37303\n",
      "37311\n",
      "37313\n",
      "37314\n",
      "37321\n",
      "37328\n",
      "37334\n",
      "37387\n",
      "37429\n",
      "37436\n",
      "37444\n",
      "37597\n",
      "37598\n",
      "37614\n",
      "37641\n",
      "37644\n",
      "37653\n",
      "37842\n",
      "37858\n",
      "37929\n",
      "37982\n",
      "38016\n",
      "38034\n",
      "38137\n",
      "38268\n",
      "38300\n",
      "38309\n",
      "38326\n",
      "38579\n",
      "38597\n",
      "38601\n",
      "38624\n",
      "38714\n",
      "38719\n",
      "38754\n",
      "38906\n",
      "38913\n",
      "39013\n",
      "39053\n",
      "39058\n",
      "39148\n",
      "39153\n",
      "39216\n",
      "39259\n",
      "39272\n",
      "39374\n",
      "39391\n",
      "39408\n",
      "39498\n",
      "39719\n",
      "39746\n",
      "39757\n",
      "39758\n",
      "39762\n",
      "39773\n",
      "39779\n",
      "39881\n",
      "39893\n",
      "39932\n",
      "39948\n",
      "39963\n",
      "39968\n",
      "39980\n",
      "40007\n",
      "40014\n",
      "40076\n",
      "40085\n",
      "40183\n",
      "40198\n",
      "40213\n",
      "40308\n",
      "40345\n",
      "40349\n",
      "40407\n",
      "40484\n",
      "40707\n",
      "40716\n",
      "40717\n",
      "40797\n",
      "40802\n",
      "40818\n",
      "40824\n",
      "41127\n",
      "41128\n",
      "41160\n",
      "41164\n",
      "41276\n",
      "41327\n",
      "41328\n",
      "41373\n",
      "41507\n",
      "41534\n",
      "41537\n",
      "41584\n",
      "41619\n",
      "41657\n",
      "41701\n",
      "41777\n",
      "41789\n",
      "41858\n",
      "41861\n",
      "41871\n",
      "41893\n",
      "41894\n",
      "41958\n",
      "41968\n",
      "41988\n",
      "42018\n",
      "42049\n",
      "42079\n",
      "42161\n",
      "42249\n",
      "42250\n",
      "42387\n",
      "42388\n",
      "42393\n",
      "42410\n",
      "42512\n",
      "42523\n",
      "42640\n",
      "42651\n",
      "42652\n",
      "42671\n",
      "42680\n",
      "42698\n",
      "42703\n",
      "42748\n",
      "42796\n",
      "42824\n",
      "42870\n",
      "42979\n",
      "43087\n",
      "43089\n",
      "43103\n",
      "43104\n",
      "43144\n",
      "43175\n",
      "43179\n",
      "43184\n",
      "43232\n",
      "43260\n",
      "43297\n",
      "43304\n",
      "43318\n",
      "43319\n",
      "43495\n",
      "43501\n",
      "43509\n",
      "43561\n",
      "43563\n",
      "43579\n",
      "43618\n",
      "43657\n",
      "43717\n",
      "43718\n",
      "43734\n",
      "43774\n",
      "43825\n",
      "43904\n",
      "43944\n",
      "44004\n",
      "44028\n",
      "44044\n",
      "44057\n",
      "44081\n",
      "44100\n",
      "44125\n",
      "44129\n",
      "44351\n",
      "44462\n",
      "44485\n",
      "44532\n",
      "44539\n",
      "44556\n",
      "44604\n",
      "44719\n",
      "44742\n",
      "44767\n",
      "44783\n",
      "44800\n",
      "44820\n",
      "44823\n",
      "44836\n",
      "44837\n",
      "44898\n",
      "44901\n",
      "44903\n",
      "44926\n",
      "44945\n",
      "44953\n",
      "44985\n",
      "44988\n",
      "44989\n",
      "45010\n",
      "45076\n",
      "45167\n",
      "45169\n",
      "45181\n",
      "45234\n",
      "45272\n",
      "45306\n",
      "45321\n",
      "45326\n",
      "45328\n",
      "45347\n",
      "45391\n",
      "45454\n",
      "45471\n",
      "45519\n",
      "45547\n",
      "45573\n",
      "45639\n",
      "45682\n",
      "45698\n",
      "45735\n",
      "45801\n",
      "45803\n",
      "45834\n",
      "45843\n",
      "45871\n",
      "46005\n",
      "46020\n",
      "46028\n",
      "46038\n",
      "46077\n",
      "46163\n",
      "46179\n",
      "46208\n",
      "46253\n",
      "46291\n",
      "46391\n",
      "46421\n",
      "46458\n",
      "46512\n",
      "46617\n",
      "46635\n",
      "46723\n",
      "46843\n",
      "46849\n",
      "46947\n",
      "46990\n",
      "47010\n",
      "47041\n",
      "47042\n",
      "47043\n",
      "47044\n",
      "47143\n",
      "47231\n",
      "47252\n",
      "47254\n",
      "47308\n",
      "47384\n",
      "47395\n",
      "47405\n",
      "47408\n",
      "47409\n",
      "47414\n",
      "47416\n",
      "47480\n",
      "47489\n",
      "47599\n",
      "47620\n",
      "47644\n",
      "47657\n",
      "47679\n",
      "47685\n",
      "47743\n",
      "47746\n",
      "47747\n",
      "47748\n",
      "47749\n",
      "47750\n",
      "47752\n",
      "47789\n",
      "47807\n",
      "47822\n",
      "47876\n",
      "47877\n",
      "47924\n",
      "47973\n",
      "47981\n",
      "47985\n",
      "47988\n",
      "47989\n",
      "47990\n",
      "47997\n",
      "48039\n",
      "48072\n",
      "48117\n",
      "48118\n",
      "48140\n",
      "48187\n",
      "48210\n",
      "48238\n",
      "48279\n",
      "48293\n",
      "48309\n",
      "48334\n",
      "48390\n",
      "48402\n",
      "48424\n",
      "48443\n",
      "48512\n",
      "48513\n",
      "48514\n",
      "48525\n",
      "48540\n",
      "48555\n",
      "48559\n",
      "48564\n",
      "48627\n",
      "48638\n",
      "48766\n",
      "48768\n",
      "48778\n",
      "48786\n",
      "48795\n",
      "48847\n",
      "48848\n",
      "48860\n",
      "48881\n",
      "48883\n",
      "48923\n",
      "48937\n",
      "48957\n",
      "48959\n",
      "48971\n",
      "48974\n",
      "48991\n",
      "48992\n",
      "49017\n",
      "49165\n",
      "49167\n",
      "49218\n",
      "49246\n",
      "49253\n",
      "49284\n",
      "49308\n",
      "49332\n",
      "49334\n",
      "49340\n",
      "49378\n",
      "49396\n",
      "49428\n",
      "49444\n",
      "49488\n",
      "49501\n",
      "49518\n",
      "49521\n",
      "49572\n",
      "49604\n",
      "49612\n",
      "49621\n",
      "49667\n",
      "49669\n",
      "49806\n",
      "49903\n",
      "49936\n",
      "49959\n",
      "49968\n",
      "49989\n"
     ]
    }
   ],
   "source": [
    "dic = dict()\n",
    "if n_clusters > 1:\n",
    "    dic = torch.load('dic.torch')\n",
    "# print(\"  idx|\", \"bad_soft|\", \"bad_knn|\", \"real_cl|\", \"pred_knn|\", \"pred_soft|\", \"max_max_val|\", \"sim_real_cl|\", \"sim_best_knn|\", \"sim_max_max|\")\n",
    "\n",
    "for idx, t in (enumerate(testx)):\n",
    "    z = None # sofmax_values[idx].numpy().item()\n",
    "    p = [idx // 5, int(softmax_classes[idx])]\n",
    "    index = np.where(np.array(pairs)[:,0] == idx)[0]\n",
    "    if len(index) > 0:\n",
    "        p = pairs[index.item()]\n",
    "        z = utility_functions.cos_sim(torch.Tensor(np.array([testx[p[0]]])), torch.Tensor(np.array([traincenterx[p[1]]])))[0].numpy()[0]\n",
    "    x = utility_functions.cos_sim(torch.Tensor(np.array([t])), torch.Tensor(np.array([traincenterx[idx // 5]])))[0]\n",
    "    y = utility_functions.cos_sim(torch.Tensor(np.array([t])), torch.Tensor(np.array(traincenterx)))[0].numpy()\n",
    "    if not z:\n",
    "        z = np.max(y)\n",
    "    type1 = (idx // 5) != np.argmax(y)\n",
    "    type2 = len(index) > 0 # or (p[1] != (idx // 5))\n",
    "    # if type2:\n",
    "    #     print(len(index))\n",
    "    #     print(p[1])\n",
    "    #     print(idx // 5)\n",
    "    if type1 or type2:\n",
    "        ddd = {'bad_knn':type1, 'real_class':idx//5, 'pred_knn': np.argmax(y), 'sim_real_cl':x.numpy()[0], 'sim_best_knn': np.max(y)}\n",
    "        if method == 'kmeans' and n_clusters == 1:\n",
    "            ddd['pred_soft'] = p[1]\n",
    "            ddd['bad_soft'] = type2\n",
    "            ddd['soft_val'] = sofmax_values[idx].item()\n",
    "            ddd['sim_soft'] = z\n",
    "        if method == 'full_random' and n_clusters > 1:\n",
    "            print(idx)\n",
    "            ddd['pred_max_max'] = p[1]\n",
    "            ddd['bad_max_max'] = type2\n",
    "            ddd['max_max_val'] = sofmax_values[idx].item()\n",
    "            ddd['sim_max_max'] = z\n",
    "\n",
    "        if idx in dic:\n",
    "            dic[idx].update(ddd)\n",
    "        else:\n",
    "            dic[idx] = ddd\n",
    "        # print('{:>5}|'.format(idx), '{:>8}|'.format(str(type2)), '{:>7}|'.format(str(type1)), '{:>7}|'.format(idx//5), \\\n",
    "        #     '{:>8}|'.format(np.argmax(y)), '{:>9}|'.format(p[1]), '{:11.6f}|'.format(sofmax_values[idx].item()), '{:11.6f}|'.format(x.numpy()[0]), \\\n",
    "        #     '{:12.6f}|'.format(np.max(y)), '{:>11}|'.format(str(z)))\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{46: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 9,\n",
       "  'pred_knn': 9,\n",
       "  'sim_real_cl': 0.3787729,\n",
       "  'sim_best_knn': 0.3787729,\n",
       "  'pred_soft': 5742,\n",
       "  'soft_val': 0.27831315994262695,\n",
       "  'sim_soft': 0.25589907},\n",
       " 47: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 9,\n",
       "  'pred_knn': 1422,\n",
       "  'sim_real_cl': 0.33357036,\n",
       "  'sim_best_knn': 0.42919788,\n",
       "  'pred_soft': 1422,\n",
       "  'soft_val': 0.332141250371933,\n",
       "  'sim_soft': 0.42919788,\n",
       "  'pred_max_max': 1422,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.34811079502105713,\n",
       "  'sim_max_max': 0.42919788},\n",
       " 48: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 9,\n",
       "  'pred_knn': 9,\n",
       "  'sim_real_cl': 0.34057662,\n",
       "  'sim_best_knn': 0.34057662,\n",
       "  'pred_soft': 5934,\n",
       "  'soft_val': 0.2668454647064209,\n",
       "  'sim_soft': 0.27725807,\n",
       "  'pred_max_max': 1014,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28383156657218933,\n",
       "  'sim_max_max': 0.33405167},\n",
       " 77: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 15,\n",
       "  'pred_knn': 15,\n",
       "  'sim_real_cl': 0.40127861,\n",
       "  'sim_best_knn': 0.40127861,\n",
       "  'pred_soft': 3754,\n",
       "  'soft_val': 0.315265417098999,\n",
       "  'sim_soft': 0.2993618},\n",
       " 88: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 17,\n",
       "  'pred_knn': 4226,\n",
       "  'sim_real_cl': 0.26933986,\n",
       "  'sim_best_knn': 0.2897963,\n",
       "  'pred_soft': 5811,\n",
       "  'soft_val': 0.23430435359477997,\n",
       "  'sim_soft': 0.2473363,\n",
       "  'pred_max_max': 17,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.26537662744522095,\n",
       "  'sim_max_max': 0.2897963},\n",
       " 96: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 19,\n",
       "  'pred_knn': 19,\n",
       "  'sim_real_cl': 0.3935858,\n",
       "  'sim_best_knn': 0.3935858,\n",
       "  'pred_soft': 8757,\n",
       "  'soft_val': 0.24133679270744324,\n",
       "  'sim_soft': 0.2677138,\n",
       "  'pred_max_max': 7303,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30067959427833557,\n",
       "  'sim_max_max': 0.31270564},\n",
       " 98: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 19,\n",
       "  'pred_knn': 3576,\n",
       "  'sim_real_cl': 0.18410777,\n",
       "  'sim_best_knn': 0.3012818,\n",
       "  'pred_soft': 3576,\n",
       "  'soft_val': 0.22918349504470825,\n",
       "  'sim_soft': 0.3012818,\n",
       "  'pred_max_max': 3576,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.23530806601047516,\n",
       "  'sim_max_max': 0.3012818},\n",
       " 116: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 23,\n",
       "  'pred_knn': 2471,\n",
       "  'sim_real_cl': -0.02725693,\n",
       "  'sim_best_knn': 0.28370714,\n",
       "  'pred_soft': 2471,\n",
       "  'soft_val': 0.22246502339839935,\n",
       "  'sim_soft': 0.28370714,\n",
       "  'pred_max_max': 8988,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.23516100645065308,\n",
       "  'sim_max_max': 0.23942345},\n",
       " 220: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 44,\n",
       "  'pred_knn': 7272,\n",
       "  'sim_real_cl': 0.26364726,\n",
       "  'sim_best_knn': 0.2730968,\n",
       "  'pred_soft': 44,\n",
       "  'soft_val': 0.2767032980918884,\n",
       "  'sim_soft': array(0.2767033, dtype=float32),\n",
       "  'pred_max_max': 44,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.29168856143951416,\n",
       "  'sim_max_max': 0.2730968},\n",
       " 228: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 45,\n",
       "  'pred_knn': 1981,\n",
       "  'sim_real_cl': 0.32124493,\n",
       "  'sim_best_knn': 0.3799205,\n",
       "  'pred_soft': 1981,\n",
       "  'soft_val': 0.2668970823287964,\n",
       "  'sim_soft': 0.3799205,\n",
       "  'pred_max_max': 3526,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28002065420150757,\n",
       "  'sim_max_max': 0.34626275},\n",
       " 229: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 45,\n",
       "  'pred_knn': 4368,\n",
       "  'sim_real_cl': 0.2091138,\n",
       "  'sim_best_knn': 0.35196215,\n",
       "  'pred_soft': 5708,\n",
       "  'soft_val': 0.31250321865081787,\n",
       "  'sim_soft': 0.3207671,\n",
       "  'pred_max_max': 5708,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.33652037382125854,\n",
       "  'sim_max_max': 0.3207671},\n",
       " 235: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 47,\n",
       "  'pred_knn': 47,\n",
       "  'sim_real_cl': 0.42649508,\n",
       "  'sim_best_knn': 0.42649508,\n",
       "  'pred_soft': 7155,\n",
       "  'soft_val': 0.25223731994628906,\n",
       "  'sim_soft': 0.23442489},\n",
       " 237: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 47,\n",
       "  'pred_knn': 47,\n",
       "  'sim_real_cl': 0.52084446,\n",
       "  'sim_best_knn': 0.52084446,\n",
       "  'pred_soft': 5994,\n",
       "  'soft_val': 0.2938522398471832,\n",
       "  'sim_soft': 0.35655278},\n",
       " 246: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 49,\n",
       "  'pred_knn': 49,\n",
       "  'sim_real_cl': 0.43190652,\n",
       "  'sim_best_knn': 0.43190652,\n",
       "  'pred_soft': 8887,\n",
       "  'soft_val': 0.31701165437698364,\n",
       "  'sim_soft': 0.38433865},\n",
       " 256: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 51,\n",
       "  'pred_knn': 51,\n",
       "  'sim_real_cl': 0.38331258,\n",
       "  'sim_best_knn': 0.38331258,\n",
       "  'pred_soft': 980,\n",
       "  'soft_val': 0.2553049325942993,\n",
       "  'sim_soft': 0.2474509,\n",
       "  'pred_max_max': 4882,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2547963261604309,\n",
       "  'sim_max_max': 0.24300031},\n",
       " 317: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 63,\n",
       "  'pred_knn': 63,\n",
       "  'sim_real_cl': 0.37018222,\n",
       "  'sim_best_knn': 0.37018222,\n",
       "  'pred_soft': 8921,\n",
       "  'soft_val': 0.26961711049079895,\n",
       "  'sim_soft': 0.3034425},\n",
       " 349: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 69,\n",
       "  'pred_knn': 5790,\n",
       "  'sim_real_cl': 0.18947527,\n",
       "  'sim_best_knn': 0.28940135,\n",
       "  'pred_soft': 5790,\n",
       "  'soft_val': 0.27480077743530273,\n",
       "  'sim_soft': 0.28940135,\n",
       "  'pred_max_max': 5790,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3088163137435913,\n",
       "  'sim_max_max': 0.28940135},\n",
       " 411: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 82,\n",
       "  'pred_knn': 2691,\n",
       "  'sim_real_cl': 0.3203189,\n",
       "  'sim_best_knn': 0.32563576,\n",
       "  'pred_soft': 2916,\n",
       "  'soft_val': 0.30548468232154846,\n",
       "  'sim_soft': 0.2880761,\n",
       "  'pred_max_max': 2916,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2881833016872406,\n",
       "  'sim_max_max': 0.2880761},\n",
       " 434: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 86,\n",
       "  'pred_knn': 5632,\n",
       "  'sim_real_cl': 0.37928212,\n",
       "  'sim_best_knn': 0.3890313,\n",
       "  'pred_soft': 86,\n",
       "  'soft_val': 0.2933461666107178,\n",
       "  'sim_soft': array(0.29334617, dtype=float32),\n",
       "  'pred_max_max': 86,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.31359684467315674,\n",
       "  'sim_max_max': 0.3890313},\n",
       " 444: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 88,\n",
       "  'pred_knn': 5705,\n",
       "  'sim_real_cl': 0.25097913,\n",
       "  'sim_best_knn': 0.27012256,\n",
       "  'pred_soft': 5705,\n",
       "  'soft_val': 0.2758784592151642,\n",
       "  'sim_soft': 0.27012256,\n",
       "  'pred_max_max': 9935,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27317875623703003,\n",
       "  'sim_max_max': 0.2695074},\n",
       " 500: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 100,\n",
       "  'pred_knn': 9252,\n",
       "  'sim_real_cl': 0.36523888,\n",
       "  'sim_best_knn': 0.38114762,\n",
       "  'pred_soft': 100,\n",
       "  'soft_val': 0.3640541136264801,\n",
       "  'sim_soft': array(0.3640541, dtype=float32),\n",
       "  'pred_max_max': 6107,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30596432089805603,\n",
       "  'sim_max_max': 0.32845914},\n",
       " 563: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 112,\n",
       "  'pred_knn': 8561,\n",
       "  'sim_real_cl': 0.35098022,\n",
       "  'sim_best_knn': 0.38469598,\n",
       "  'pred_soft': 7426,\n",
       "  'soft_val': 0.2585009038448334,\n",
       "  'sim_soft': 0.21405734,\n",
       "  'pred_max_max': 8561,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24497924745082855,\n",
       "  'sim_max_max': 0.38469598},\n",
       " 642: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 128,\n",
       "  'pred_knn': 3024,\n",
       "  'sim_real_cl': 0.20751303,\n",
       "  'sim_best_knn': 0.4198647,\n",
       "  'pred_soft': 890,\n",
       "  'soft_val': 0.2628743648529053,\n",
       "  'sim_soft': 0.27744195,\n",
       "  'pred_max_max': 890,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2687322795391083,\n",
       "  'sim_max_max': 0.27744195},\n",
       " 648: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 129,\n",
       "  'pred_knn': 2725,\n",
       "  'sim_real_cl': 0.14229026,\n",
       "  'sim_best_knn': 0.29701722,\n",
       "  'pred_soft': 7895,\n",
       "  'soft_val': 0.21683335304260254,\n",
       "  'sim_soft': 0.2204422,\n",
       "  'pred_max_max': 1371,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2496815323829651,\n",
       "  'sim_max_max': 0.29226097},\n",
       " 691: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 138,\n",
       "  'pred_knn': 4671,\n",
       "  'sim_real_cl': 0.29570252,\n",
       "  'sim_best_knn': 0.30783913,\n",
       "  'pred_soft': 8910,\n",
       "  'soft_val': 0.23778976500034332,\n",
       "  'sim_soft': 0.2224376,\n",
       "  'pred_max_max': 138,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.27611789107322693,\n",
       "  'sim_max_max': 0.30783913},\n",
       " 715: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 143,\n",
       "  'pred_knn': 7630,\n",
       "  'sim_real_cl': 0.24355383,\n",
       "  'sim_best_knn': 0.3177352,\n",
       "  'pred_soft': 6603,\n",
       "  'soft_val': 0.2731461226940155,\n",
       "  'sim_soft': 0.29226983,\n",
       "  'pred_max_max': 3389,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2572678327560425,\n",
       "  'sim_max_max': 0.26767415},\n",
       " 728: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 145,\n",
       "  'pred_knn': 1184,\n",
       "  'sim_real_cl': 0.25747424,\n",
       "  'sim_best_knn': 0.32597142,\n",
       "  'pred_soft': 2164,\n",
       "  'soft_val': 0.26893532276153564,\n",
       "  'sim_soft': 0.28474015,\n",
       "  'pred_max_max': 1184,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.330926775932312,\n",
       "  'sim_max_max': 0.32597142},\n",
       " 740: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 148,\n",
       "  'pred_knn': 4400,\n",
       "  'sim_real_cl': 0.2977659,\n",
       "  'sim_best_knn': 0.31122804,\n",
       "  'pred_soft': 148,\n",
       "  'soft_val': 0.40389150381088257,\n",
       "  'sim_soft': array(0.4038915, dtype=float32),\n",
       "  'pred_max_max': 148,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3777574598789215,\n",
       "  'sim_max_max': 0.31122804},\n",
       " 745: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 149,\n",
       "  'pred_knn': 3685,\n",
       "  'sim_real_cl': 0.31844375,\n",
       "  'sim_best_knn': 0.33221495,\n",
       "  'pred_soft': 1471,\n",
       "  'soft_val': 0.2735306918621063,\n",
       "  'sim_soft': 0.26955202,\n",
       "  'pred_max_max': 149,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3271561861038208,\n",
       "  'sim_max_max': 0.33221495},\n",
       " 768: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 153,\n",
       "  'pred_knn': 153,\n",
       "  'sim_real_cl': 0.45606214,\n",
       "  'sim_best_knn': 0.45606214,\n",
       "  'pred_soft': 8599,\n",
       "  'soft_val': 0.3239707052707672,\n",
       "  'sim_soft': 0.38904876,\n",
       "  'pred_max_max': 8599,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3269253075122833,\n",
       "  'sim_max_max': 0.38904876},\n",
       " 771: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 154,\n",
       "  'pred_knn': 154,\n",
       "  'sim_real_cl': 0.4928539,\n",
       "  'sim_best_knn': 0.4928539,\n",
       "  'pred_soft': 5583,\n",
       "  'soft_val': 0.2845207452774048,\n",
       "  'sim_soft': 0.3028926},\n",
       " 829: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 165,\n",
       "  'pred_knn': 5359,\n",
       "  'sim_real_cl': 0.213122,\n",
       "  'sim_best_knn': 0.2986697,\n",
       "  'pred_soft': 5359,\n",
       "  'soft_val': 0.26146838068962097,\n",
       "  'sim_soft': 0.2986697,\n",
       "  'pred_max_max': 9225,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28235819935798645,\n",
       "  'sim_max_max': 0.2797945},\n",
       " 864: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 172,\n",
       "  'pred_knn': 172,\n",
       "  'sim_real_cl': 0.36719498,\n",
       "  'sim_best_knn': 0.36719498,\n",
       "  'pred_soft': 3583,\n",
       "  'soft_val': 0.33476102352142334,\n",
       "  'sim_soft': 0.35084665},\n",
       " 892: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 178,\n",
       "  'pred_knn': 178,\n",
       "  'sim_real_cl': 0.40999764,\n",
       "  'sim_best_knn': 0.40999764,\n",
       "  'pred_soft': 2715,\n",
       "  'soft_val': 0.3089408278465271,\n",
       "  'sim_soft': 0.39708388},\n",
       " 899: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 179,\n",
       "  'pred_knn': 754,\n",
       "  'sim_real_cl': 0.3509307,\n",
       "  'sim_best_knn': 0.36337793,\n",
       "  'pred_soft': 179,\n",
       "  'soft_val': 0.3400764763355255,\n",
       "  'sim_soft': array(0.34007648, dtype=float32),\n",
       "  'pred_max_max': 179,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3466193675994873,\n",
       "  'sim_max_max': 0.36337793},\n",
       " 934: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 186,\n",
       "  'pred_knn': 186,\n",
       "  'sim_real_cl': 0.37673873,\n",
       "  'sim_best_knn': 0.37673873,\n",
       "  'pred_soft': 6344,\n",
       "  'soft_val': 0.22545242309570312,\n",
       "  'sim_soft': 0.27839756,\n",
       "  'pred_max_max': 7949,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2716144025325775,\n",
       "  'sim_max_max': 0.2460793},\n",
       " 957: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 191,\n",
       "  'pred_knn': 191,\n",
       "  'sim_real_cl': 0.42558947,\n",
       "  'sim_best_knn': 0.42558947,\n",
       "  'pred_soft': 6938,\n",
       "  'soft_val': 0.27689164876937866,\n",
       "  'sim_soft': 0.32258517},\n",
       " 1029: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 205,\n",
       "  'pred_knn': 9739,\n",
       "  'sim_real_cl': 0.34885246,\n",
       "  'sim_best_knn': 0.34958708,\n",
       "  'pred_soft': 4814,\n",
       "  'soft_val': 0.2866087555885315,\n",
       "  'sim_soft': 0.30327904,\n",
       "  'pred_max_max': 205,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.32490962743759155,\n",
       "  'sim_max_max': 0.34958708},\n",
       " 1138: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 227,\n",
       "  'pred_knn': 227,\n",
       "  'sim_real_cl': 0.3792551,\n",
       "  'sim_best_knn': 0.3792551,\n",
       "  'pred_soft': 8225,\n",
       "  'soft_val': 0.31908389925956726,\n",
       "  'sim_soft': 0.29951888},\n",
       " 1149: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 229,\n",
       "  'pred_knn': 1172,\n",
       "  'sim_real_cl': 0.43224815,\n",
       "  'sim_best_knn': 0.44238305,\n",
       "  'pred_soft': 229,\n",
       "  'soft_val': 0.4209824800491333,\n",
       "  'sim_soft': array(0.42098248, dtype=float32),\n",
       "  'pred_max_max': 229,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.43487149477005005,\n",
       "  'sim_max_max': 0.44238305},\n",
       " 1154: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 230,\n",
       "  'pred_knn': 230,\n",
       "  'sim_real_cl': 0.42149827,\n",
       "  'sim_best_knn': 0.42149827,\n",
       "  'pred_soft': 1927,\n",
       "  'soft_val': 0.26989424228668213,\n",
       "  'sim_soft': 0.2728123},\n",
       " 1160: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 232,\n",
       "  'pred_knn': 4980,\n",
       "  'sim_real_cl': 0.342443,\n",
       "  'sim_best_knn': 0.37488732,\n",
       "  'pred_soft': 2563,\n",
       "  'soft_val': 0.2718641459941864,\n",
       "  'sim_soft': 0.26068908,\n",
       "  'pred_max_max': 232,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.29307225346565247,\n",
       "  'sim_max_max': 0.37488732},\n",
       " 1169: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 233,\n",
       "  'pred_knn': 2924,\n",
       "  'sim_real_cl': 0.30489296,\n",
       "  'sim_best_knn': 0.327809,\n",
       "  'pred_soft': 233,\n",
       "  'soft_val': 0.3268992602825165,\n",
       "  'sim_soft': array(0.32689926, dtype=float32),\n",
       "  'pred_max_max': 233,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3501582145690918,\n",
       "  'sim_max_max': 0.327809},\n",
       " 1183: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 236,\n",
       "  'pred_knn': 236,\n",
       "  'sim_real_cl': 0.31195438,\n",
       "  'sim_best_knn': 0.31195438,\n",
       "  'pred_soft': 5615,\n",
       "  'soft_val': 0.2380364090204239,\n",
       "  'sim_soft': 0.21647805},\n",
       " 1190: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 238,\n",
       "  'pred_knn': 238,\n",
       "  'sim_real_cl': 0.32134223,\n",
       "  'sim_best_knn': 0.32134223,\n",
       "  'pred_soft': 4728,\n",
       "  'soft_val': 0.2552562952041626,\n",
       "  'sim_soft': 0.29062805,\n",
       "  'pred_max_max': 4728,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27231913805007935,\n",
       "  'sim_max_max': 0.29062805},\n",
       " 1218: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 243,\n",
       "  'pred_knn': 5159,\n",
       "  'sim_real_cl': 0.2336345,\n",
       "  'sim_best_knn': 0.43066204,\n",
       "  'pred_soft': 2597,\n",
       "  'soft_val': 0.352816104888916,\n",
       "  'sim_soft': 0.41527522,\n",
       "  'pred_max_max': 5159,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3502696454524994,\n",
       "  'sim_max_max': 0.43066204},\n",
       " 1273: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 254,\n",
       "  'pred_knn': 8595,\n",
       "  'sim_real_cl': 0.30275887,\n",
       "  'sim_best_knn': 0.32640645,\n",
       "  'pred_soft': 4739,\n",
       "  'soft_val': 0.25702232122421265,\n",
       "  'sim_soft': 0.23324984,\n",
       "  'pred_max_max': 7420,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3181798458099365,\n",
       "  'sim_max_max': 0.30620003},\n",
       " 1298: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 259,\n",
       "  'pred_knn': 1534,\n",
       "  'sim_real_cl': 0.21646827,\n",
       "  'sim_best_knn': 0.32864124,\n",
       "  'pred_soft': 8484,\n",
       "  'soft_val': 0.2812115550041199,\n",
       "  'sim_soft': 0.1820311,\n",
       "  'pred_max_max': 1534,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28174516558647156,\n",
       "  'sim_max_max': 0.32864124},\n",
       " 1299: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 259,\n",
       "  'pred_knn': 2593,\n",
       "  'sim_real_cl': 0.26664498,\n",
       "  'sim_best_knn': 0.31735444,\n",
       "  'pred_soft': 2197,\n",
       "  'soft_val': 0.2669782042503357,\n",
       "  'sim_soft': 0.2786783,\n",
       "  'pred_max_max': 2197,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27879074215888977,\n",
       "  'sim_max_max': 0.2786783},\n",
       " 1359: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 271,\n",
       "  'pred_knn': 271,\n",
       "  'sim_real_cl': 0.34172577,\n",
       "  'sim_best_knn': 0.34172577,\n",
       "  'pred_soft': 4728,\n",
       "  'soft_val': 0.27246129512786865,\n",
       "  'sim_soft': 0.3100204},\n",
       " 1361: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 272,\n",
       "  'pred_knn': 2543,\n",
       "  'sim_real_cl': 0.26329032,\n",
       "  'sim_best_knn': 0.31939608,\n",
       "  'pred_soft': 2543,\n",
       "  'soft_val': 0.2919471859931946,\n",
       "  'sim_soft': 0.31939608,\n",
       "  'pred_max_max': 2543,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26890063285827637,\n",
       "  'sim_max_max': 0.31939608},\n",
       " 1391: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 278,\n",
       "  'pred_knn': 2531,\n",
       "  'sim_real_cl': 0.19079506,\n",
       "  'sim_best_knn': 0.41817093,\n",
       "  'pred_soft': 2531,\n",
       "  'soft_val': 0.36429327726364136,\n",
       "  'sim_soft': 0.41817093,\n",
       "  'pred_max_max': 2531,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.36510390043258667,\n",
       "  'sim_max_max': 0.41817093},\n",
       " 1486: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 297,\n",
       "  'pred_knn': 1310,\n",
       "  'sim_real_cl': 0.38968784,\n",
       "  'sim_best_knn': 0.39443552,\n",
       "  'pred_soft': 2878,\n",
       "  'soft_val': 0.2614280581474304,\n",
       "  'sim_soft': 0.2464604,\n",
       "  'pred_max_max': 297,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3369140326976776,\n",
       "  'sim_max_max': 0.39443552},\n",
       " 1489: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 297,\n",
       "  'pred_knn': 297,\n",
       "  'sim_real_cl': 0.46470517,\n",
       "  'sim_best_knn': 0.46470517,\n",
       "  'pred_soft': 9707,\n",
       "  'soft_val': 0.3206617534160614,\n",
       "  'sim_soft': 0.30393612},\n",
       " 1578: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 315,\n",
       "  'pred_knn': 4060,\n",
       "  'sim_real_cl': 0.28184986,\n",
       "  'sim_best_knn': 0.31228778,\n",
       "  'pred_soft': 4060,\n",
       "  'soft_val': 0.24606864154338837,\n",
       "  'sim_soft': 0.31228778,\n",
       "  'pred_max_max': 4060,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28595516085624695,\n",
       "  'sim_max_max': 0.31228778},\n",
       " 1621: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 324,\n",
       "  'pred_knn': 324,\n",
       "  'sim_real_cl': 0.47577477,\n",
       "  'sim_best_knn': 0.47577477,\n",
       "  'pred_soft': 4082,\n",
       "  'soft_val': 0.25370240211486816,\n",
       "  'sim_soft': 0.3215031},\n",
       " 1676: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 335,\n",
       "  'pred_knn': 335,\n",
       "  'sim_real_cl': 0.3236446,\n",
       "  'sim_best_knn': 0.3236446,\n",
       "  'pred_soft': 8629,\n",
       "  'soft_val': 0.2540692985057831,\n",
       "  'sim_soft': 0.27788126},\n",
       " 1712: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 342,\n",
       "  'pred_knn': 9796,\n",
       "  'sim_real_cl': 0.17012051,\n",
       "  'sim_best_knn': 0.2754858,\n",
       "  'pred_soft': 9796,\n",
       "  'soft_val': 0.23393894731998444,\n",
       "  'sim_soft': 0.2754858,\n",
       "  'pred_max_max': 1581,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.23788659274578094,\n",
       "  'sim_max_max': 0.25360644},\n",
       " 1779: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 355,\n",
       "  'pred_knn': 355,\n",
       "  'sim_real_cl': 0.4184053,\n",
       "  'sim_best_knn': 0.4184053,\n",
       "  'pred_soft': 4908,\n",
       "  'soft_val': 0.3095668852329254,\n",
       "  'sim_soft': 0.34359264},\n",
       " 1800: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 360,\n",
       "  'pred_knn': 7455,\n",
       "  'sim_real_cl': 0.29155838,\n",
       "  'sim_best_knn': 0.39031088,\n",
       "  'pred_soft': 5820,\n",
       "  'soft_val': 0.32269543409347534,\n",
       "  'sim_soft': 0.3366799,\n",
       "  'pred_max_max': 1392,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3572539687156677,\n",
       "  'sim_max_max': 0.38613564},\n",
       " 1853: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 370,\n",
       "  'pred_knn': 370,\n",
       "  'sim_real_cl': 0.37198472,\n",
       "  'sim_best_knn': 0.37198472,\n",
       "  'pred_soft': 9390,\n",
       "  'soft_val': 0.28559020161628723,\n",
       "  'sim_soft': 0.34568763},\n",
       " 1900: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 380,\n",
       "  'pred_knn': 380,\n",
       "  'sim_real_cl': 0.3039737,\n",
       "  'sim_best_knn': 0.3039737,\n",
       "  'pred_soft': 9641,\n",
       "  'soft_val': 0.27308282256126404,\n",
       "  'sim_soft': 0.29485366,\n",
       "  'pred_max_max': 9641,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27306970953941345,\n",
       "  'sim_max_max': 0.29485366},\n",
       " 1901: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 380,\n",
       "  'pred_knn': 2134,\n",
       "  'sim_real_cl': 0.2955222,\n",
       "  'sim_best_knn': 0.31903297,\n",
       "  'pred_soft': 2134,\n",
       "  'soft_val': 0.28988760709762573,\n",
       "  'sim_soft': 0.31903297,\n",
       "  'pred_max_max': 3034,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26639536023139954,\n",
       "  'sim_max_max': 0.282371},\n",
       " 1929: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 385,\n",
       "  'pred_knn': 1250,\n",
       "  'sim_real_cl': 0.32766107,\n",
       "  'sim_best_knn': 0.3396576,\n",
       "  'pred_soft': 6946,\n",
       "  'soft_val': 0.33460208773612976,\n",
       "  'sim_soft': 0.28037155,\n",
       "  'pred_max_max': 6544,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30862367153167725,\n",
       "  'sim_max_max': 0.33644602},\n",
       " 1956: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 391,\n",
       "  'pred_knn': 391,\n",
       "  'sim_real_cl': 0.4565553,\n",
       "  'sim_best_knn': 0.4565553,\n",
       "  'pred_soft': 2222,\n",
       "  'soft_val': 0.3499893844127655,\n",
       "  'sim_soft': 0.41094923,\n",
       "  'pred_max_max': 2222,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3837742805480957,\n",
       "  'sim_max_max': 0.41094923},\n",
       " 1959: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 391,\n",
       "  'pred_knn': 391,\n",
       "  'sim_real_cl': 0.4379038,\n",
       "  'sim_best_knn': 0.4379038,\n",
       "  'pred_soft': 2222,\n",
       "  'soft_val': 0.26361092925071716,\n",
       "  'sim_soft': 0.3023617},\n",
       " 1993: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 398,\n",
       "  'pred_knn': 2154,\n",
       "  'sim_real_cl': 0.23703375,\n",
       "  'sim_best_knn': 0.29853916,\n",
       "  'pred_soft': 3931,\n",
       "  'soft_val': 0.24001462757587433,\n",
       "  'sim_soft': 0.2874872,\n",
       "  'pred_max_max': 2075,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24608540534973145,\n",
       "  'sim_max_max': 0.25673977},\n",
       " 1999: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 399,\n",
       "  'pred_knn': 3805,\n",
       "  'sim_real_cl': 0.2778447,\n",
       "  'sim_best_knn': 0.38006932,\n",
       "  'pred_soft': 3805,\n",
       "  'soft_val': 0.3428209722042084,\n",
       "  'sim_soft': 0.38006932,\n",
       "  'pred_max_max': 3805,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3457079231739044,\n",
       "  'sim_max_max': 0.38006932},\n",
       " 2010: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 402,\n",
       "  'pred_knn': 402,\n",
       "  'sim_real_cl': 0.35937935,\n",
       "  'sim_best_knn': 0.35937935,\n",
       "  'pred_soft': 4998,\n",
       "  'soft_val': 0.3008204996585846,\n",
       "  'sim_soft': 0.32810998,\n",
       "  'pred_max_max': 4998,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30151256918907166,\n",
       "  'sim_max_max': 0.32810998},\n",
       " 2011: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 402,\n",
       "  'pred_knn': 402,\n",
       "  'sim_real_cl': 0.41440386,\n",
       "  'sim_best_knn': 0.41440386,\n",
       "  'pred_soft': 783,\n",
       "  'soft_val': 0.30995699763298035,\n",
       "  'sim_soft': 0.3180944},\n",
       " 2014: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 402,\n",
       "  'pred_knn': 402,\n",
       "  'sim_real_cl': 0.4636144,\n",
       "  'sim_best_knn': 0.4636144,\n",
       "  'pred_soft': 4954,\n",
       "  'soft_val': 0.3191962242126465,\n",
       "  'sim_soft': 0.3324328},\n",
       " 2077: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 415,\n",
       "  'pred_knn': 1820,\n",
       "  'sim_real_cl': 0.2584417,\n",
       "  'sim_best_knn': 0.30890834,\n",
       "  'pred_soft': 1241,\n",
       "  'soft_val': 0.23060263693332672,\n",
       "  'sim_soft': 0.247502,\n",
       "  'pred_max_max': 1820,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2736181318759918,\n",
       "  'sim_max_max': 0.30890834},\n",
       " 2084: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 416,\n",
       "  'pred_knn': 416,\n",
       "  'sim_real_cl': 0.3699586,\n",
       "  'sim_best_knn': 0.3699586,\n",
       "  'pred_soft': 203,\n",
       "  'soft_val': 0.25756707787513733,\n",
       "  'sim_soft': 0.30213663,\n",
       "  'pred_max_max': 1610,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27279338240623474,\n",
       "  'sim_max_max': 0.2746388},\n",
       " 2099: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 419,\n",
       "  'pred_knn': 4537,\n",
       "  'sim_real_cl': 0.1459429,\n",
       "  'sim_best_knn': 0.3187923,\n",
       "  'pred_soft': 2916,\n",
       "  'soft_val': 0.2606159448623657,\n",
       "  'sim_soft': 0.26503813,\n",
       "  'pred_max_max': 2916,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2687302827835083,\n",
       "  'sim_max_max': 0.26503813},\n",
       " 2104: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 420,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.29883504,\n",
       "  'sim_best_knn': 0.33489037,\n",
       "  'pred_soft': 9385,\n",
       "  'soft_val': 0.2854018211364746,\n",
       "  'sim_soft': 0.29027334,\n",
       "  'pred_max_max': 5924,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27520862221717834,\n",
       "  'sim_max_max': 0.3261434},\n",
       " 2117: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 423,\n",
       "  'pred_knn': 6984,\n",
       "  'sim_real_cl': 0.25679862,\n",
       "  'sim_best_knn': 0.32644165,\n",
       "  'pred_soft': 6984,\n",
       "  'soft_val': 0.29150959849357605,\n",
       "  'sim_soft': 0.32644165,\n",
       "  'pred_max_max': 6984,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28709423542022705,\n",
       "  'sim_max_max': 0.32644165},\n",
       " 2125: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 425,\n",
       "  'pred_knn': 3274,\n",
       "  'sim_real_cl': 0.19214186,\n",
       "  'sim_best_knn': 0.2437264,\n",
       "  'pred_soft': 3274,\n",
       "  'soft_val': 0.2616107761859894,\n",
       "  'sim_soft': 0.2437264,\n",
       "  'pred_max_max': 693,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.254904180765152,\n",
       "  'sim_max_max': 0.21490486},\n",
       " 2141: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 428,\n",
       "  'pred_knn': 428,\n",
       "  'sim_real_cl': 0.37062454,\n",
       "  'sim_best_knn': 0.37062454,\n",
       "  'pred_soft': 2793,\n",
       "  'soft_val': 0.2523101270198822,\n",
       "  'sim_soft': 0.2649673,\n",
       "  'pred_max_max': 1098,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2936819791793823,\n",
       "  'sim_max_max': 0.27687544},\n",
       " 2142: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 428,\n",
       "  'pred_knn': 428,\n",
       "  'sim_real_cl': 0.2946927,\n",
       "  'sim_best_knn': 0.2946927,\n",
       "  'pred_soft': 7970,\n",
       "  'soft_val': 0.21290884912014008,\n",
       "  'sim_soft': 0.21412744,\n",
       "  'pred_max_max': 7970,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.22645694017410278,\n",
       "  'sim_max_max': 0.21412744},\n",
       " 2143: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 428,\n",
       "  'pred_knn': 428,\n",
       "  'sim_real_cl': 0.5634748,\n",
       "  'sim_best_knn': 0.5634748,\n",
       "  'pred_soft': 5708,\n",
       "  'soft_val': 0.3285391926765442,\n",
       "  'sim_soft': 0.31371439},\n",
       " 2144: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 428,\n",
       "  'pred_knn': 428,\n",
       "  'sim_real_cl': 0.42650035,\n",
       "  'sim_best_knn': 0.42650035,\n",
       "  'pred_soft': 9789,\n",
       "  'soft_val': 0.25161808729171753,\n",
       "  'sim_soft': 0.2914613},\n",
       " 2153: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 430,\n",
       "  'pred_knn': 4506,\n",
       "  'sim_real_cl': 0.09040867,\n",
       "  'sim_best_knn': 0.21934769,\n",
       "  'pred_soft': 8644,\n",
       "  'soft_val': 0.22335001826286316,\n",
       "  'sim_soft': 0.2012903,\n",
       "  'pred_max_max': 7075,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.22034621238708496,\n",
       "  'sim_max_max': 0.21098179},\n",
       " 2159: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 431,\n",
       "  'pred_knn': 431,\n",
       "  'sim_real_cl': 0.38539058,\n",
       "  'sim_best_knn': 0.38539058,\n",
       "  'pred_soft': 7431,\n",
       "  'soft_val': 0.26547351479530334,\n",
       "  'sim_soft': 0.28804162},\n",
       " 2174: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 434,\n",
       "  'pred_knn': 434,\n",
       "  'sim_real_cl': 0.44813305,\n",
       "  'sim_best_knn': 0.44813305,\n",
       "  'pred_soft': 6084,\n",
       "  'soft_val': 0.30911800265312195,\n",
       "  'sim_soft': 0.3305541},\n",
       " 2198: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 439,\n",
       "  'pred_knn': 439,\n",
       "  'sim_real_cl': 0.3657999,\n",
       "  'sim_best_knn': 0.3657999,\n",
       "  'pred_soft': 5492,\n",
       "  'soft_val': 0.2503766715526581,\n",
       "  'sim_soft': 0.28096572},\n",
       " 2199: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 439,\n",
       "  'pred_knn': 2044,\n",
       "  'sim_real_cl': 0.22164902,\n",
       "  'sim_best_knn': 0.3102197,\n",
       "  'pred_soft': 3420,\n",
       "  'soft_val': 0.23728878796100616,\n",
       "  'sim_soft': 0.22995694,\n",
       "  'pred_max_max': 2044,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28772297501564026,\n",
       "  'sim_max_max': 0.3102197},\n",
       " 2205: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 441,\n",
       "  'pred_knn': 2091,\n",
       "  'sim_real_cl': 0.3193235,\n",
       "  'sim_best_knn': 0.37307107,\n",
       "  'pred_soft': 2091,\n",
       "  'soft_val': 0.279217392206192,\n",
       "  'sim_soft': 0.37307107,\n",
       "  'pred_max_max': 2091,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3406207859516144,\n",
       "  'sim_max_max': 0.37307107},\n",
       " 2227: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 445,\n",
       "  'pred_knn': 7880,\n",
       "  'sim_real_cl': 0.23355615,\n",
       "  'sim_best_knn': 0.2812924,\n",
       "  'pred_soft': 7880,\n",
       "  'soft_val': 0.2376558482646942,\n",
       "  'sim_soft': 0.2812924,\n",
       "  'pred_max_max': 1214,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.22524072229862213,\n",
       "  'sim_max_max': 0.26099735},\n",
       " 2322: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 464,\n",
       "  'pred_knn': 7949,\n",
       "  'sim_real_cl': 0.23814763,\n",
       "  'sim_best_knn': 0.36471772,\n",
       "  'pred_soft': 7949,\n",
       "  'soft_val': 0.26311978697776794,\n",
       "  'sim_soft': 0.36471772,\n",
       "  'pred_max_max': 1493,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3041146993637085,\n",
       "  'sim_max_max': 0.3514688},\n",
       " 2402: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 480,\n",
       "  'pred_knn': 480,\n",
       "  'sim_real_cl': 0.26242074,\n",
       "  'sim_best_knn': 0.26242074,\n",
       "  'pred_soft': 9782,\n",
       "  'soft_val': 0.24677011370658875,\n",
       "  'sim_soft': 0.23359475,\n",
       "  'pred_max_max': 9782,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2298256903886795,\n",
       "  'sim_max_max': 0.23359475},\n",
       " 2412: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 482,\n",
       "  'pred_knn': 482,\n",
       "  'sim_real_cl': 0.33779934,\n",
       "  'sim_best_knn': 0.33779934,\n",
       "  'pred_soft': 2663,\n",
       "  'soft_val': 0.25366881489753723,\n",
       "  'sim_soft': 0.28265858,\n",
       "  'pred_max_max': 3827,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2605277895927429,\n",
       "  'sim_max_max': 0.3052403},\n",
       " 2413: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 482,\n",
       "  'pred_knn': 482,\n",
       "  'sim_real_cl': 0.38991258,\n",
       "  'sim_best_knn': 0.38991258,\n",
       "  'pred_soft': 2018,\n",
       "  'soft_val': 0.31116753816604614,\n",
       "  'sim_soft': 0.36349416,\n",
       "  'pred_max_max': 2018,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.31018421053886414,\n",
       "  'sim_max_max': 0.36349416},\n",
       " 2414: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 482,\n",
       "  'pred_knn': 1172,\n",
       "  'sim_real_cl': 0.21593297,\n",
       "  'sim_best_knn': 0.3587632,\n",
       "  'pred_soft': 1904,\n",
       "  'soft_val': 0.26977458596229553,\n",
       "  'sim_soft': 0.30082986,\n",
       "  'pred_max_max': 2829,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24902838468551636,\n",
       "  'sim_max_max': 0.24912369},\n",
       " 2437: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 487,\n",
       "  'pred_knn': 487,\n",
       "  'sim_real_cl': 0.38072777,\n",
       "  'sim_best_knn': 0.38072777,\n",
       "  'pred_soft': 5073,\n",
       "  'soft_val': 0.24773633480072021,\n",
       "  'sim_soft': 0.2741093},\n",
       " 2438: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 487,\n",
       "  'pred_knn': 2015,\n",
       "  'sim_real_cl': 0.006670229,\n",
       "  'sim_best_knn': 0.28617555,\n",
       "  'pred_soft': 3290,\n",
       "  'soft_val': 0.2515939772129059,\n",
       "  'sim_soft': 0.14135942,\n",
       "  'pred_max_max': 4342,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27090299129486084,\n",
       "  'sim_max_max': 0.2386936},\n",
       " 2439: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 487,\n",
       "  'pred_knn': 5981,\n",
       "  'sim_real_cl': 0.355469,\n",
       "  'sim_best_knn': 0.39314497,\n",
       "  'pred_soft': 5981,\n",
       "  'soft_val': 0.282495379447937,\n",
       "  'sim_soft': 0.39314497,\n",
       "  'pred_max_max': 487,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3259624242782593,\n",
       "  'sim_max_max': 0.39314497},\n",
       " 2482: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 496,\n",
       "  'pred_knn': 2484,\n",
       "  'sim_real_cl': 0.21754567,\n",
       "  'sim_best_knn': 0.35239452,\n",
       "  'pred_soft': 2484,\n",
       "  'soft_val': 0.31089192628860474,\n",
       "  'sim_soft': 0.35239452,\n",
       "  'pred_max_max': 2484,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30806824564933777,\n",
       "  'sim_max_max': 0.35239452},\n",
       " 2623: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 524,\n",
       "  'pred_knn': 524,\n",
       "  'sim_real_cl': 0.3703248,\n",
       "  'sim_best_knn': 0.3703248,\n",
       "  'pred_soft': 742,\n",
       "  'soft_val': 0.2721165716648102,\n",
       "  'sim_soft': 0.29341695,\n",
       "  'pred_max_max': 4562,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2869250476360321,\n",
       "  'sim_max_max': 0.32929462},\n",
       " 2627: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 525,\n",
       "  'pred_knn': 4638,\n",
       "  'sim_real_cl': 0.37533367,\n",
       "  'sim_best_knn': 0.39057454,\n",
       "  'pred_soft': 4638,\n",
       "  'soft_val': 0.27723225951194763,\n",
       "  'sim_soft': 0.39057454,\n",
       "  'pred_max_max': 4638,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3243767023086548,\n",
       "  'sim_max_max': 0.39057454},\n",
       " 2637: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 527,\n",
       "  'pred_knn': 527,\n",
       "  'sim_real_cl': 0.46206337,\n",
       "  'sim_best_knn': 0.46206337,\n",
       "  'pred_soft': 9311,\n",
       "  'soft_val': 0.27241072058677673,\n",
       "  'sim_soft': 0.30456233},\n",
       " 2691: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 538,\n",
       "  'pred_knn': 1933,\n",
       "  'sim_real_cl': 0.26841798,\n",
       "  'sim_best_knn': 0.3696918,\n",
       "  'pred_soft': 1933,\n",
       "  'soft_val': 0.30875182151794434,\n",
       "  'sim_soft': 0.3696918,\n",
       "  'pred_max_max': 1933,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.31017112731933594,\n",
       "  'sim_max_max': 0.3696918},\n",
       " 2772: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 554,\n",
       "  'pred_knn': 829,\n",
       "  'sim_real_cl': 0.31381726,\n",
       "  'sim_best_knn': 0.37468138,\n",
       "  'pred_soft': 554,\n",
       "  'soft_val': 0.319789856672287,\n",
       "  'sim_soft': array(0.31978986, dtype=float32),\n",
       "  'pred_max_max': 554,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.35990819334983826,\n",
       "  'sim_max_max': 0.37468138},\n",
       " 2782: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 556,\n",
       "  'pred_knn': 8856,\n",
       "  'sim_real_cl': 0.3527118,\n",
       "  'sim_best_knn': 0.38148302,\n",
       "  'pred_soft': 8856,\n",
       "  'soft_val': 0.3421138823032379,\n",
       "  'sim_soft': 0.38148302,\n",
       "  'pred_max_max': 8856,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.34492939710617065,\n",
       "  'sim_max_max': 0.38148302},\n",
       " 2794: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 558,\n",
       "  'pred_knn': 558,\n",
       "  'sim_real_cl': 0.29139495,\n",
       "  'sim_best_knn': 0.29139495,\n",
       "  'pred_soft': 8369,\n",
       "  'soft_val': 0.2974783778190613,\n",
       "  'sim_soft': 0.25325263},\n",
       " 2818: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 563,\n",
       "  'pred_knn': 563,\n",
       "  'sim_real_cl': 0.4066427,\n",
       "  'sim_best_knn': 0.4066427,\n",
       "  'pred_soft': 1270,\n",
       "  'soft_val': 0.2815355062484741,\n",
       "  'sim_soft': 0.3792409},\n",
       " 2839: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 567,\n",
       "  'pred_knn': 567,\n",
       "  'sim_real_cl': 0.35451347,\n",
       "  'sim_best_knn': 0.35451347,\n",
       "  'pred_soft': 4865,\n",
       "  'soft_val': 0.2808704972267151,\n",
       "  'sim_soft': 0.30264634,\n",
       "  'pred_max_max': 6776,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2773876488208771,\n",
       "  'sim_max_max': 0.2954437},\n",
       " 2884: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 576,\n",
       "  'pred_knn': 256,\n",
       "  'sim_real_cl': 0.3427993,\n",
       "  'sim_best_knn': 0.35047406,\n",
       "  'pred_soft': 576,\n",
       "  'soft_val': 0.30732348561286926,\n",
       "  'sim_soft': array(0.3073235, dtype=float32),\n",
       "  'pred_max_max': 576,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.35838931798934937,\n",
       "  'sim_max_max': 0.35047406},\n",
       " 2902: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 580,\n",
       "  'pred_knn': 6943,\n",
       "  'sim_real_cl': 0.22809482,\n",
       "  'sim_best_knn': 0.2678833,\n",
       "  'pred_soft': 1807,\n",
       "  'soft_val': 0.25443992018699646,\n",
       "  'sim_soft': 0.25331587,\n",
       "  'pred_max_max': 1807,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2725445032119751,\n",
       "  'sim_max_max': 0.25331587},\n",
       " 2971: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 594,\n",
       "  'pred_knn': 594,\n",
       "  'sim_real_cl': 0.60851157,\n",
       "  'sim_best_knn': 0.60851157,\n",
       "  'pred_soft': 2664,\n",
       "  'soft_val': 0.3498670160770416,\n",
       "  'sim_soft': 0.36641818},\n",
       " 2972: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 594,\n",
       "  'pred_knn': 594,\n",
       "  'sim_real_cl': 0.5783013,\n",
       "  'sim_best_knn': 0.5783013,\n",
       "  'pred_soft': 2664,\n",
       "  'soft_val': 0.36403515934944153,\n",
       "  'sim_soft': 0.38569582},\n",
       " 2974: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 594,\n",
       "  'pred_knn': 594,\n",
       "  'sim_real_cl': 0.45473248,\n",
       "  'sim_best_knn': 0.45473248,\n",
       "  'pred_soft': 2135,\n",
       "  'soft_val': 0.2712809443473816,\n",
       "  'sim_soft': 0.33596358},\n",
       " 3011: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 602,\n",
       "  'pred_knn': 602,\n",
       "  'sim_real_cl': 0.5249647,\n",
       "  'sim_best_knn': 0.5249647,\n",
       "  'pred_soft': 4246,\n",
       "  'soft_val': 0.3742621839046478,\n",
       "  'sim_soft': 0.4964652,\n",
       "  'pred_max_max': 4246,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.46072694659233093,\n",
       "  'sim_max_max': 0.4964652},\n",
       " 3031: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 606,\n",
       "  'pred_knn': 3493,\n",
       "  'sim_real_cl': 0.2793603,\n",
       "  'sim_best_knn': 0.3097785,\n",
       "  'pred_soft': 606,\n",
       "  'soft_val': 0.3815360963344574,\n",
       "  'sim_soft': array(0.3815361, dtype=float32),\n",
       "  'pred_max_max': 606,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.29450494050979614,\n",
       "  'sim_max_max': 0.3097785},\n",
       " 3050: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 610,\n",
       "  'pred_knn': 8523,\n",
       "  'sim_real_cl': 0.17586529,\n",
       "  'sim_best_knn': 0.27983335,\n",
       "  'pred_soft': 4156,\n",
       "  'soft_val': 0.22285325825214386,\n",
       "  'sim_soft': 0.25238436,\n",
       "  'pred_max_max': 8372,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.22446781396865845,\n",
       "  'sim_max_max': 0.22970791},\n",
       " 3074: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 614,\n",
       "  'pred_knn': 2028,\n",
       "  'sim_real_cl': 0.12398876,\n",
       "  'sim_best_knn': 0.32677287,\n",
       "  'pred_soft': 4438,\n",
       "  'soft_val': 0.23717187345027924,\n",
       "  'sim_soft': 0.28707558,\n",
       "  'pred_max_max': 2028,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28399205207824707,\n",
       "  'sim_max_max': 0.32677287},\n",
       " 3129: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 625,\n",
       "  'pred_knn': 8001,\n",
       "  'sim_real_cl': 0.12776591,\n",
       "  'sim_best_knn': 0.29182738,\n",
       "  'pred_soft': 8001,\n",
       "  'soft_val': 0.22816143929958344,\n",
       "  'sim_soft': 0.29182738,\n",
       "  'pred_max_max': 160,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24999991059303284,\n",
       "  'sim_max_max': 0.27549845},\n",
       " 3133: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 626,\n",
       "  'pred_knn': 626,\n",
       "  'sim_real_cl': 0.4025011,\n",
       "  'sim_best_knn': 0.4025011,\n",
       "  'pred_soft': 5327,\n",
       "  'soft_val': 0.280635803937912,\n",
       "  'sim_soft': 0.27088773},\n",
       " 3136: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 627,\n",
       "  'pred_knn': 627,\n",
       "  'sim_real_cl': 0.44717357,\n",
       "  'sim_best_knn': 0.44717357,\n",
       "  'pred_soft': 2544,\n",
       "  'soft_val': 0.30589911341667175,\n",
       "  'sim_soft': 0.3494075},\n",
       " 3164: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 632,\n",
       "  'pred_knn': 1483,\n",
       "  'sim_real_cl': 0.37318462,\n",
       "  'sim_best_knn': 0.39492097,\n",
       "  'pred_soft': 632,\n",
       "  'soft_val': 0.285795122385025,\n",
       "  'sim_soft': array(0.28579512, dtype=float32),\n",
       "  'pred_max_max': 632,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3286482095718384,\n",
       "  'sim_max_max': 0.39492097},\n",
       " 3167: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 633,\n",
       "  'pred_knn': 633,\n",
       "  'sim_real_cl': 0.34161538,\n",
       "  'sim_best_knn': 0.34161538,\n",
       "  'pred_soft': 4836,\n",
       "  'soft_val': 0.24995014071464539,\n",
       "  'sim_soft': 0.19646609,\n",
       "  'pred_max_max': 4587,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24830439686775208,\n",
       "  'sim_max_max': 0.24773984},\n",
       " 3171: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 634,\n",
       "  'pred_knn': 1134,\n",
       "  'sim_real_cl': 0.3578186,\n",
       "  'sim_best_knn': 0.377431,\n",
       "  'pred_soft': 634,\n",
       "  'soft_val': 0.3100288510322571,\n",
       "  'sim_soft': array(0.31002885, dtype=float32),\n",
       "  'pred_max_max': 1134,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3242253065109253,\n",
       "  'sim_max_max': 0.377431},\n",
       " 3178: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 635,\n",
       "  'pred_knn': 635,\n",
       "  'sim_real_cl': 0.41126505,\n",
       "  'sim_best_knn': 0.41126505,\n",
       "  'pred_soft': 5368,\n",
       "  'soft_val': 0.3393060863018036,\n",
       "  'sim_soft': 0.38523132,\n",
       "  'pred_max_max': 5368,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3623272180557251,\n",
       "  'sim_max_max': 0.38523132},\n",
       " 3187: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 637,\n",
       "  'pred_knn': 2059,\n",
       "  'sim_real_cl': 0.1778372,\n",
       "  'sim_best_knn': 0.30058914,\n",
       "  'pred_soft': 2059,\n",
       "  'soft_val': 0.30353474617004395,\n",
       "  'sim_soft': 0.30058914,\n",
       "  'pred_max_max': 2059,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30370408296585083,\n",
       "  'sim_max_max': 0.30058914},\n",
       " 3197: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 639,\n",
       "  'pred_knn': 639,\n",
       "  'sim_real_cl': 0.42082605,\n",
       "  'sim_best_knn': 0.42082605,\n",
       "  'pred_soft': 8384,\n",
       "  'soft_val': 0.3192029595375061,\n",
       "  'sim_soft': 0.35515726,\n",
       "  'pred_max_max': 8384,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.32138076424598694,\n",
       "  'sim_max_max': 0.35515726},\n",
       " 3199: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 639,\n",
       "  'pred_knn': 639,\n",
       "  'sim_real_cl': 0.3607023,\n",
       "  'sim_best_knn': 0.3607023,\n",
       "  'pred_soft': 2830,\n",
       "  'soft_val': 0.28494319319725037,\n",
       "  'sim_soft': 0.2887946},\n",
       " 3217: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 643,\n",
       "  'pred_knn': 8093,\n",
       "  'sim_real_cl': 0.105744235,\n",
       "  'sim_best_knn': 0.30626047,\n",
       "  'pred_soft': 8783,\n",
       "  'soft_val': 0.22719678282737732,\n",
       "  'sim_soft': 0.1948601,\n",
       "  'pred_max_max': 642,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.22744351625442505,\n",
       "  'sim_max_max': 0.23323749},\n",
       " 3254: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 650,\n",
       "  'pred_knn': 650,\n",
       "  'sim_real_cl': 0.36186773,\n",
       "  'sim_best_knn': 0.36186773,\n",
       "  'pred_soft': 2731,\n",
       "  'soft_val': 0.3019482493400574,\n",
       "  'sim_soft': 0.2974936},\n",
       " 3276: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 655,\n",
       "  'pred_knn': 6309,\n",
       "  'sim_real_cl': 0.31577653,\n",
       "  'sim_best_knn': 0.33581606,\n",
       "  'pred_soft': 655,\n",
       "  'soft_val': 0.2746003568172455,\n",
       "  'sim_soft': array(0.27460036, dtype=float32),\n",
       "  'pred_max_max': 1441,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2790215313434601,\n",
       "  'sim_max_max': 0.31885198},\n",
       " 3311: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 662,\n",
       "  'pred_knn': 662,\n",
       "  'sim_real_cl': 0.65484613,\n",
       "  'sim_best_knn': 0.65484613,\n",
       "  'pred_soft': 4990,\n",
       "  'soft_val': 0.30105844140052795,\n",
       "  'sim_soft': 0.34813237},\n",
       " 3317: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 663,\n",
       "  'pred_knn': 2532,\n",
       "  'sim_real_cl': 0.26831466,\n",
       "  'sim_best_knn': 0.3792373,\n",
       "  'pred_soft': 8995,\n",
       "  'soft_val': 0.2712458074092865,\n",
       "  'sim_soft': 0.29010963,\n",
       "  'pred_max_max': 3408,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27736881375312805,\n",
       "  'sim_max_max': 0.28736356},\n",
       " 3387: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 677,\n",
       "  'pred_knn': 8273,\n",
       "  'sim_real_cl': 0.17223302,\n",
       "  'sim_best_knn': 0.3196314,\n",
       "  'pred_soft': 6980,\n",
       "  'soft_val': 0.2536053955554962,\n",
       "  'sim_soft': 0.27432442,\n",
       "  'pred_max_max': 6980,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25672316551208496,\n",
       "  'sim_max_max': 0.27432442},\n",
       " 3423: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 684,\n",
       "  'pred_knn': 684,\n",
       "  'sim_real_cl': 0.45462653,\n",
       "  'sim_best_knn': 0.45462653,\n",
       "  'pred_soft': 8731,\n",
       "  'soft_val': 0.3489939272403717,\n",
       "  'sim_soft': 0.35530317},\n",
       " 3435: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 687,\n",
       "  'pred_knn': 9006,\n",
       "  'sim_real_cl': -0.034710795,\n",
       "  'sim_best_knn': 0.26782402,\n",
       "  'pred_soft': 5675,\n",
       "  'soft_val': 0.23886598646640778,\n",
       "  'sim_soft': 0.2437652,\n",
       "  'pred_max_max': 5514,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24482357501983643,\n",
       "  'sim_max_max': 0.26152945},\n",
       " 3446: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 689,\n",
       "  'pred_knn': 6050,\n",
       "  'sim_real_cl': 0.4408933,\n",
       "  'sim_best_knn': 0.4794932,\n",
       "  'pred_soft': 6050,\n",
       "  'soft_val': 0.36026254296302795,\n",
       "  'sim_soft': 0.4794932,\n",
       "  'pred_max_max': 689,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.411643922328949,\n",
       "  'sim_max_max': 0.4794932},\n",
       " 3448: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 689,\n",
       "  'pred_knn': 689,\n",
       "  'sim_real_cl': 0.43140513,\n",
       "  'sim_best_knn': 0.43140513,\n",
       "  'pred_soft': 5442,\n",
       "  'soft_val': 0.2784855365753174,\n",
       "  'sim_soft': 0.30444917},\n",
       " 3451: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 690,\n",
       "  'pred_knn': 6043,\n",
       "  'sim_real_cl': 0.2975729,\n",
       "  'sim_best_knn': 0.32070696,\n",
       "  'pred_soft': 6043,\n",
       "  'soft_val': 0.2823196351528168,\n",
       "  'sim_soft': 0.32070696,\n",
       "  'pred_max_max': 6043,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3348855674266815,\n",
       "  'sim_max_max': 0.32070696},\n",
       " 3453: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 690,\n",
       "  'pred_knn': 9528,\n",
       "  'sim_real_cl': 0.16361752,\n",
       "  'sim_best_knn': 0.31266123,\n",
       "  'pred_soft': 3877,\n",
       "  'soft_val': 0.26347100734710693,\n",
       "  'sim_soft': 0.22400054,\n",
       "  'pred_max_max': 7884,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2619401216506958,\n",
       "  'sim_max_max': 0.23904467},\n",
       " 3454: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 690,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.38986683,\n",
       "  'sim_best_knn': 0.38986683,\n",
       "  'pred_soft': 5573,\n",
       "  'soft_val': 0.2415160834789276,\n",
       "  'sim_soft': 0.26432246,\n",
       "  'pred_max_max': 5573,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.23493856191635132,\n",
       "  'sim_max_max': 0.26432246},\n",
       " 3474: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 694,\n",
       "  'pred_knn': 694,\n",
       "  'sim_real_cl': 0.36841536,\n",
       "  'sim_best_knn': 0.36841536,\n",
       "  'pred_soft': 3073,\n",
       "  'soft_val': 0.30909639596939087,\n",
       "  'sim_soft': 0.30117774},\n",
       " 3544: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 708,\n",
       "  'pred_knn': 2589,\n",
       "  'sim_real_cl': 0.16119911,\n",
       "  'sim_best_knn': 0.2942354,\n",
       "  'pred_soft': 2197,\n",
       "  'soft_val': 0.286236435174942,\n",
       "  'sim_soft': 0.28845897,\n",
       "  'pred_max_max': 2197,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26956525444984436,\n",
       "  'sim_max_max': 0.28845897},\n",
       " 3554: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 710,\n",
       "  'pred_knn': 5447,\n",
       "  'sim_real_cl': 0.14757597,\n",
       "  'sim_best_knn': 0.25435108,\n",
       "  'pred_soft': 564,\n",
       "  'soft_val': 0.25366732478141785,\n",
       "  'sim_soft': 0.24550444,\n",
       "  'pred_max_max': 564,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26433753967285156,\n",
       "  'sim_max_max': 0.24550444},\n",
       " 3565: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 713,\n",
       "  'pred_knn': 713,\n",
       "  'sim_real_cl': 0.3606065,\n",
       "  'sim_best_knn': 0.3606065,\n",
       "  'pred_soft': 7573,\n",
       "  'soft_val': 0.2650567889213562,\n",
       "  'sim_soft': 0.2981649},\n",
       " 3568: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 713,\n",
       "  'pred_knn': 713,\n",
       "  'sim_real_cl': 0.40010127,\n",
       "  'sim_best_knn': 0.40010127,\n",
       "  'pred_soft': 7725,\n",
       "  'soft_val': 0.2658126652240753,\n",
       "  'sim_soft': 0.32480264},\n",
       " 3569: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 713,\n",
       "  'pred_knn': 5631,\n",
       "  'sim_real_cl': 0.28260383,\n",
       "  'sim_best_knn': 0.298075,\n",
       "  'pred_soft': 4807,\n",
       "  'soft_val': 0.24777571856975555,\n",
       "  'sim_soft': 0.24077913,\n",
       "  'pred_max_max': 5358,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.23316030204296112,\n",
       "  'sim_max_max': 0.23314406},\n",
       " 3716: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 743,\n",
       "  'pred_knn': 7983,\n",
       "  'sim_real_cl': 0.2704673,\n",
       "  'sim_best_knn': 0.31232917,\n",
       "  'pred_soft': 743,\n",
       "  'soft_val': 0.27682584524154663,\n",
       "  'sim_soft': array(0.27682585, dtype=float32),\n",
       "  'pred_max_max': 743,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.29574882984161377,\n",
       "  'sim_max_max': 0.31232917},\n",
       " 3720: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 744,\n",
       "  'pred_knn': 639,\n",
       "  'sim_real_cl': 0.037324503,\n",
       "  'sim_best_knn': 0.42423669,\n",
       "  'pred_soft': 2683,\n",
       "  'soft_val': 0.21805840730667114,\n",
       "  'sim_soft': 0.19448876,\n",
       "  'pred_max_max': 1534,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.23697897791862488,\n",
       "  'sim_max_max': 0.32076818},\n",
       " 3769: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 753,\n",
       "  'pred_knn': 753,\n",
       "  'sim_real_cl': 0.35611895,\n",
       "  'sim_best_knn': 0.35611895,\n",
       "  'pred_soft': 5623,\n",
       "  'soft_val': 0.299039751291275,\n",
       "  'sim_soft': 0.3155112},\n",
       " 3852: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 770,\n",
       "  'pred_knn': 770,\n",
       "  'sim_real_cl': 0.4133738,\n",
       "  'sim_best_knn': 0.4133738,\n",
       "  'pred_soft': 4433,\n",
       "  'soft_val': 0.2871882915496826,\n",
       "  'sim_soft': 0.35135025},\n",
       " 3876: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 775,\n",
       "  'pred_knn': 3661,\n",
       "  'sim_real_cl': 0.37434924,\n",
       "  'sim_best_knn': 0.37956688,\n",
       "  'pred_soft': 3661,\n",
       "  'soft_val': 0.3499350845813751,\n",
       "  'sim_soft': 0.37956688,\n",
       "  'pred_max_max': 3661,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.37680548429489136,\n",
       "  'sim_max_max': 0.37956688},\n",
       " 3989: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 797,\n",
       "  'pred_knn': 6374,\n",
       "  'sim_real_cl': 0.18662629,\n",
       "  'sim_best_knn': 0.3640293,\n",
       "  'pred_soft': 4958,\n",
       "  'soft_val': 0.2750532329082489,\n",
       "  'sim_soft': 0.29591438,\n",
       "  'pred_max_max': 6374,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.29438817501068115,\n",
       "  'sim_max_max': 0.3640293},\n",
       " 4018: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 803,\n",
       "  'pred_knn': 3445,\n",
       "  'sim_real_cl': 0.2873228,\n",
       "  'sim_best_knn': 0.29446742,\n",
       "  'pred_soft': 803,\n",
       "  'soft_val': 0.2809184789657593,\n",
       "  'sim_soft': array(0.28091848, dtype=float32),\n",
       "  'pred_max_max': 803,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.29841703176498413,\n",
       "  'sim_max_max': 0.29446742},\n",
       " 4029: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 805,\n",
       "  'pred_knn': 805,\n",
       "  'sim_real_cl': 0.30991238,\n",
       "  'sim_best_knn': 0.30991238,\n",
       "  'pred_soft': 4297,\n",
       "  'soft_val': 0.2787710726261139,\n",
       "  'sim_soft': 0.24781984},\n",
       " 4097: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 819,\n",
       "  'pred_knn': 5047,\n",
       "  'sim_real_cl': 0.3217488,\n",
       "  'sim_best_knn': 0.35009798,\n",
       "  'pred_soft': 819,\n",
       "  'soft_val': 0.2633574306964874,\n",
       "  'sim_soft': array(0.26335743, dtype=float32),\n",
       "  'pred_max_max': 819,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.26660066843032837,\n",
       "  'sim_max_max': 0.35009798},\n",
       " 4119: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 823,\n",
       "  'pred_knn': 823,\n",
       "  'sim_real_cl': 0.345231,\n",
       "  'sim_best_knn': 0.345231,\n",
       "  'pred_soft': 2770,\n",
       "  'soft_val': 0.2872542440891266,\n",
       "  'sim_soft': 0.3098411,\n",
       "  'pred_max_max': 3401,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3048592209815979,\n",
       "  'sim_max_max': 0.31935558},\n",
       " 4128: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 825,\n",
       "  'pred_knn': 825,\n",
       "  'sim_real_cl': 0.29366237,\n",
       "  'sim_best_knn': 0.29366237,\n",
       "  'pred_soft': 2728,\n",
       "  'soft_val': 0.2434135228395462,\n",
       "  'sim_soft': 0.21840258,\n",
       "  'pred_max_max': 590,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.23388536274433136,\n",
       "  'sim_max_max': 0.28108823},\n",
       " 4143: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 828,\n",
       "  'pred_knn': 3845,\n",
       "  'sim_real_cl': 0.34268847,\n",
       "  'sim_best_knn': 0.3510899,\n",
       "  'pred_soft': 828,\n",
       "  'soft_val': 0.2593238949775696,\n",
       "  'sim_soft': array(0.2593239, dtype=float32),\n",
       "  'pred_max_max': 828,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3037964105606079,\n",
       "  'sim_max_max': 0.3510899},\n",
       " 4148: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 829,\n",
       "  'pred_knn': 829,\n",
       "  'sim_real_cl': 0.43924046,\n",
       "  'sim_best_knn': 0.43924046,\n",
       "  'pred_soft': 2829,\n",
       "  'soft_val': 0.3021645247936249,\n",
       "  'sim_soft': 0.32729948,\n",
       "  'pred_max_max': 6584,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3330021798610687,\n",
       "  'sim_max_max': 0.2910222},\n",
       " 4155: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 831,\n",
       "  'pred_knn': 7368,\n",
       "  'sim_real_cl': 0.10582006,\n",
       "  'sim_best_knn': 0.38163465,\n",
       "  'pred_soft': 4865,\n",
       "  'soft_val': 0.2336338609457016,\n",
       "  'sim_soft': 0.28421262,\n",
       "  'pred_max_max': 4674,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.29309529066085815,\n",
       "  'sim_max_max': 0.31902745},\n",
       " 4162: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 832,\n",
       "  'pred_knn': 5249,\n",
       "  'sim_real_cl': 0.28680503,\n",
       "  'sim_best_knn': 0.35004622,\n",
       "  'pred_soft': 5249,\n",
       "  'soft_val': 0.28257423639297485,\n",
       "  'sim_soft': 0.35004622,\n",
       "  'pred_max_max': 2891,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.311186820268631,\n",
       "  'sim_max_max': 0.32838637},\n",
       " 4163: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 832,\n",
       "  'pred_knn': 591,\n",
       "  'sim_real_cl': 0.31947792,\n",
       "  'sim_best_knn': 0.35630265,\n",
       "  'pred_soft': 3497,\n",
       "  'soft_val': 0.2630676329135895,\n",
       "  'sim_soft': 0.22432509,\n",
       "  'pred_max_max': 591,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28789642453193665,\n",
       "  'sim_max_max': 0.35630265},\n",
       " 4164: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 832,\n",
       "  'pred_knn': 1277,\n",
       "  'sim_real_cl': 0.23722412,\n",
       "  'sim_best_knn': 0.347193,\n",
       "  'pred_soft': 7530,\n",
       "  'soft_val': 0.21694417297840118,\n",
       "  'sim_soft': 0.2454982,\n",
       "  'pred_max_max': 7360,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.23955227434635162,\n",
       "  'sim_max_max': 0.25808924},\n",
       " 4223: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 844,\n",
       "  'pred_knn': 9669,\n",
       "  'sim_real_cl': 0.3196853,\n",
       "  'sim_best_knn': 0.33460182,\n",
       "  'pred_soft': 844,\n",
       "  'soft_val': 0.307254433631897,\n",
       "  'sim_soft': array(0.30725443, dtype=float32),\n",
       "  'pred_max_max': 9669,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.31675148010253906,\n",
       "  'sim_max_max': 0.33460182},\n",
       " 4272: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 854,\n",
       "  'pred_knn': 6068,\n",
       "  'sim_real_cl': 0.2792542,\n",
       "  'sim_best_knn': 0.33957922,\n",
       "  'pred_soft': 854,\n",
       "  'soft_val': 0.26535096764564514,\n",
       "  'sim_soft': array(0.26535097, dtype=float32),\n",
       "  'pred_max_max': 2075,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2818254232406616,\n",
       "  'sim_max_max': 0.3022717},\n",
       " 4285: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 857,\n",
       "  'pred_knn': 2070,\n",
       "  'sim_real_cl': 0.2996548,\n",
       "  'sim_best_knn': 0.34321177,\n",
       "  'pred_soft': 2070,\n",
       "  'soft_val': 0.30822494626045227,\n",
       "  'sim_soft': 0.34321177,\n",
       "  'pred_max_max': 5573,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.29846760630607605,\n",
       "  'sim_max_max': 0.32509726},\n",
       " 4292: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 858,\n",
       "  'pred_knn': 7198,\n",
       "  'sim_real_cl': 0.36475638,\n",
       "  'sim_best_knn': 0.40920198,\n",
       "  'pred_soft': 858,\n",
       "  'soft_val': 0.29728877544403076,\n",
       "  'sim_soft': array(0.29728878, dtype=float32),\n",
       "  'pred_max_max': 7198,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.35252368450164795,\n",
       "  'sim_max_max': 0.40920198},\n",
       " 4317: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 863,\n",
       "  'pred_knn': 863,\n",
       "  'sim_real_cl': 0.32673752,\n",
       "  'sim_best_knn': 0.32673752,\n",
       "  'pred_soft': 7984,\n",
       "  'soft_val': 0.2786587178707123,\n",
       "  'sim_soft': 0.30738044},\n",
       " 4332: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 866,\n",
       "  'pred_knn': 866,\n",
       "  'sim_real_cl': 0.395213,\n",
       "  'sim_best_knn': 0.395213,\n",
       "  'pred_soft': 829,\n",
       "  'soft_val': 0.26733946800231934,\n",
       "  'sim_soft': 0.25932235},\n",
       " 4334: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 866,\n",
       "  'pred_knn': 2075,\n",
       "  'sim_real_cl': 0.23776942,\n",
       "  'sim_best_knn': 0.3137775,\n",
       "  'pred_soft': 2075,\n",
       "  'soft_val': 0.28375086188316345,\n",
       "  'sim_soft': 0.3137775,\n",
       "  'pred_max_max': 64,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2722904086112976,\n",
       "  'sim_max_max': 0.2493746},\n",
       " 4394: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 878,\n",
       "  'pred_knn': 5547,\n",
       "  'sim_real_cl': 0.1208905,\n",
       "  'sim_best_knn': 0.29151157,\n",
       "  'pred_soft': 2590,\n",
       "  'soft_val': 0.26721930503845215,\n",
       "  'sim_soft': 0.25527173,\n",
       "  'pred_max_max': 2590,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2504436671733856,\n",
       "  'sim_max_max': 0.25527173},\n",
       " 4433: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 886,\n",
       "  'pred_knn': 7460,\n",
       "  'sim_real_cl': 0.2865997,\n",
       "  'sim_best_knn': 0.34230256,\n",
       "  'pred_soft': 2316,\n",
       "  'soft_val': 0.3020108938217163,\n",
       "  'sim_soft': 0.33082065,\n",
       "  'pred_max_max': 2316,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.31132444739341736,\n",
       "  'sim_max_max': 0.33082065},\n",
       " 4488: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 897,\n",
       "  'pred_knn': 5934,\n",
       "  'sim_real_cl': 0.30278373,\n",
       "  'sim_best_knn': 0.33496502,\n",
       "  'pred_soft': 5934,\n",
       "  'soft_val': 0.3016989231109619,\n",
       "  'sim_soft': 0.33496502,\n",
       "  'pred_max_max': 1445,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30277174711227417,\n",
       "  'sim_max_max': 0.3082361},\n",
       " 4523: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 904,\n",
       "  'pred_knn': 1432,\n",
       "  'sim_real_cl': 0.074638456,\n",
       "  'sim_best_knn': 0.37150022,\n",
       "  'pred_soft': 5857,\n",
       "  'soft_val': 0.2591303884983063,\n",
       "  'sim_soft': 0.2158011,\n",
       "  'pred_max_max': 7423,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27069294452667236,\n",
       "  'sim_max_max': 0.262343},\n",
       " 4535: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 907,\n",
       "  'pred_knn': 1308,\n",
       "  'sim_real_cl': 0.25418264,\n",
       "  'sim_best_knn': 0.28383452,\n",
       "  'pred_soft': 907,\n",
       "  'soft_val': 0.39115455746650696,\n",
       "  'sim_soft': array(0.39115456, dtype=float32),\n",
       "  'pred_max_max': 1308,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28583335876464844,\n",
       "  'sim_max_max': 0.28383452},\n",
       " 4549: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 909,\n",
       "  'pred_knn': 639,\n",
       "  'sim_real_cl': 0.103733875,\n",
       "  'sim_best_knn': 0.32878888,\n",
       "  'pred_soft': 6636,\n",
       "  'soft_val': 0.24873629212379456,\n",
       "  'sim_soft': 0.2181294,\n",
       "  'pred_max_max': 9875,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.20738442242145538,\n",
       "  'sim_max_max': 0.14797795},\n",
       " 4572: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 914,\n",
       "  'pred_knn': 3261,\n",
       "  'sim_real_cl': 0.23697227,\n",
       "  'sim_best_knn': 0.33327183,\n",
       "  'pred_soft': 731,\n",
       "  'soft_val': 0.2636268138885498,\n",
       "  'sim_soft': 0.25573272,\n",
       "  'pred_max_max': 3261,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2700220048427582,\n",
       "  'sim_max_max': 0.33327183},\n",
       " 4640: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 928,\n",
       "  'pred_knn': 890,\n",
       "  'sim_real_cl': 0.22572044,\n",
       "  'sim_best_knn': 0.25913364,\n",
       "  'pred_soft': 890,\n",
       "  'soft_val': 0.2608312964439392,\n",
       "  'sim_soft': 0.25913364,\n",
       "  'pred_max_max': 890,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24025431275367737,\n",
       "  'sim_max_max': 0.25913364},\n",
       " 4642: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 928,\n",
       "  'pred_knn': 928,\n",
       "  'sim_real_cl': 0.2928269,\n",
       "  'sim_best_knn': 0.2928269,\n",
       "  'pred_soft': 5939,\n",
       "  'soft_val': 0.2482168972492218,\n",
       "  'sim_soft': 0.28244925,\n",
       "  'pred_max_max': 5939,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2557404339313507,\n",
       "  'sim_max_max': 0.28244925},\n",
       " 4699: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 939,\n",
       "  'pred_knn': 939,\n",
       "  'sim_real_cl': 0.4374851,\n",
       "  'sim_best_knn': 0.4374851,\n",
       "  'pred_soft': 7222,\n",
       "  'soft_val': 0.3057127892971039,\n",
       "  'sim_soft': 0.39162597},\n",
       " 4746: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 949,\n",
       "  'pred_knn': 3653,\n",
       "  'sim_real_cl': 0.29363763,\n",
       "  'sim_best_knn': 0.3737389,\n",
       "  'pred_soft': 2075,\n",
       "  'soft_val': 0.2378198653459549,\n",
       "  'sim_soft': 0.26995844,\n",
       "  'pred_max_max': 2075,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2501946985721588,\n",
       "  'sim_max_max': 0.26995844},\n",
       " 4769: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 953,\n",
       "  'pred_knn': 1134,\n",
       "  'sim_real_cl': 0.3016799,\n",
       "  'sim_best_knn': 0.38975132,\n",
       "  'pred_soft': 6856,\n",
       "  'soft_val': 0.26438677310943604,\n",
       "  'sim_soft': 0.24685164,\n",
       "  'pred_max_max': 1134,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.32591286301612854,\n",
       "  'sim_max_max': 0.38975132},\n",
       " 4822: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 964,\n",
       "  'pred_knn': 964,\n",
       "  'sim_real_cl': 0.46274403,\n",
       "  'sim_best_knn': 0.46274403,\n",
       "  'pred_soft': 2150,\n",
       "  'soft_val': 0.32037436962127686,\n",
       "  'sim_soft': 0.35858747},\n",
       " 4834: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 966,\n",
       "  'pred_knn': 9794,\n",
       "  'sim_real_cl': 0.10605295,\n",
       "  'sim_best_knn': 0.41273218,\n",
       "  'pred_soft': 2945,\n",
       "  'soft_val': 0.22833414375782013,\n",
       "  'sim_soft': 0.22819656,\n",
       "  'pred_max_max': 9065,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.23233163356781006,\n",
       "  'sim_max_max': 0.21329029},\n",
       " 4897: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 979,\n",
       "  'pred_knn': 979,\n",
       "  'sim_real_cl': 0.68819547,\n",
       "  'sim_best_knn': 0.68819547,\n",
       "  'pred_soft': 9935,\n",
       "  'soft_val': 0.31459519267082214,\n",
       "  'sim_soft': 0.3991685},\n",
       " 4937: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 987,\n",
       "  'pred_knn': 987,\n",
       "  'sim_real_cl': 0.34731105,\n",
       "  'sim_best_knn': 0.34731105,\n",
       "  'pred_soft': 2316,\n",
       "  'soft_val': 0.27559348940849304,\n",
       "  'sim_soft': 0.2846735,\n",
       "  'pred_max_max': 8263,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27043309807777405,\n",
       "  'sim_max_max': 0.34653068},\n",
       " 5004: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1000,\n",
       "  'pred_knn': 6528,\n",
       "  'sim_real_cl': 0.3061601,\n",
       "  'sim_best_knn': 0.33906215,\n",
       "  'pred_soft': 2999,\n",
       "  'soft_val': 0.2921195328235626,\n",
       "  'sim_soft': 0.29639256,\n",
       "  'pred_max_max': 6528,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3362754285335541,\n",
       "  'sim_max_max': 0.33906215},\n",
       " 5009: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1001,\n",
       "  'pred_knn': 1001,\n",
       "  'sim_real_cl': 0.37139404,\n",
       "  'sim_best_knn': 0.37139404,\n",
       "  'pred_soft': 5587,\n",
       "  'soft_val': 0.3052946925163269,\n",
       "  'sim_soft': 0.29248226},\n",
       " 5030: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1006,\n",
       "  'pred_knn': 1006,\n",
       "  'sim_real_cl': 0.7032964,\n",
       "  'sim_best_knn': 0.7032964,\n",
       "  'pred_soft': 4090,\n",
       "  'soft_val': 0.310944527387619,\n",
       "  'sim_soft': 0.3336339},\n",
       " 5031: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1006,\n",
       "  'pred_knn': 1006,\n",
       "  'sim_real_cl': 0.6481538,\n",
       "  'sim_best_knn': 0.6481538,\n",
       "  'pred_soft': 2036,\n",
       "  'soft_val': 0.28765514492988586,\n",
       "  'sim_soft': 0.34924182},\n",
       " 5032: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1006,\n",
       "  'pred_knn': 1006,\n",
       "  'sim_real_cl': 0.33436894,\n",
       "  'sim_best_knn': 0.33436894,\n",
       "  'pred_soft': 7540,\n",
       "  'soft_val': 0.2648904323577881,\n",
       "  'sim_soft': 0.2528184},\n",
       " 5034: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1006,\n",
       "  'pred_knn': 1006,\n",
       "  'sim_real_cl': 0.55964446,\n",
       "  'sim_best_knn': 0.55964446,\n",
       "  'pred_soft': 4090,\n",
       "  'soft_val': 0.2824779450893402,\n",
       "  'sim_soft': 0.29214054},\n",
       " 5052: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1010,\n",
       "  'pred_knn': 1010,\n",
       "  'sim_real_cl': 0.49240702,\n",
       "  'sim_best_knn': 0.49240702,\n",
       "  'pred_soft': 7276,\n",
       "  'soft_val': 0.3247670531272888,\n",
       "  'sim_soft': 0.3366379},\n",
       " 5098: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1019,\n",
       "  'pred_knn': 8001,\n",
       "  'sim_real_cl': 0.21386087,\n",
       "  'sim_best_knn': 0.3063155,\n",
       "  'pred_soft': 3806,\n",
       "  'soft_val': 0.2429543137550354,\n",
       "  'sim_soft': 0.27284795,\n",
       "  'pred_max_max': 3806,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24994376301765442,\n",
       "  'sim_max_max': 0.27284795},\n",
       " 5127: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1025,\n",
       "  'pred_knn': 1025,\n",
       "  'sim_real_cl': 0.39079362,\n",
       "  'sim_best_knn': 0.39079362,\n",
       "  'pred_soft': 4959,\n",
       "  'soft_val': 0.297813355922699,\n",
       "  'sim_soft': 0.31603062,\n",
       "  'pred_max_max': 4959,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2903444766998291,\n",
       "  'sim_max_max': 0.31603062},\n",
       " 5149: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1029,\n",
       "  'pred_knn': 1029,\n",
       "  'sim_real_cl': 0.468408,\n",
       "  'sim_best_knn': 0.468408,\n",
       "  'pred_soft': 2075,\n",
       "  'soft_val': 0.296968936920166,\n",
       "  'sim_soft': 0.32986826},\n",
       " 5156: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1031,\n",
       "  'pred_knn': 3868,\n",
       "  'sim_real_cl': 0.26902044,\n",
       "  'sim_best_knn': 0.29373968,\n",
       "  'pred_soft': 4819,\n",
       "  'soft_val': 0.26830050349235535,\n",
       "  'sim_soft': 0.29323155,\n",
       "  'pred_max_max': 1031,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3034577965736389,\n",
       "  'sim_max_max': 0.29373968},\n",
       " 5165: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1033,\n",
       "  'pred_knn': 5593,\n",
       "  'sim_real_cl': 0.31848183,\n",
       "  'sim_best_knn': 0.32115573,\n",
       "  'pred_soft': 1033,\n",
       "  'soft_val': 0.34445348381996155,\n",
       "  'sim_soft': array(0.34445348, dtype=float32),\n",
       "  'pred_max_max': 1033,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3116713762283325,\n",
       "  'sim_max_max': 0.32115573},\n",
       " 5168: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1033,\n",
       "  'pred_knn': 6716,\n",
       "  'sim_real_cl': 0.2449289,\n",
       "  'sim_best_knn': 0.31017476,\n",
       "  'pred_soft': 6716,\n",
       "  'soft_val': 0.2479643076658249,\n",
       "  'sim_soft': 0.31017476,\n",
       "  'pred_max_max': 7677,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26797670125961304,\n",
       "  'sim_max_max': 0.29068533},\n",
       " 5198: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1039,\n",
       "  'pred_knn': 4084,\n",
       "  'sim_real_cl': 0.22549212,\n",
       "  'sim_best_knn': 0.29347092,\n",
       "  'pred_soft': 7331,\n",
       "  'soft_val': 0.2569648027420044,\n",
       "  'sim_soft': 0.2619921,\n",
       "  'pred_max_max': 7331,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25962233543395996,\n",
       "  'sim_max_max': 0.2619921},\n",
       " 5246: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1049,\n",
       "  'pred_knn': 8654,\n",
       "  'sim_real_cl': 0.2514611,\n",
       "  'sim_best_knn': 0.28301018,\n",
       "  'pred_soft': 1049,\n",
       "  'soft_val': 0.24620278179645538,\n",
       "  'sim_soft': array(0.24620278, dtype=float32),\n",
       "  'pred_max_max': 1049,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.23980507254600525,\n",
       "  'sim_max_max': 0.28301018},\n",
       " 5393: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1078,\n",
       "  'pred_knn': 1078,\n",
       "  'sim_real_cl': 0.45975935,\n",
       "  'sim_best_knn': 0.45975935,\n",
       "  'pred_soft': 8698,\n",
       "  'soft_val': 0.33605438470840454,\n",
       "  'sim_soft': 0.3749837},\n",
       " 5446: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1089,\n",
       "  'pred_knn': 1089,\n",
       "  'sim_real_cl': 0.5054563,\n",
       "  'sim_best_knn': 0.5054563,\n",
       "  'pred_soft': 923,\n",
       "  'soft_val': 0.27584367990493774,\n",
       "  'sim_soft': 0.28871524},\n",
       " 5449: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1089,\n",
       "  'pred_knn': 1089,\n",
       "  'sim_real_cl': 0.50845623,\n",
       "  'sim_best_knn': 0.50845623,\n",
       "  'pred_soft': 5692,\n",
       "  'soft_val': 0.2754243016242981,\n",
       "  'sim_soft': 0.2863747},\n",
       " 5479: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1095,\n",
       "  'pred_knn': 2368,\n",
       "  'sim_real_cl': 0.2869135,\n",
       "  'sim_best_knn': 0.30822763,\n",
       "  'pred_soft': 2368,\n",
       "  'soft_val': 0.25398731231689453,\n",
       "  'sim_soft': 0.30822763,\n",
       "  'pred_max_max': 1095,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.2758239805698395,\n",
       "  'sim_max_max': 0.30822763},\n",
       " 5494: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1098,\n",
       "  'pred_knn': 5982,\n",
       "  'sim_real_cl': 0.24704859,\n",
       "  'sim_best_knn': 0.2764837,\n",
       "  'pred_soft': 8921,\n",
       "  'soft_val': 0.28812646865844727,\n",
       "  'sim_soft': 0.22755808,\n",
       "  'pred_max_max': 1098,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3401329517364502,\n",
       "  'sim_max_max': 0.2764837},\n",
       " 5557: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1111,\n",
       "  'pred_knn': 1111,\n",
       "  'sim_real_cl': 0.41505522,\n",
       "  'sim_best_knn': 0.41505522,\n",
       "  'pred_soft': 9927,\n",
       "  'soft_val': 0.28148847818374634,\n",
       "  'sim_soft': 0.3363787},\n",
       " 5574: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1114,\n",
       "  'pred_knn': 1114,\n",
       "  'sim_real_cl': 0.5617101,\n",
       "  'sim_best_knn': 0.5617101,\n",
       "  'pred_soft': 2714,\n",
       "  'soft_val': 0.26860490441322327,\n",
       "  'sim_soft': 0.2692001},\n",
       " 5604: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1120,\n",
       "  'pred_knn': 1120,\n",
       "  'sim_real_cl': 0.38863164,\n",
       "  'sim_best_knn': 0.38863164,\n",
       "  'pred_soft': 405,\n",
       "  'soft_val': 0.2950318455696106,\n",
       "  'sim_soft': 0.30077255,\n",
       "  'pred_max_max': 405,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.31813520193099976,\n",
       "  'sim_max_max': 0.30077255},\n",
       " 5617: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1123,\n",
       "  'pred_knn': 1123,\n",
       "  'sim_real_cl': 0.40108037,\n",
       "  'sim_best_knn': 0.40108037,\n",
       "  'pred_soft': 5358,\n",
       "  'soft_val': 0.2567620277404785,\n",
       "  'sim_soft': 0.24912852,\n",
       "  'pred_max_max': 4271,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30702438950538635,\n",
       "  'sim_max_max': 0.2942241},\n",
       " 5619: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1123,\n",
       "  'pred_knn': 1123,\n",
       "  'sim_real_cl': 0.40735948,\n",
       "  'sim_best_knn': 0.40735948,\n",
       "  'pred_soft': 5281,\n",
       "  'soft_val': 0.28304794430732727,\n",
       "  'sim_soft': 0.3053693,\n",
       "  'pred_max_max': 4202,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27620598673820496,\n",
       "  'sim_max_max': 0.34086865},\n",
       " 5625: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1125,\n",
       "  'pred_knn': 2381,\n",
       "  'sim_real_cl': 0.3777827,\n",
       "  'sim_best_knn': 0.41187292,\n",
       "  'pred_soft': 6413,\n",
       "  'soft_val': 0.27841487526893616,\n",
       "  'sim_soft': 0.38714156,\n",
       "  'pred_max_max': 2381,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3200289011001587,\n",
       "  'sim_max_max': 0.41187292},\n",
       " 5757: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1151,\n",
       "  'pred_knn': 1151,\n",
       "  'sim_real_cl': 0.35507533,\n",
       "  'sim_best_knn': 0.35507533,\n",
       "  'pred_soft': 5577,\n",
       "  'soft_val': 0.28129714727401733,\n",
       "  'sim_soft': 0.30961078},\n",
       " 5763: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1152,\n",
       "  'pred_knn': 1152,\n",
       "  'sim_real_cl': 0.30054674,\n",
       "  'sim_best_knn': 0.30054674,\n",
       "  'pred_soft': 7717,\n",
       "  'soft_val': 0.2601527273654938,\n",
       "  'sim_soft': 0.21575643},\n",
       " 5788: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1157,\n",
       "  'pred_knn': 639,\n",
       "  'sim_real_cl': 0.12250869,\n",
       "  'sim_best_knn': 0.36770713,\n",
       "  'pred_soft': 352,\n",
       "  'soft_val': 0.23049508035182953,\n",
       "  'sim_soft': 0.30107766,\n",
       "  'pred_max_max': 352,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24276743829250336,\n",
       "  'sim_max_max': 0.30107766},\n",
       " 5792: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1158,\n",
       "  'pred_knn': 6933,\n",
       "  'sim_real_cl': 0.35738742,\n",
       "  'sim_best_knn': 0.3645322,\n",
       "  'pred_soft': 6933,\n",
       "  'soft_val': 0.2861148715019226,\n",
       "  'sim_soft': 0.3645322,\n",
       "  'pred_max_max': 6933,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3415313959121704,\n",
       "  'sim_max_max': 0.3645322},\n",
       " 5848: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1169,\n",
       "  'pred_knn': 1169,\n",
       "  'sim_real_cl': 0.38869327,\n",
       "  'sim_best_knn': 0.38869327,\n",
       "  'pred_soft': 2148,\n",
       "  'soft_val': 0.25843778252601624,\n",
       "  'sim_soft': 0.25316304},\n",
       " 5864: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1172,\n",
       "  'pred_knn': 1172,\n",
       "  'sim_real_cl': 0.42099515,\n",
       "  'sim_best_knn': 0.42099515,\n",
       "  'pred_soft': 2428,\n",
       "  'soft_val': 0.2708878219127655,\n",
       "  'sim_soft': 0.3329685,\n",
       "  'pred_max_max': 2428,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26482832431793213,\n",
       "  'sim_max_max': 0.3329685},\n",
       " 5893: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1178,\n",
       "  'pred_knn': 3354,\n",
       "  'sim_real_cl': 0.28497714,\n",
       "  'sim_best_knn': 0.30347466,\n",
       "  'pred_soft': 2518,\n",
       "  'soft_val': 0.2401231825351715,\n",
       "  'sim_soft': 0.25804663,\n",
       "  'pred_max_max': 1178,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.2969021201133728,\n",
       "  'sim_max_max': 0.30347466},\n",
       " 5904: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1180,\n",
       "  'pred_knn': 1180,\n",
       "  'sim_real_cl': 0.31473464,\n",
       "  'sim_best_knn': 0.31473464,\n",
       "  'pred_soft': 5962,\n",
       "  'soft_val': 0.2890755236148834,\n",
       "  'sim_soft': 0.27502596},\n",
       " 5927: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1185,\n",
       "  'pred_knn': 8587,\n",
       "  'sim_real_cl': 0.31411296,\n",
       "  'sim_best_knn': 0.35187247,\n",
       "  'pred_soft': 2856,\n",
       "  'soft_val': 0.2635440230369568,\n",
       "  'sim_soft': 0.33995977,\n",
       "  'pred_max_max': 2064,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28926098346710205,\n",
       "  'sim_max_max': 0.34985167},\n",
       " 5939: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1187,\n",
       "  'pred_knn': 4525,\n",
       "  'sim_real_cl': 0.31332102,\n",
       "  'sim_best_knn': 0.33734202,\n",
       "  'pred_soft': 1187,\n",
       "  'soft_val': 0.3713580369949341,\n",
       "  'sim_soft': array(0.37135804, dtype=float32),\n",
       "  'pred_max_max': 3278,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.29789999127388,\n",
       "  'sim_max_max': 0.2903385},\n",
       " 5962: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1192,\n",
       "  'pred_knn': 8943,\n",
       "  'sim_real_cl': 0.27431104,\n",
       "  'sim_best_knn': 0.31154546,\n",
       "  'pred_soft': 2191,\n",
       "  'soft_val': 0.2548356056213379,\n",
       "  'sim_soft': 0.30375955,\n",
       "  'pred_max_max': 3242,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24312710762023926,\n",
       "  'sim_max_max': 0.2704966},\n",
       " 5966: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1193,\n",
       "  'pred_knn': 2252,\n",
       "  'sim_real_cl': 0.32962996,\n",
       "  'sim_best_knn': 0.3420257,\n",
       "  'pred_soft': 9716,\n",
       "  'soft_val': 0.24422495067119598,\n",
       "  'sim_soft': 0.2663318,\n",
       "  'pred_max_max': 1193,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.28178149461746216,\n",
       "  'sim_max_max': 0.3420257},\n",
       " 6015: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1203,\n",
       "  'pred_knn': 7544,\n",
       "  'sim_real_cl': 0.306643,\n",
       "  'sim_best_knn': 0.3400636,\n",
       "  'pred_soft': 2916,\n",
       "  'soft_val': 0.2645756006240845,\n",
       "  'sim_soft': 0.271717,\n",
       "  'pred_max_max': 1203,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.32745495438575745,\n",
       "  'sim_max_max': 0.3400636},\n",
       " 6024: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1204,\n",
       "  'pred_knn': 8170,\n",
       "  'sim_real_cl': 0.2248585,\n",
       "  'sim_best_knn': 0.32461745,\n",
       "  'pred_soft': 4459,\n",
       "  'soft_val': 0.23446957767009735,\n",
       "  'sim_soft': 0.22591357,\n",
       "  'pred_max_max': 20,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2435954362154007,\n",
       "  'sim_max_max': 0.26472062},\n",
       " 6052: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1210,\n",
       "  'pred_knn': 4117,\n",
       "  'sim_real_cl': 0.2416254,\n",
       "  'sim_best_knn': 0.33434874,\n",
       "  'pred_soft': 2911,\n",
       "  'soft_val': 0.2733076512813568,\n",
       "  'sim_soft': 0.2894264,\n",
       "  'pred_max_max': 3856,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26272279024124146,\n",
       "  'sim_max_max': 0.31659257},\n",
       " 6082: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1216,\n",
       "  'pred_knn': 4728,\n",
       "  'sim_real_cl': 0.21856067,\n",
       "  'sim_best_knn': 0.27585113,\n",
       "  'pred_soft': 4728,\n",
       "  'soft_val': 0.2685224115848541,\n",
       "  'sim_soft': 0.27585113,\n",
       "  'pred_max_max': 4728,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27303385734558105,\n",
       "  'sim_max_max': 0.27585113},\n",
       " 6094: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1218,\n",
       "  'pred_knn': 7602,\n",
       "  'sim_real_cl': 0.33391652,\n",
       "  'sim_best_knn': 0.3361743,\n",
       "  'pred_soft': 1218,\n",
       "  'soft_val': 0.26663899421691895,\n",
       "  'sim_soft': array(0.266639, dtype=float32),\n",
       "  'pred_max_max': 1218,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3373481035232544,\n",
       "  'sim_max_max': 0.3361743},\n",
       " 6173: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1234,\n",
       "  'pred_knn': 3995,\n",
       "  'sim_real_cl': 0.15984324,\n",
       "  'sim_best_knn': 0.2674699,\n",
       "  'pred_soft': 2271,\n",
       "  'soft_val': 0.21966952085494995,\n",
       "  'sim_soft': 0.2071678,\n",
       "  'pred_max_max': 532,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.20834314823150635,\n",
       "  'sim_max_max': 0.21533923},\n",
       " 6192: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1238,\n",
       "  'pred_knn': 1238,\n",
       "  'sim_real_cl': 0.3279441,\n",
       "  'sim_best_knn': 0.3279441,\n",
       "  'pred_soft': 5747,\n",
       "  'soft_val': 0.27406448125839233,\n",
       "  'sim_soft': 0.2749793},\n",
       " 6207: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1241,\n",
       "  'pred_knn': 1241,\n",
       "  'sim_real_cl': 0.39792246,\n",
       "  'sim_best_knn': 0.39792246,\n",
       "  'pred_soft': 6630,\n",
       "  'soft_val': 0.3365078568458557,\n",
       "  'sim_soft': 0.37644967},\n",
       " 6241: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1248,\n",
       "  'pred_knn': 1248,\n",
       "  'sim_real_cl': 0.35142183,\n",
       "  'sim_best_knn': 0.35142183,\n",
       "  'pred_soft': 5911,\n",
       "  'soft_val': 0.22471171617507935,\n",
       "  'sim_soft': 0.2596982,\n",
       "  'pred_max_max': 539,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2431253343820572,\n",
       "  'sim_max_max': 0.3220573},\n",
       " 6253: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1250,\n",
       "  'pred_knn': 2091,\n",
       "  'sim_real_cl': 0.31697655,\n",
       "  'sim_best_knn': 0.31868443,\n",
       "  'pred_soft': 6302,\n",
       "  'soft_val': 0.27779412269592285,\n",
       "  'sim_soft': 0.29153776,\n",
       "  'pred_max_max': 6302,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.29032427072525024,\n",
       "  'sim_max_max': 0.29153776},\n",
       " 6258: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1251,\n",
       "  'pred_knn': 2018,\n",
       "  'sim_real_cl': 0.2496351,\n",
       "  'sim_best_knn': 0.34737396,\n",
       "  'pred_soft': 2018,\n",
       "  'soft_val': 0.25183266401290894,\n",
       "  'sim_soft': 0.34737396,\n",
       "  'pred_max_max': 2018,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24694006145000458,\n",
       "  'sim_max_max': 0.34737396},\n",
       " 6372: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1274,\n",
       "  'pred_knn': 1632,\n",
       "  'sim_real_cl': 0.27685654,\n",
       "  'sim_best_knn': 0.40744388,\n",
       "  'pred_soft': 5083,\n",
       "  'soft_val': 0.2761287987232208,\n",
       "  'sim_soft': 0.33124712,\n",
       "  'pred_max_max': 1632,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2953304350376129,\n",
       "  'sim_max_max': 0.40744388},\n",
       " 6373: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1274,\n",
       "  'pred_knn': 7533,\n",
       "  'sim_real_cl': 0.19822055,\n",
       "  'sim_best_knn': 0.3363231,\n",
       "  'pred_soft': 7533,\n",
       "  'soft_val': 0.2501692473888397,\n",
       "  'sim_soft': 0.3363231,\n",
       "  'pred_max_max': 7533,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28076380491256714,\n",
       "  'sim_max_max': 0.3363231},\n",
       " 6382: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1276,\n",
       "  'pred_knn': 1276,\n",
       "  'sim_real_cl': 0.39254576,\n",
       "  'sim_best_knn': 0.39254576,\n",
       "  'pred_soft': 5411,\n",
       "  'soft_val': 0.2669689953327179,\n",
       "  'sim_soft': 0.28458464},\n",
       " 6384: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1276,\n",
       "  'pred_knn': 8588,\n",
       "  'sim_real_cl': 0.296673,\n",
       "  'sim_best_knn': 0.4366682,\n",
       "  'pred_soft': 8588,\n",
       "  'soft_val': 0.3374708592891693,\n",
       "  'sim_soft': 0.4366682,\n",
       "  'pred_max_max': 8588,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.4009849727153778,\n",
       "  'sim_max_max': 0.4366682},\n",
       " 6394: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1278,\n",
       "  'pred_knn': 1278,\n",
       "  'sim_real_cl': 0.38232592,\n",
       "  'sim_best_knn': 0.38232592,\n",
       "  'pred_soft': 5757,\n",
       "  'soft_val': 0.26522982120513916,\n",
       "  'sim_soft': 0.32468042},\n",
       " 6464: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1292,\n",
       "  'pred_knn': 7222,\n",
       "  'sim_real_cl': 0.17421165,\n",
       "  'sim_best_knn': 0.30743077,\n",
       "  'pred_soft': 5359,\n",
       "  'soft_val': 0.23644590377807617,\n",
       "  'sim_soft': 0.2759769,\n",
       "  'pred_max_max': 7222,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2692916989326477,\n",
       "  'sim_max_max': 0.30743077},\n",
       " 6478: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1295,\n",
       "  'pred_knn': 1295,\n",
       "  'sim_real_cl': 0.31916294,\n",
       "  'sim_best_knn': 0.31916294,\n",
       "  'pred_soft': 8757,\n",
       "  'soft_val': 0.22366464138031006,\n",
       "  'sim_soft': 0.21323937},\n",
       " 6486: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1297,\n",
       "  'pred_knn': 1297,\n",
       "  'sim_real_cl': 0.4303757,\n",
       "  'sim_best_knn': 0.4303757,\n",
       "  'pred_soft': 9504,\n",
       "  'soft_val': 0.3059658110141754,\n",
       "  'sim_soft': 0.33699456},\n",
       " 6603: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1320,\n",
       "  'pred_knn': 2279,\n",
       "  'sim_real_cl': 0.22975382,\n",
       "  'sim_best_knn': 0.29790685,\n",
       "  'pred_soft': 2279,\n",
       "  'soft_val': 0.2653547525405884,\n",
       "  'sim_soft': 0.29790685,\n",
       "  'pred_max_max': 9017,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.253758043050766,\n",
       "  'sim_max_max': 0.2666819},\n",
       " 6689: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1337,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.13012901,\n",
       "  'sim_best_knn': 0.3749656,\n",
       "  'pred_soft': 6799,\n",
       "  'soft_val': 0.24237388372421265,\n",
       "  'sim_soft': 0.31270072,\n",
       "  'pred_max_max': 3577,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26510846614837646,\n",
       "  'sim_max_max': 0.2880531},\n",
       " 6715: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1343,\n",
       "  'pred_knn': 1343,\n",
       "  'sim_real_cl': 0.8371222,\n",
       "  'sim_best_knn': 0.8371222,\n",
       "  'pred_soft': 322,\n",
       "  'soft_val': 0.2565813660621643,\n",
       "  'sim_soft': 0.23427686},\n",
       " 6717: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1343,\n",
       "  'pred_knn': 1343,\n",
       "  'sim_real_cl': 0.5841209,\n",
       "  'sim_best_knn': 0.5841209,\n",
       "  'pred_soft': 6031,\n",
       "  'soft_val': 0.2997785210609436,\n",
       "  'sim_soft': 0.3192636},\n",
       " 6725: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1345,\n",
       "  'pred_knn': 974,\n",
       "  'sim_real_cl': 0.34337765,\n",
       "  'sim_best_knn': 0.35054556,\n",
       "  'pred_soft': 6361,\n",
       "  'soft_val': 0.2632814049720764,\n",
       "  'sim_soft': 0.31372893,\n",
       "  'pred_max_max': 6361,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.291717529296875,\n",
       "  'sim_max_max': 0.31372893},\n",
       " 6727: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1345,\n",
       "  'pred_knn': 639,\n",
       "  'sim_real_cl': 0.14982411,\n",
       "  'sim_best_knn': 0.32833767,\n",
       "  'pred_soft': 1700,\n",
       "  'soft_val': 0.2379039078950882,\n",
       "  'sim_soft': 0.25196093,\n",
       "  'pred_max_max': 8731,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2521984875202179,\n",
       "  'sim_max_max': 0.18157177},\n",
       " 6729: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1345,\n",
       "  'pred_knn': 1345,\n",
       "  'sim_real_cl': 0.41528583,\n",
       "  'sim_best_knn': 0.41528583,\n",
       "  'pred_soft': 2932,\n",
       "  'soft_val': 0.25863945484161377,\n",
       "  'sim_soft': 0.2671618},\n",
       " 6749: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1349,\n",
       "  'pred_knn': 1349,\n",
       "  'sim_real_cl': 0.40026367,\n",
       "  'sim_best_knn': 0.40026367,\n",
       "  'pred_soft': 2325,\n",
       "  'soft_val': 0.283811092376709,\n",
       "  'sim_soft': 0.29445136},\n",
       " 6773: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1354,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.12647867,\n",
       "  'sim_best_knn': 0.26234454,\n",
       "  'pred_soft': 106,\n",
       "  'soft_val': 0.254748672246933,\n",
       "  'sim_soft': 0.22187506,\n",
       "  'pred_max_max': 9047,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2208484560251236,\n",
       "  'sim_max_max': 0.21861798},\n",
       " 6777: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1355,\n",
       "  'pred_knn': 8541,\n",
       "  'sim_real_cl': 0.3389252,\n",
       "  'sim_best_knn': 0.35050377,\n",
       "  'pred_soft': 1861,\n",
       "  'soft_val': 0.30828654766082764,\n",
       "  'sim_soft': 0.3251792,\n",
       "  'pred_max_max': 1861,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.29404038190841675,\n",
       "  'sim_max_max': 0.3251792},\n",
       " 6779: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1355,\n",
       "  'pred_knn': 1355,\n",
       "  'sim_real_cl': 0.34386873,\n",
       "  'sim_best_knn': 0.34386873,\n",
       "  'pred_soft': 1861,\n",
       "  'soft_val': 0.2608727216720581,\n",
       "  'sim_soft': 0.30354106,\n",
       "  'pred_max_max': 1346,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25389397144317627,\n",
       "  'sim_max_max': 0.28037977},\n",
       " 6797: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1359,\n",
       "  'pred_knn': 7540,\n",
       "  'sim_real_cl': 0.32344273,\n",
       "  'sim_best_knn': 0.38446534,\n",
       "  'pred_soft': 7540,\n",
       "  'soft_val': 0.3396104574203491,\n",
       "  'sim_soft': 0.38446534,\n",
       "  'pred_max_max': 7540,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.35150283575057983,\n",
       "  'sim_max_max': 0.38446534},\n",
       " 6804: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1360,\n",
       "  'pred_knn': 395,\n",
       "  'sim_real_cl': 0.28978065,\n",
       "  'sim_best_knn': 0.34908336,\n",
       "  'pred_soft': 1360,\n",
       "  'soft_val': 0.3004978597164154,\n",
       "  'sim_soft': array(0.30049786, dtype=float32),\n",
       "  'pred_max_max': 1360,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3360685110092163,\n",
       "  'sim_max_max': 0.34908336},\n",
       " 6819: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1363,\n",
       "  'pred_knn': 9794,\n",
       "  'sim_real_cl': 0.2531302,\n",
       "  'sim_best_knn': 0.34428868,\n",
       "  'pred_soft': 9110,\n",
       "  'soft_val': 0.2808225154876709,\n",
       "  'sim_soft': 0.3259037,\n",
       "  'pred_max_max': 9110,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2459706962108612,\n",
       "  'sim_max_max': 0.3259037},\n",
       " 6852: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1370,\n",
       "  'pred_knn': 1370,\n",
       "  'sim_real_cl': 0.35579824,\n",
       "  'sim_best_knn': 0.35579824,\n",
       "  'pred_soft': 1357,\n",
       "  'soft_val': 0.2561705708503723,\n",
       "  'sim_soft': 0.3442021,\n",
       "  'pred_max_max': 1357,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2803400158882141,\n",
       "  'sim_max_max': 0.3442021},\n",
       " 6906: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1381,\n",
       "  'pred_knn': 8185,\n",
       "  'sim_real_cl': 0.16945094,\n",
       "  'sim_best_knn': 0.28839892,\n",
       "  'pred_soft': 8185,\n",
       "  'soft_val': 0.24197709560394287,\n",
       "  'sim_soft': 0.28839892,\n",
       "  'pred_max_max': 8185,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28600916266441345,\n",
       "  'sim_max_max': 0.28839892},\n",
       " 6926: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1385,\n",
       "  'pred_knn': 2333,\n",
       "  'sim_real_cl': 0.24290575,\n",
       "  'sim_best_knn': 0.36234203,\n",
       "  'pred_soft': 2333,\n",
       "  'soft_val': 0.25839993357658386,\n",
       "  'sim_soft': 0.36234203,\n",
       "  'pred_max_max': 3881,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3157225549221039,\n",
       "  'sim_max_max': 0.30433917},\n",
       " 6953: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1390,\n",
       "  'pred_knn': 6022,\n",
       "  'sim_real_cl': 0.3351859,\n",
       "  'sim_best_knn': 0.35843328,\n",
       "  'pred_soft': 1390,\n",
       "  'soft_val': 0.28624722361564636,\n",
       "  'sim_soft': array(0.28624722, dtype=float32),\n",
       "  'pred_max_max': 6022,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28717470169067383,\n",
       "  'sim_max_max': 0.35843328},\n",
       " 6971: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1394,\n",
       "  'pred_knn': 1394,\n",
       "  'sim_real_cl': 0.48900333,\n",
       "  'sim_best_knn': 0.48900333,\n",
       "  'pred_soft': 2316,\n",
       "  'soft_val': 0.3622901737689972,\n",
       "  'sim_soft': 0.4065346},\n",
       " 6993: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1398,\n",
       "  'pred_knn': 1398,\n",
       "  'sim_real_cl': 0.4251233,\n",
       "  'sim_best_knn': 0.4251233,\n",
       "  'pred_soft': 667,\n",
       "  'soft_val': 0.29635030031204224,\n",
       "  'sim_soft': 0.31920004,\n",
       "  'pred_max_max': 667,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.288463830947876,\n",
       "  'sim_max_max': 0.31920004},\n",
       " 6994: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1398,\n",
       "  'pred_knn': 7114,\n",
       "  'sim_real_cl': 0.15740275,\n",
       "  'sim_best_knn': 0.2656393,\n",
       "  'pred_soft': 2124,\n",
       "  'soft_val': 0.23101671040058136,\n",
       "  'sim_soft': 0.2603269,\n",
       "  'pred_max_max': 2124,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24621212482452393,\n",
       "  'sim_max_max': 0.2603269},\n",
       " 6998: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1399,\n",
       "  'pred_knn': 3625,\n",
       "  'sim_real_cl': 0.3093766,\n",
       "  'sim_best_knn': 0.31763458,\n",
       "  'pred_soft': 6366,\n",
       "  'soft_val': 0.20783928036689758,\n",
       "  'sim_soft': 0.25754738,\n",
       "  'pred_max_max': 1399,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.30695223808288574,\n",
       "  'sim_max_max': 0.31763458},\n",
       " 7009: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1401,\n",
       "  'pred_knn': 5817,\n",
       "  'sim_real_cl': 0.12710658,\n",
       "  'sim_best_knn': 0.35182926,\n",
       "  'pred_soft': 5817,\n",
       "  'soft_val': 0.25172725319862366,\n",
       "  'sim_soft': 0.35182926,\n",
       "  'pred_max_max': 101,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30331185460090637,\n",
       "  'sim_max_max': 0.32730988},\n",
       " 7039: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1407,\n",
       "  'pred_knn': 8561,\n",
       "  'sim_real_cl': 0.2600562,\n",
       "  'sim_best_knn': 0.31902844,\n",
       "  'pred_soft': 9565,\n",
       "  'soft_val': 0.24502705037593842,\n",
       "  'sim_soft': 0.2384412,\n",
       "  'pred_max_max': 8561,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2517150342464447,\n",
       "  'sim_max_max': 0.31902844},\n",
       " 7040: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1408,\n",
       "  'pred_knn': 6488,\n",
       "  'sim_real_cl': 0.360188,\n",
       "  'sim_best_knn': 0.38045222,\n",
       "  'pred_soft': 1408,\n",
       "  'soft_val': 0.35028114914894104,\n",
       "  'sim_soft': array(0.35028115, dtype=float32),\n",
       "  'pred_max_max': 1408,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.40411582589149475,\n",
       "  'sim_max_max': 0.38045222},\n",
       " 7056: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1411,\n",
       "  'pred_knn': 1411,\n",
       "  'sim_real_cl': 0.54932255,\n",
       "  'sim_best_knn': 0.54932255,\n",
       "  'pred_soft': 6839,\n",
       "  'soft_val': 0.24091418087482452,\n",
       "  'sim_soft': 0.27286452},\n",
       " 7075: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1415,\n",
       "  'pred_knn': 1415,\n",
       "  'sim_real_cl': 0.33595356,\n",
       "  'sim_best_knn': 0.33595356,\n",
       "  'pred_soft': 4976,\n",
       "  'soft_val': 0.23896817862987518,\n",
       "  'sim_soft': 0.23236516},\n",
       " 7079: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1415,\n",
       "  'pred_knn': 4976,\n",
       "  'sim_real_cl': 0.2484807,\n",
       "  'sim_best_knn': 0.28057295,\n",
       "  'pred_soft': 4976,\n",
       "  'soft_val': 0.2855369448661804,\n",
       "  'sim_soft': 0.28057295,\n",
       "  'pred_max_max': 1415,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.34646347165107727,\n",
       "  'sim_max_max': 0.28057295},\n",
       " 7080: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1416,\n",
       "  'pred_knn': 1416,\n",
       "  'sim_real_cl': 0.42639172,\n",
       "  'sim_best_knn': 0.42639172,\n",
       "  'pred_soft': 5796,\n",
       "  'soft_val': 0.3191908597946167,\n",
       "  'sim_soft': 0.3788429},\n",
       " 7093: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1418,\n",
       "  'pred_knn': 1418,\n",
       "  'sim_real_cl': 0.39075303,\n",
       "  'sim_best_knn': 0.39075303,\n",
       "  'pred_soft': 7808,\n",
       "  'soft_val': 0.3049750030040741,\n",
       "  'sim_soft': 0.35060614},\n",
       " 7094: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1418,\n",
       "  'pred_knn': 1418,\n",
       "  'sim_real_cl': 0.31114167,\n",
       "  'sim_best_knn': 0.31114167,\n",
       "  'pred_soft': 5883,\n",
       "  'soft_val': 0.26783397793769836,\n",
       "  'sim_soft': 0.26718938,\n",
       "  'pred_max_max': 3399,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2713727355003357,\n",
       "  'sim_max_max': 0.2990048},\n",
       " 7104: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1420,\n",
       "  'pred_knn': 1420,\n",
       "  'sim_real_cl': 0.2544822,\n",
       "  'sim_best_knn': 0.2544822,\n",
       "  'pred_soft': 5734,\n",
       "  'soft_val': 0.242184579372406,\n",
       "  'sim_soft': 0.2331365,\n",
       "  'pred_max_max': 5734,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26251164078712463,\n",
       "  'sim_max_max': 0.2331365},\n",
       " 7109: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1421,\n",
       "  'pred_knn': 5982,\n",
       "  'sim_real_cl': 0.30311984,\n",
       "  'sim_best_knn': 0.3416008,\n",
       "  'pred_soft': 5156,\n",
       "  'soft_val': 0.27737557888031006,\n",
       "  'sim_soft': 0.28082776,\n",
       "  'pred_max_max': 2793,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27588945627212524,\n",
       "  'sim_max_max': 0.24532928},\n",
       " 7163: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1432,\n",
       "  'pred_knn': 1432,\n",
       "  'sim_real_cl': 0.39364067,\n",
       "  'sim_best_knn': 0.39364067,\n",
       "  'pred_soft': 1020,\n",
       "  'soft_val': 0.30853497982025146,\n",
       "  'sim_soft': 0.347503},\n",
       " 7188: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1437,\n",
       "  'pred_knn': 1437,\n",
       "  'sim_real_cl': 0.47737247,\n",
       "  'sim_best_knn': 0.47737247,\n",
       "  'pred_soft': 7164,\n",
       "  'soft_val': 0.3589569926261902,\n",
       "  'sim_soft': 0.3555043},\n",
       " 7248: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1449,\n",
       "  'pred_knn': 1449,\n",
       "  'sim_real_cl': 0.3858334,\n",
       "  'sim_best_knn': 0.3858334,\n",
       "  'pred_soft': 980,\n",
       "  'soft_val': 0.29568785429000854,\n",
       "  'sim_soft': 0.29801214},\n",
       " 7260: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1452,\n",
       "  'pred_knn': 1452,\n",
       "  'sim_real_cl': 0.5484719,\n",
       "  'sim_best_knn': 0.5484719,\n",
       "  'pred_soft': 7141,\n",
       "  'soft_val': 0.2687152326107025,\n",
       "  'sim_soft': 0.29722863},\n",
       " 7367: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1473,\n",
       "  'pred_knn': 6501,\n",
       "  'sim_real_cl': 0.31904316,\n",
       "  'sim_best_knn': 0.33399168,\n",
       "  'pred_soft': 2863,\n",
       "  'soft_val': 0.2998213768005371,\n",
       "  'sim_soft': 0.27400666,\n",
       "  'pred_max_max': 2863,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.32010728120803833,\n",
       "  'sim_max_max': 0.27400666},\n",
       " 7368: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1473,\n",
       "  'pred_knn': 2974,\n",
       "  'sim_real_cl': 0.27050987,\n",
       "  'sim_best_knn': 0.30425704,\n",
       "  'pred_soft': 2877,\n",
       "  'soft_val': 0.28044459223747253,\n",
       "  'sim_soft': 0.29542053,\n",
       "  'pred_max_max': 1473,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.2733875811100006,\n",
       "  'sim_max_max': 0.30425704},\n",
       " 7397: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1479,\n",
       "  'pred_knn': 6479,\n",
       "  'sim_real_cl': 0.2717136,\n",
       "  'sim_best_knn': 0.31776196,\n",
       "  'pred_soft': 6479,\n",
       "  'soft_val': 0.2835071086883545,\n",
       "  'sim_soft': 0.31776196,\n",
       "  'pred_max_max': 6479,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3334353268146515,\n",
       "  'sim_max_max': 0.31776196},\n",
       " 7409: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1481,\n",
       "  'pred_knn': 1481,\n",
       "  'sim_real_cl': 0.48399597,\n",
       "  'sim_best_knn': 0.48399597,\n",
       "  'pred_soft': 7880,\n",
       "  'soft_val': 0.3278029263019562,\n",
       "  'sim_soft': 0.4003235,\n",
       "  'pred_max_max': 352,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.32366490364074707,\n",
       "  'sim_max_max': 0.34592313},\n",
       " 7417: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1483,\n",
       "  'pred_knn': 1483,\n",
       "  'sim_real_cl': 0.36543366,\n",
       "  'sim_best_knn': 0.36543366,\n",
       "  'pred_soft': 873,\n",
       "  'soft_val': 0.2624388039112091,\n",
       "  'sim_soft': 0.2647109},\n",
       " 7522: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1504,\n",
       "  'pred_knn': 5695,\n",
       "  'sim_real_cl': 0.20826057,\n",
       "  'sim_best_knn': 0.2883755,\n",
       "  'pred_soft': 3820,\n",
       "  'soft_val': 0.26247429847717285,\n",
       "  'sim_soft': 0.24398717,\n",
       "  'pred_max_max': 4865,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2371465265750885,\n",
       "  'sim_max_max': 0.27189776},\n",
       " 7532: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1506,\n",
       "  'pred_knn': 1506,\n",
       "  'sim_real_cl': 0.35023257,\n",
       "  'sim_best_knn': 0.35023257,\n",
       "  'pred_soft': 2585,\n",
       "  'soft_val': 0.232106551527977,\n",
       "  'sim_soft': 0.32638162,\n",
       "  'pred_max_max': 1445,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2297656238079071,\n",
       "  'sim_max_max': 0.22268502},\n",
       " 7547: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1509,\n",
       "  'pred_knn': 1509,\n",
       "  'sim_real_cl': 0.48847902,\n",
       "  'sim_best_knn': 0.48847902,\n",
       "  'pred_soft': 1073,\n",
       "  'soft_val': 0.3399766683578491,\n",
       "  'sim_soft': 0.39256808},\n",
       " 7551: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1510,\n",
       "  'pred_knn': 1510,\n",
       "  'sim_real_cl': 0.64263403,\n",
       "  'sim_best_knn': 0.64263403,\n",
       "  'pred_soft': 2696,\n",
       "  'soft_val': 0.2893798351287842,\n",
       "  'sim_soft': 0.28256682},\n",
       " 7558: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1511,\n",
       "  'pred_knn': 1511,\n",
       "  'sim_real_cl': 0.36329693,\n",
       "  'sim_best_knn': 0.36329693,\n",
       "  'pred_soft': 7392,\n",
       "  'soft_val': 0.2648119330406189,\n",
       "  'sim_soft': 0.34355378,\n",
       "  'pred_max_max': 587,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27734553813934326,\n",
       "  'sim_max_max': 0.3202541},\n",
       " 7578: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1515,\n",
       "  'pred_knn': 713,\n",
       "  'sim_real_cl': 0.29128623,\n",
       "  'sim_best_knn': 0.3047726,\n",
       "  'pred_soft': 2059,\n",
       "  'soft_val': 0.28927043080329895,\n",
       "  'sim_soft': 0.27931714,\n",
       "  'pred_max_max': 2059,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2901631295681,\n",
       "  'sim_max_max': 0.27931714},\n",
       " 7583: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1516,\n",
       "  'pred_knn': 8306,\n",
       "  'sim_real_cl': 0.37783653,\n",
       "  'sim_best_knn': 0.3919869,\n",
       "  'pred_soft': 1516,\n",
       "  'soft_val': 0.3149055540561676,\n",
       "  'sim_soft': array(0.31490555, dtype=float32),\n",
       "  'pred_max_max': 8306,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.33715906739234924,\n",
       "  'sim_max_max': 0.3919869},\n",
       " 7611: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1522,\n",
       "  'pred_knn': 1522,\n",
       "  'sim_real_cl': 0.34876063,\n",
       "  'sim_best_knn': 0.34876063,\n",
       "  'pred_soft': 9838,\n",
       "  'soft_val': 0.2561261057853699,\n",
       "  'sim_soft': 0.24671248,\n",
       "  'pred_max_max': 1239,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27624890208244324,\n",
       "  'sim_max_max': 0.2822835},\n",
       " 7660: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1532,\n",
       "  'pred_knn': 1532,\n",
       "  'sim_real_cl': 0.45350924,\n",
       "  'sim_best_knn': 0.45350924,\n",
       "  'pred_soft': 6356,\n",
       "  'soft_val': 0.30244937539100647,\n",
       "  'sim_soft': 0.27580845},\n",
       " 7661: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1532,\n",
       "  'pred_knn': 2856,\n",
       "  'sim_real_cl': 0.2498357,\n",
       "  'sim_best_knn': 0.30306393,\n",
       "  'pred_soft': 8384,\n",
       "  'soft_val': 0.23156118392944336,\n",
       "  'sim_soft': 0.23486897,\n",
       "  'pred_max_max': 7120,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2550053000450134,\n",
       "  'sim_max_max': 0.27936578},\n",
       " 7664: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1532,\n",
       "  'pred_knn': 7899,\n",
       "  'sim_real_cl': 0.086918615,\n",
       "  'sim_best_knn': 0.29869017,\n",
       "  'pred_soft': 7989,\n",
       "  'soft_val': 0.27664247155189514,\n",
       "  'sim_soft': 0.27697474,\n",
       "  'pred_max_max': 7989,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24985048174858093,\n",
       "  'sim_max_max': 0.27697474},\n",
       " 7679: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1535,\n",
       "  'pred_knn': 2540,\n",
       "  'sim_real_cl': 0.24470645,\n",
       "  'sim_best_knn': 0.34172702,\n",
       "  'pred_soft': 4406,\n",
       "  'soft_val': 0.2524193227291107,\n",
       "  'sim_soft': 0.32741898,\n",
       "  'pred_max_max': 4406,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2710196077823639,\n",
       "  'sim_max_max': 0.32741898},\n",
       " 7770: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1554,\n",
       "  'pred_knn': 9248,\n",
       "  'sim_real_cl': 0.20597276,\n",
       "  'sim_best_knn': 0.24655226,\n",
       "  'pred_soft': 9711,\n",
       "  'soft_val': 0.21390682458877563,\n",
       "  'sim_soft': 0.20415048,\n",
       "  'pred_max_max': 4359,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24966400861740112,\n",
       "  'sim_max_max': 0.23495914},\n",
       " 7849: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1569,\n",
       "  'pred_knn': 2333,\n",
       "  'sim_real_cl': 0.30824777,\n",
       "  'sim_best_knn': 0.361108,\n",
       "  'pred_soft': 9222,\n",
       "  'soft_val': 0.24752306938171387,\n",
       "  'sim_soft': 0.24442293,\n",
       "  'pred_max_max': 1569,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.31402575969696045,\n",
       "  'sim_max_max': 0.361108},\n",
       " 7912: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1582,\n",
       "  'pred_knn': 1582,\n",
       "  'sim_real_cl': 0.41116992,\n",
       "  'sim_best_knn': 0.41116992,\n",
       "  'pred_soft': 8469,\n",
       "  'soft_val': 0.3054361045360565,\n",
       "  'sim_soft': 0.37812737,\n",
       "  'pred_max_max': 8469,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.31787964701652527,\n",
       "  'sim_max_max': 0.37812737},\n",
       " 7914: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1582,\n",
       "  'pred_knn': 1582,\n",
       "  'sim_real_cl': 0.4107648,\n",
       "  'sim_best_knn': 0.4107648,\n",
       "  'pred_soft': 986,\n",
       "  'soft_val': 0.2590506076812744,\n",
       "  'sim_soft': 0.22877902,\n",
       "  'pred_max_max': 870,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2694314122200012,\n",
       "  'sim_max_max': 0.29576147},\n",
       " 7948: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1589,\n",
       "  'pred_knn': 2507,\n",
       "  'sim_real_cl': 0.21281692,\n",
       "  'sim_best_knn': 0.3167851,\n",
       "  'pred_soft': 2507,\n",
       "  'soft_val': 0.2672533094882965,\n",
       "  'sim_soft': 0.3167851,\n",
       "  'pred_max_max': 2507,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2640902101993561,\n",
       "  'sim_max_max': 0.3167851},\n",
       " 7977: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1595,\n",
       "  'pred_knn': 7036,\n",
       "  'sim_real_cl': 0.34457627,\n",
       "  'sim_best_knn': 0.34802634,\n",
       "  'pred_soft': 1595,\n",
       "  'soft_val': 0.2865414023399353,\n",
       "  'sim_soft': array(0.2865414, dtype=float32),\n",
       "  'pred_max_max': 7036,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2801820635795593,\n",
       "  'sim_max_max': 0.34802634},\n",
       " 7986: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1597,\n",
       "  'pred_knn': 1597,\n",
       "  'sim_real_cl': 0.50026643,\n",
       "  'sim_best_knn': 0.50026643,\n",
       "  'pred_soft': 967,\n",
       "  'soft_val': 0.2771869897842407,\n",
       "  'sim_soft': 0.31576264},\n",
       " 7987: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1597,\n",
       "  'pred_knn': 1597,\n",
       "  'sim_real_cl': 0.4481678,\n",
       "  'sim_best_knn': 0.4481678,\n",
       "  'pred_soft': 4821,\n",
       "  'soft_val': 0.294720858335495,\n",
       "  'sim_soft': 0.29784372},\n",
       " 7989: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1597,\n",
       "  'pred_knn': 1597,\n",
       "  'sim_real_cl': 0.45642638,\n",
       "  'sim_best_knn': 0.45642638,\n",
       "  'pred_soft': 967,\n",
       "  'soft_val': 0.344291627407074,\n",
       "  'sim_soft': 0.3816577,\n",
       "  'pred_max_max': 967,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3462446331977844,\n",
       "  'sim_max_max': 0.3816577},\n",
       " 8072: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1614,\n",
       "  'pred_knn': 1614,\n",
       "  'sim_real_cl': 0.3519879,\n",
       "  'sim_best_knn': 0.3519879,\n",
       "  'pred_soft': 4958,\n",
       "  'soft_val': 0.25714394450187683,\n",
       "  'sim_soft': 0.27269647},\n",
       " 8073: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1614,\n",
       "  'pred_knn': 9035,\n",
       "  'sim_real_cl': 0.20245342,\n",
       "  'sim_best_knn': 0.32951587,\n",
       "  'pred_soft': 9035,\n",
       "  'soft_val': 0.25365984439849854,\n",
       "  'sim_soft': 0.32951587,\n",
       "  'pred_max_max': 9035,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2719985246658325,\n",
       "  'sim_max_max': 0.32951587},\n",
       " 8206: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1641,\n",
       "  'pred_knn': 1641,\n",
       "  'sim_real_cl': 0.39019915,\n",
       "  'sim_best_knn': 0.39019915,\n",
       "  'pred_soft': 1847,\n",
       "  'soft_val': 0.3346211314201355,\n",
       "  'sim_soft': 0.34615004},\n",
       " 8251: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1650,\n",
       "  'pred_knn': 1650,\n",
       "  'sim_real_cl': 0.41213298,\n",
       "  'sim_best_knn': 0.41213298,\n",
       "  'pred_soft': 7530,\n",
       "  'soft_val': 0.31766214966773987,\n",
       "  'sim_soft': 0.365296,\n",
       "  'pred_max_max': 7530,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.29980260133743286,\n",
       "  'sim_max_max': 0.365296},\n",
       " 8278: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1655,\n",
       "  'pred_knn': 9731,\n",
       "  'sim_real_cl': 0.28176022,\n",
       "  'sim_best_knn': 0.2833731,\n",
       "  'pred_soft': 1655,\n",
       "  'soft_val': 0.3689998984336853,\n",
       "  'sim_soft': array(0.3689999, dtype=float32),\n",
       "  'pred_max_max': 1655,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3201284110546112,\n",
       "  'sim_max_max': 0.2833731},\n",
       " 8299: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1659,\n",
       "  'pred_knn': 1659,\n",
       "  'sim_real_cl': 0.40068975,\n",
       "  'sim_best_knn': 0.40068975,\n",
       "  'pred_soft': 2666,\n",
       "  'soft_val': 0.31213048100471497,\n",
       "  'sim_soft': 0.34692448,\n",
       "  'pred_max_max': 2666,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28966251015663147,\n",
       "  'sim_max_max': 0.34692448},\n",
       " 8308: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1661,\n",
       "  'pred_knn': 1234,\n",
       "  'sim_real_cl': 0.06690055,\n",
       "  'sim_best_knn': 0.22145243,\n",
       "  'pred_soft': 1234,\n",
       "  'soft_val': 0.21689021587371826,\n",
       "  'sim_soft': 0.22145243,\n",
       "  'pred_max_max': 1234,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2531546950340271,\n",
       "  'sim_max_max': 0.22145243},\n",
       " 8311: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1662,\n",
       "  'pred_knn': 1662,\n",
       "  'sim_real_cl': 0.5485502,\n",
       "  'sim_best_knn': 0.5485502,\n",
       "  'pred_soft': 242,\n",
       "  'soft_val': 0.40793320536613464,\n",
       "  'sim_soft': 0.42132664},\n",
       " 8313: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1662,\n",
       "  'pred_knn': 1662,\n",
       "  'sim_real_cl': 0.6046735,\n",
       "  'sim_best_knn': 0.6046735,\n",
       "  'pred_soft': 242,\n",
       "  'soft_val': 0.43551158905029297,\n",
       "  'sim_soft': 0.4526949},\n",
       " 8314: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1662,\n",
       "  'pred_knn': 1573,\n",
       "  'sim_real_cl': 0.30250555,\n",
       "  'sim_best_knn': 0.32449692,\n",
       "  'pred_soft': 8671,\n",
       "  'soft_val': 0.2606857419013977,\n",
       "  'sim_soft': 0.30822688,\n",
       "  'pred_max_max': 1573,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27682679891586304,\n",
       "  'sim_max_max': 0.32449692},\n",
       " 8365: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1673,\n",
       "  'pred_knn': 1673,\n",
       "  'sim_real_cl': 0.47111934,\n",
       "  'sim_best_knn': 0.47111934,\n",
       "  'pred_soft': 5739,\n",
       "  'soft_val': 0.3027461767196655,\n",
       "  'sim_soft': 0.34025955},\n",
       " 8367: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1673,\n",
       "  'pred_knn': 1673,\n",
       "  'sim_real_cl': 0.32541448,\n",
       "  'sim_best_knn': 0.32541448,\n",
       "  'pred_soft': 4814,\n",
       "  'soft_val': 0.26324132084846497,\n",
       "  'sim_soft': 0.3005273},\n",
       " 8369: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1673,\n",
       "  'pred_knn': 1673,\n",
       "  'sim_real_cl': 0.4067713,\n",
       "  'sim_best_knn': 0.4067713,\n",
       "  'pred_soft': 5739,\n",
       "  'soft_val': 0.30173248052597046,\n",
       "  'sim_soft': 0.3263717,\n",
       "  'pred_max_max': 5739,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2850642800331116,\n",
       "  'sim_max_max': 0.3263717},\n",
       " 8380: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1676,\n",
       "  'pred_knn': 1676,\n",
       "  'sim_real_cl': 0.34411576,\n",
       "  'sim_best_knn': 0.34411576,\n",
       "  'pred_soft': 2153,\n",
       "  'soft_val': 0.24512578547000885,\n",
       "  'sim_soft': 0.24322519},\n",
       " 8406: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1681,\n",
       "  'pred_knn': 5026,\n",
       "  'sim_real_cl': 0.19461358,\n",
       "  'sim_best_knn': 0.25198498,\n",
       "  'pred_soft': 7956,\n",
       "  'soft_val': 0.24176223576068878,\n",
       "  'sim_soft': 0.21423072,\n",
       "  'pred_max_max': 1501,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2500290870666504,\n",
       "  'sim_max_max': 0.22700375},\n",
       " 8531: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1706,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.36031014,\n",
       "  'sim_best_knn': 0.42510986,\n",
       "  'pred_soft': 5356,\n",
       "  'soft_val': 0.24145202338695526,\n",
       "  'sim_soft': 0.29177484,\n",
       "  'pred_max_max': 1706,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.28549885749816895,\n",
       "  'sim_max_max': 0.42510986},\n",
       " 8587: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1717,\n",
       "  'pred_knn': 5620,\n",
       "  'sim_real_cl': 0.2621104,\n",
       "  'sim_best_knn': 0.32288414,\n",
       "  'pred_soft': 5620,\n",
       "  'soft_val': 0.303654283285141,\n",
       "  'sim_soft': 0.32288414,\n",
       "  'pred_max_max': 5938,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.290054589509964,\n",
       "  'sim_max_max': 0.29435647},\n",
       " 8604: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1720,\n",
       "  'pred_knn': 1534,\n",
       "  'sim_real_cl': 0.09117855,\n",
       "  'sim_best_knn': 0.26677313,\n",
       "  'pred_soft': 3931,\n",
       "  'soft_val': 0.22781187295913696,\n",
       "  'sim_soft': 0.23697719,\n",
       "  'pred_max_max': 3409,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.22552905976772308,\n",
       "  'sim_max_max': 0.25760606},\n",
       " 8607: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1721,\n",
       "  'pred_knn': 3497,\n",
       "  'sim_real_cl': 0.29102117,\n",
       "  'sim_best_knn': 0.3805738,\n",
       "  'pred_soft': 3497,\n",
       "  'soft_val': 0.33230510354042053,\n",
       "  'sim_soft': 0.3805738,\n",
       "  'pred_max_max': 8234,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3298388719558716,\n",
       "  'sim_max_max': 0.32625878},\n",
       " 8628: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1725,\n",
       "  'pred_knn': 4007,\n",
       "  'sim_real_cl': 0.3877665,\n",
       "  'sim_best_knn': 0.3971883,\n",
       "  'pred_soft': 1725,\n",
       "  'soft_val': 0.32048913836479187,\n",
       "  'sim_soft': array(0.32048914, dtype=float32),\n",
       "  'pred_max_max': 4007,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3708055019378662,\n",
       "  'sim_max_max': 0.3971883},\n",
       " 8651: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1730,\n",
       "  'pred_knn': 8583,\n",
       "  'sim_real_cl': 0.3078926,\n",
       "  'sim_best_knn': 0.3338194,\n",
       "  'pred_soft': 1690,\n",
       "  'soft_val': 0.24905376136302948,\n",
       "  'sim_soft': 0.25892195,\n",
       "  'pred_max_max': 8290,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24628899991512299,\n",
       "  'sim_max_max': 0.24240656},\n",
       " 8654: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1730,\n",
       "  'pred_knn': 1730,\n",
       "  'sim_real_cl': 0.38744003,\n",
       "  'sim_best_knn': 0.38744003,\n",
       "  'pred_soft': 8920,\n",
       "  'soft_val': 0.2798868715763092,\n",
       "  'sim_soft': 0.32030046},\n",
       " 8784: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1756,\n",
       "  'pred_knn': 3485,\n",
       "  'sim_real_cl': 0.2651633,\n",
       "  'sim_best_knn': 0.2755815,\n",
       "  'pred_soft': 3485,\n",
       "  'soft_val': 0.258571058511734,\n",
       "  'sim_soft': 0.2755815,\n",
       "  'pred_max_max': 2199,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25335919857025146,\n",
       "  'sim_max_max': 0.25730222},\n",
       " 8803: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1760,\n",
       "  'pred_knn': 1760,\n",
       "  'sim_real_cl': 0.5257171,\n",
       "  'sim_best_knn': 0.5257171,\n",
       "  'pred_soft': 8354,\n",
       "  'soft_val': 0.26557403802871704,\n",
       "  'sim_soft': 0.3297124},\n",
       " 8844: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1768,\n",
       "  'pred_knn': 7492,\n",
       "  'sim_real_cl': 0.09937758,\n",
       "  'sim_best_knn': 0.26044485,\n",
       "  'pred_soft': 5719,\n",
       "  'soft_val': 0.20939992368221283,\n",
       "  'sim_soft': 0.21419919,\n",
       "  'pred_max_max': 4117,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.22263965010643005,\n",
       "  'sim_max_max': 0.25551307},\n",
       " 8872: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1774,\n",
       "  'pred_knn': 1774,\n",
       "  'sim_real_cl': 0.38049698,\n",
       "  'sim_best_knn': 0.38049698,\n",
       "  'pred_soft': 1461,\n",
       "  'soft_val': 0.2718145251274109,\n",
       "  'sim_soft': 0.3062418,\n",
       "  'pred_max_max': 7926,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2776693105697632,\n",
       "  'sim_max_max': 0.28603753},\n",
       " 8909: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1781,\n",
       "  'pred_knn': 156,\n",
       "  'sim_real_cl': 0.12511593,\n",
       "  'sim_best_knn': 0.31557208,\n",
       "  'pred_soft': 4964,\n",
       "  'soft_val': 0.23397526144981384,\n",
       "  'sim_soft': 0.22233236,\n",
       "  'pred_max_max': 4674,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2249433398246765,\n",
       "  'sim_max_max': 0.25256342},\n",
       " 8933: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1786,\n",
       "  'pred_knn': 1786,\n",
       "  'sim_real_cl': 0.30897215,\n",
       "  'sim_best_knn': 0.30897215,\n",
       "  'pred_soft': 3606,\n",
       "  'soft_val': 0.25584688782691956,\n",
       "  'sim_soft': 0.27285,\n",
       "  'pred_max_max': 3606,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.31533750891685486,\n",
       "  'sim_max_max': 0.27285},\n",
       " 8974: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1794,\n",
       "  'pred_knn': 2593,\n",
       "  'sim_real_cl': 0.29133102,\n",
       "  'sim_best_knn': 0.4602341,\n",
       "  'pred_soft': 2593,\n",
       "  'soft_val': 0.2844982147216797,\n",
       "  'sim_soft': 0.4602341,\n",
       "  'pred_max_max': 9749,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3387627601623535,\n",
       "  'sim_max_max': 0.3116881},\n",
       " 9042: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1808,\n",
       "  'pred_knn': 1808,\n",
       "  'sim_real_cl': 0.45993495,\n",
       "  'sim_best_knn': 0.45993495,\n",
       "  'pred_soft': 8790,\n",
       "  'soft_val': 0.33469730615615845,\n",
       "  'sim_soft': 0.34972182},\n",
       " 9051: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1810,\n",
       "  'pred_knn': 1810,\n",
       "  'sim_real_cl': 0.26224804,\n",
       "  'sim_best_knn': 0.26224804,\n",
       "  'pred_soft': 8111,\n",
       "  'soft_val': 0.24575234949588776,\n",
       "  'sim_soft': 0.24418096,\n",
       "  'pred_max_max': 8111,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25691360235214233,\n",
       "  'sim_max_max': 0.24418096},\n",
       " 9058: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1811,\n",
       "  'pred_knn': 5536,\n",
       "  'sim_real_cl': 0.31883454,\n",
       "  'sim_best_knn': 0.38829404,\n",
       "  'pred_soft': 5536,\n",
       "  'soft_val': 0.3415852189064026,\n",
       "  'sim_soft': 0.38829404,\n",
       "  'pred_max_max': 7331,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.37137705087661743,\n",
       "  'sim_max_max': 0.3822407},\n",
       " 9149: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1829,\n",
       "  'pred_knn': 1829,\n",
       "  'sim_real_cl': 0.35666817,\n",
       "  'sim_best_knn': 0.35666817,\n",
       "  'pred_soft': 8835,\n",
       "  'soft_val': 0.28892314434051514,\n",
       "  'sim_soft': 0.25322,\n",
       "  'pred_max_max': 9212,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30262675881385803,\n",
       "  'sim_max_max': 0.30791527},\n",
       " 9220: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1844,\n",
       "  'pred_knn': 2396,\n",
       "  'sim_real_cl': 0.3342549,\n",
       "  'sim_best_knn': 0.34432387,\n",
       "  'pred_soft': 1844,\n",
       "  'soft_val': 0.3371971547603607,\n",
       "  'sim_soft': array(0.33719715, dtype=float32),\n",
       "  'pred_max_max': 1844,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3663446307182312,\n",
       "  'sim_max_max': 0.34432387},\n",
       " 9320: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1864,\n",
       "  'pred_knn': 7528,\n",
       "  'sim_real_cl': 0.23466992,\n",
       "  'sim_best_knn': 0.3401752,\n",
       "  'pred_soft': 1700,\n",
       "  'soft_val': 0.23069940507411957,\n",
       "  'sim_soft': 0.283866,\n",
       "  'pred_max_max': 1700,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2260333001613617,\n",
       "  'sim_max_max': 0.283866},\n",
       " 9353: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1870,\n",
       "  'pred_knn': 2827,\n",
       "  'sim_real_cl': 0.27631807,\n",
       "  'sim_best_knn': 0.2996489,\n",
       "  'pred_soft': 2823,\n",
       "  'soft_val': 0.26105767488479614,\n",
       "  'sim_soft': 0.27704006,\n",
       "  'pred_max_max': 1870,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.29433614015579224,\n",
       "  'sim_max_max': 0.2996489},\n",
       " 9358: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1871,\n",
       "  'pred_knn': 1871,\n",
       "  'sim_real_cl': 0.33035624,\n",
       "  'sim_best_knn': 0.33035624,\n",
       "  'pred_soft': 2066,\n",
       "  'soft_val': 0.320050448179245,\n",
       "  'sim_soft': 0.30049515},\n",
       " 9383: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1876,\n",
       "  'pred_knn': 1876,\n",
       "  'sim_real_cl': 0.3431661,\n",
       "  'sim_best_knn': 0.3431661,\n",
       "  'pred_soft': 3661,\n",
       "  'soft_val': 0.26749175786972046,\n",
       "  'sim_soft': 0.33137953,\n",
       "  'pred_max_max': 5062,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27533918619155884,\n",
       "  'sim_max_max': 0.30659392},\n",
       " 9436: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1887,\n",
       "  'pred_knn': 3283,\n",
       "  'sim_real_cl': 0.310273,\n",
       "  'sim_best_knn': 0.33627722,\n",
       "  'pred_soft': 1887,\n",
       "  'soft_val': 0.2550501823425293,\n",
       "  'sim_soft': array(0.25505018, dtype=float32),\n",
       "  'pred_max_max': 3283,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27156996726989746,\n",
       "  'sim_max_max': 0.33627722},\n",
       " 9479: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1895,\n",
       "  'pred_knn': 7061,\n",
       "  'sim_real_cl': 0.29893607,\n",
       "  'sim_best_knn': 0.31892896,\n",
       "  'pred_soft': 1895,\n",
       "  'soft_val': 0.3209807276725769,\n",
       "  'sim_soft': array(0.32098073, dtype=float32),\n",
       "  'pred_max_max': 1895,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.28313738107681274,\n",
       "  'sim_max_max': 0.31892896},\n",
       " 9535: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1907,\n",
       "  'pred_knn': 1907,\n",
       "  'sim_real_cl': 0.36527196,\n",
       "  'sim_best_knn': 0.36527196,\n",
       "  'pred_soft': 7097,\n",
       "  'soft_val': 0.30495986342430115,\n",
       "  'sim_soft': 0.34300297,\n",
       "  'pred_max_max': 7097,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3463085889816284,\n",
       "  'sim_max_max': 0.34300297},\n",
       " 9569: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1913,\n",
       "  'pred_knn': 4622,\n",
       "  'sim_real_cl': 0.26404098,\n",
       "  'sim_best_knn': 0.27438688,\n",
       "  'pred_soft': 2020,\n",
       "  'soft_val': 0.2277480661869049,\n",
       "  'sim_soft': 0.23146681,\n",
       "  'pred_max_max': 1913,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.27918949723243713,\n",
       "  'sim_max_max': 0.27438688},\n",
       " 9577: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1915,\n",
       "  'pred_knn': 1915,\n",
       "  'sim_real_cl': 0.29780394,\n",
       "  'sim_best_knn': 0.29780394,\n",
       "  'pred_soft': 8350,\n",
       "  'soft_val': 0.29609215259552,\n",
       "  'sim_soft': 0.2806533},\n",
       " 9593: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1918,\n",
       "  'pred_knn': 7685,\n",
       "  'sim_real_cl': 0.22040632,\n",
       "  'sim_best_knn': 0.3168438,\n",
       "  'pred_soft': 8935,\n",
       "  'soft_val': 0.27596554160118103,\n",
       "  'sim_soft': 0.3113005,\n",
       "  'pred_max_max': 8935,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27092796564102173,\n",
       "  'sim_max_max': 0.3113005},\n",
       " 9660: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1932,\n",
       "  'pred_knn': 1932,\n",
       "  'sim_real_cl': 0.56163186,\n",
       "  'sim_best_knn': 0.56163186,\n",
       "  'pred_soft': 5580,\n",
       "  'soft_val': 0.3811907172203064,\n",
       "  'sim_soft': 0.42408657},\n",
       " 9705: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1941,\n",
       "  'pred_knn': 1291,\n",
       "  'sim_real_cl': 0.38686946,\n",
       "  'sim_best_knn': 0.40500164,\n",
       "  'pred_soft': 1941,\n",
       "  'soft_val': 0.3927614986896515,\n",
       "  'sim_soft': array(0.3927615, dtype=float32),\n",
       "  'pred_max_max': 1941,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.43533211946487427,\n",
       "  'sim_max_max': 0.40500164},\n",
       " 9717: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1943,\n",
       "  'pred_knn': 1864,\n",
       "  'sim_real_cl': 0.24935366,\n",
       "  'sim_best_knn': 0.26817012,\n",
       "  'pred_soft': 4405,\n",
       "  'soft_val': 0.22532349824905396,\n",
       "  'sim_soft': 0.23395614,\n",
       "  'pred_max_max': 1943,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.24528029561042786,\n",
       "  'sim_max_max': 0.26817012},\n",
       " 9807: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1961,\n",
       "  'pred_knn': 434,\n",
       "  'sim_real_cl': 0.36828077,\n",
       "  'sim_best_knn': 0.38841057,\n",
       "  'pred_soft': 1961,\n",
       "  'soft_val': 0.38790199160575867,\n",
       "  'sim_soft': array(0.387902, dtype=float32),\n",
       "  'pred_max_max': 1961,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.30560553073883057,\n",
       "  'sim_max_max': 0.38841057},\n",
       " 9879: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1975,\n",
       "  'pred_knn': 3024,\n",
       "  'sim_real_cl': 0.3155081,\n",
       "  'sim_best_knn': 0.506462,\n",
       "  'pred_soft': 2873,\n",
       "  'soft_val': 0.2530772089958191,\n",
       "  'sim_soft': 0.25600082,\n",
       "  'pred_max_max': 3024,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.29743435978889465,\n",
       "  'sim_max_max': 0.506462},\n",
       " 9962: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1992,\n",
       "  'pred_knn': 3929,\n",
       "  'sim_real_cl': 0.33832353,\n",
       "  'sim_best_knn': 0.40564865,\n",
       "  'pred_soft': 9875,\n",
       "  'soft_val': 0.31506025791168213,\n",
       "  'sim_soft': 0.36172092,\n",
       "  'pred_max_max': 3929,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3274936079978943,\n",
       "  'sim_max_max': 0.40564865},\n",
       " 9971: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1994,\n",
       "  'pred_knn': 7345,\n",
       "  'sim_real_cl': 0.31832498,\n",
       "  'sim_best_knn': 0.35956433,\n",
       "  'pred_soft': 7863,\n",
       "  'soft_val': 0.2868555784225464,\n",
       "  'sim_soft': 0.31229204,\n",
       "  'pred_max_max': 1994,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3035365343093872,\n",
       "  'sim_max_max': 0.35956433},\n",
       " 9972: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 1994,\n",
       "  'pred_knn': 7345,\n",
       "  'sim_real_cl': 0.298163,\n",
       "  'sim_best_knn': 0.37119657,\n",
       "  'pred_soft': 7863,\n",
       "  'soft_val': 0.3307325839996338,\n",
       "  'sim_soft': 0.3382282,\n",
       "  'pred_max_max': 7863,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3256373107433319,\n",
       "  'sim_max_max': 0.3382282},\n",
       " 9995: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 1999,\n",
       "  'pred_knn': 1999,\n",
       "  'sim_real_cl': 0.40971118,\n",
       "  'sim_best_knn': 0.40971118,\n",
       "  'pred_soft': 184,\n",
       "  'soft_val': 0.26953020691871643,\n",
       "  'sim_soft': 0.32763273},\n",
       " 10136: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2027,\n",
       "  'pred_knn': 2655,\n",
       "  'sim_real_cl': 0.26590568,\n",
       "  'sim_best_knn': 0.36112288,\n",
       "  'pred_soft': 2655,\n",
       "  'soft_val': 0.3291740119457245,\n",
       "  'sim_soft': 0.36112288,\n",
       "  'pred_max_max': 2655,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.34754595160484314,\n",
       "  'sim_max_max': 0.36112288},\n",
       " 10162: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2032,\n",
       "  'pred_knn': 2032,\n",
       "  'sim_real_cl': 0.39318103,\n",
       "  'sim_best_knn': 0.39318103,\n",
       "  'pred_soft': 2639,\n",
       "  'soft_val': 0.2659768760204315,\n",
       "  'sim_soft': 0.25617743},\n",
       " 10235: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2047,\n",
       "  'pred_knn': 4456,\n",
       "  'sim_real_cl': 0.2784722,\n",
       "  'sim_best_knn': 0.33788803,\n",
       "  'pred_soft': 1664,\n",
       "  'soft_val': 0.26482832431793213,\n",
       "  'sim_soft': 0.25807527,\n",
       "  'pred_max_max': 2124,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26161402463912964,\n",
       "  'sim_max_max': 0.2935487},\n",
       " 10342: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2068,\n",
       "  'pred_knn': 4952,\n",
       "  'sim_real_cl': 0.33632654,\n",
       "  'sim_best_knn': 0.4067762,\n",
       "  'pred_soft': 4952,\n",
       "  'soft_val': 0.2838861644268036,\n",
       "  'sim_soft': 0.4067762,\n",
       "  'pred_max_max': 8599,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.303977906703949,\n",
       "  'sim_max_max': 0.31896445},\n",
       " 10350: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2070,\n",
       "  'pred_knn': 2738,\n",
       "  'sim_real_cl': 0.2987997,\n",
       "  'sim_best_knn': 0.34138632,\n",
       "  'pred_soft': 2070,\n",
       "  'soft_val': 0.2848936915397644,\n",
       "  'sim_soft': array(0.2848937, dtype=float32),\n",
       "  'pred_max_max': 2070,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.29256585240364075,\n",
       "  'sim_max_max': 0.34138632},\n",
       " 10370: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2074,\n",
       "  'pred_knn': 2074,\n",
       "  'sim_real_cl': 0.85913384,\n",
       "  'sim_best_knn': 0.85913384,\n",
       "  'pred_soft': 1640,\n",
       "  'soft_val': 0.33000829815864563,\n",
       "  'sim_soft': 0.36435777},\n",
       " 10371: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2074,\n",
       "  'pred_knn': 2074,\n",
       "  'sim_real_cl': 0.6219476,\n",
       "  'sim_best_knn': 0.6219476,\n",
       "  'pred_soft': 5193,\n",
       "  'soft_val': 0.29068201780319214,\n",
       "  'sim_soft': 0.3301181},\n",
       " 10374: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2074,\n",
       "  'pred_knn': 2074,\n",
       "  'sim_real_cl': 0.52909505,\n",
       "  'sim_best_knn': 0.52909505,\n",
       "  'pred_soft': 2690,\n",
       "  'soft_val': 0.30347225069999695,\n",
       "  'sim_soft': 0.35485566},\n",
       " 10519: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2103,\n",
       "  'pred_knn': 2103,\n",
       "  'sim_real_cl': 0.29940027,\n",
       "  'sim_best_knn': 0.29940027,\n",
       "  'pred_soft': 9770,\n",
       "  'soft_val': 0.26891353726387024,\n",
       "  'sim_soft': 0.28365964},\n",
       " 10527: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2105,\n",
       "  'pred_knn': 4379,\n",
       "  'sim_real_cl': 0.29743147,\n",
       "  'sim_best_knn': 0.32332414,\n",
       "  'pred_soft': 5585,\n",
       "  'soft_val': 0.2731800079345703,\n",
       "  'sim_soft': 0.286887,\n",
       "  'pred_max_max': 4379,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28041550517082214,\n",
       "  'sim_max_max': 0.32332414},\n",
       " 10649: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2129,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.27172178,\n",
       "  'sim_best_knn': 0.43022746,\n",
       "  'pred_soft': 1738,\n",
       "  'soft_val': 0.25871041417121887,\n",
       "  'sim_soft': 0.30347967,\n",
       "  'pred_max_max': 1738,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2793194055557251,\n",
       "  'sim_max_max': 0.30347967},\n",
       " 10693: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2138,\n",
       "  'pred_knn': 9006,\n",
       "  'sim_real_cl': 0.21058695,\n",
       "  'sim_best_knn': 0.3494347,\n",
       "  'pred_soft': 2316,\n",
       "  'soft_val': 0.2574703097343445,\n",
       "  'sim_soft': 0.2838418,\n",
       "  'pred_max_max': 6305,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.255687415599823,\n",
       "  'sim_max_max': 0.26295632},\n",
       " 10834: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2166,\n",
       "  'pred_knn': 2166,\n",
       "  'sim_real_cl': 0.37209147,\n",
       "  'sim_best_knn': 0.37209147,\n",
       "  'pred_soft': 2633,\n",
       "  'soft_val': 0.2747568190097809,\n",
       "  'sim_soft': 0.27007985},\n",
       " 10880: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2176,\n",
       "  'pred_knn': 675,\n",
       "  'sim_real_cl': 0.25241792,\n",
       "  'sim_best_knn': 0.3111454,\n",
       "  'pred_soft': 2176,\n",
       "  'soft_val': 0.3109874129295349,\n",
       "  'sim_soft': array(0.3109874, dtype=float32),\n",
       "  'pred_max_max': 1475,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28199175000190735,\n",
       "  'sim_max_max': 0.2867322},\n",
       " 10934: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2186,\n",
       "  'pred_knn': 2186,\n",
       "  'sim_real_cl': 0.3907627,\n",
       "  'sim_best_knn': 0.3907627,\n",
       "  'pred_soft': 2388,\n",
       "  'soft_val': 0.3076130747795105,\n",
       "  'sim_soft': 0.33757043},\n",
       " 10943: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2188,\n",
       "  'pred_knn': 2188,\n",
       "  'sim_real_cl': 0.33451727,\n",
       "  'sim_best_knn': 0.33451727,\n",
       "  'pred_soft': 2162,\n",
       "  'soft_val': 0.26120948791503906,\n",
       "  'sim_soft': 0.24765348},\n",
       " 10996: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2199,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.3907978,\n",
       "  'sim_best_knn': 0.39116323,\n",
       "  'pred_soft': 2770,\n",
       "  'soft_val': 0.28044480085372925,\n",
       "  'sim_soft': 0.2947197,\n",
       "  'pred_max_max': 2199,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.2977364957332611,\n",
       "  'sim_max_max': 0.39116323},\n",
       " 11011: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2202,\n",
       "  'pred_knn': 7485,\n",
       "  'sim_real_cl': 0.018858213,\n",
       "  'sim_best_knn': 0.35425574,\n",
       "  'pred_soft': 5767,\n",
       "  'soft_val': 0.31575629115104675,\n",
       "  'sim_soft': 0.3432629,\n",
       "  'pred_max_max': 5767,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.31575679779052734,\n",
       "  'sim_max_max': 0.3432629},\n",
       " 11027: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2205,\n",
       "  'pred_knn': 2205,\n",
       "  'sim_real_cl': 0.31788245,\n",
       "  'sim_best_knn': 0.31788245,\n",
       "  'pred_soft': 4829,\n",
       "  'soft_val': 0.2705532908439636,\n",
       "  'sim_soft': 0.26310584,\n",
       "  'pred_max_max': 8183,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24742868542671204,\n",
       "  'sim_max_max': 0.28780174},\n",
       " 11086: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2217,\n",
       "  'pred_knn': 2479,\n",
       "  'sim_real_cl': 0.07659031,\n",
       "  'sim_best_knn': 0.22053567,\n",
       "  'pred_soft': 7613,\n",
       "  'soft_val': 0.21693699061870575,\n",
       "  'sim_soft': 0.20755658,\n",
       "  'pred_max_max': 7613,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.19978897273540497,\n",
       "  'sim_max_max': 0.20755658},\n",
       " 11135: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2227,\n",
       "  'pred_knn': 3653,\n",
       "  'sim_real_cl': 0.35591936,\n",
       "  'sim_best_knn': 0.4603116,\n",
       "  'pred_soft': 184,\n",
       "  'soft_val': 0.2970733940601349,\n",
       "  'sim_soft': 0.3611647,\n",
       "  'pred_max_max': 4401,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.327608585357666,\n",
       "  'sim_max_max': 0.39822257},\n",
       " 11137: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2227,\n",
       "  'pred_knn': 736,\n",
       "  'sim_real_cl': 0.33391774,\n",
       "  'sim_best_knn': 0.36975813,\n",
       "  'pred_soft': 6674,\n",
       "  'soft_val': 0.2673519253730774,\n",
       "  'sim_soft': 0.32695043,\n",
       "  'pred_max_max': 736,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2937370836734772,\n",
       "  'sim_max_max': 0.36975813},\n",
       " 11186: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2237,\n",
       "  'pred_knn': 2237,\n",
       "  'sim_real_cl': 0.32837468,\n",
       "  'sim_best_knn': 0.32837468,\n",
       "  'pred_soft': 3461,\n",
       "  'soft_val': 0.24706700444221497,\n",
       "  'sim_soft': 0.3005566},\n",
       " 11204: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2240,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.35498828,\n",
       "  'sim_best_knn': 0.37807393,\n",
       "  'pred_soft': 2240,\n",
       "  'soft_val': 0.2680503726005554,\n",
       "  'sim_soft': array(0.26805037, dtype=float32),\n",
       "  'pred_max_max': 741,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26983004808425903,\n",
       "  'sim_max_max': 0.33615524},\n",
       " 11258: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2251,\n",
       "  'pred_knn': 1647,\n",
       "  'sim_real_cl': 0.32880473,\n",
       "  'sim_best_knn': 0.37192902,\n",
       "  'pred_soft': 8834,\n",
       "  'soft_val': 0.31994497776031494,\n",
       "  'sim_soft': 0.29279777,\n",
       "  'pred_max_max': 8834,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30321750044822693,\n",
       "  'sim_max_max': 0.29279777},\n",
       " 11292: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2258,\n",
       "  'pred_knn': 2698,\n",
       "  'sim_real_cl': 0.25766253,\n",
       "  'sim_best_knn': 0.402692,\n",
       "  'pred_soft': 2698,\n",
       "  'soft_val': 0.2694934010505676,\n",
       "  'sim_soft': 0.402692,\n",
       "  'pred_max_max': 2698,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.33347955346107483,\n",
       "  'sim_max_max': 0.402692},\n",
       " 11361: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2272,\n",
       "  'pred_knn': 6340,\n",
       "  'sim_real_cl': 0.27926317,\n",
       "  'sim_best_knn': 0.3027205,\n",
       "  'pred_soft': 7981,\n",
       "  'soft_val': 0.25856485962867737,\n",
       "  'sim_soft': 0.29036474,\n",
       "  'pred_max_max': 3903,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25869014859199524,\n",
       "  'sim_max_max': 0.27609342},\n",
       " 11379: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2275,\n",
       "  'pred_knn': 9495,\n",
       "  'sim_real_cl': -0.037731007,\n",
       "  'sim_best_knn': 0.24968123,\n",
       "  'pred_soft': 7387,\n",
       "  'soft_val': 0.20396897196769714,\n",
       "  'sim_soft': 0.16299713,\n",
       "  'pred_max_max': 9495,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.21870212256908417,\n",
       "  'sim_max_max': 0.24968123},\n",
       " 11386: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2277,\n",
       "  'pred_knn': 5836,\n",
       "  'sim_real_cl': 0.38088804,\n",
       "  'sim_best_knn': 0.3921818,\n",
       "  'pred_soft': 2277,\n",
       "  'soft_val': 0.3095870912075043,\n",
       "  'sim_soft': array(0.3095871, dtype=float32),\n",
       "  'pred_max_max': 2277,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.40682798624038696,\n",
       "  'sim_max_max': 0.3921818},\n",
       " 11423: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2284,\n",
       "  'pred_knn': 7009,\n",
       "  'sim_real_cl': 0.07730405,\n",
       "  'sim_best_knn': 0.28429973,\n",
       "  'pred_soft': 352,\n",
       "  'soft_val': 0.21470999717712402,\n",
       "  'sim_soft': 0.2440002,\n",
       "  'pred_max_max': 352,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.23751187324523926,\n",
       "  'sim_max_max': 0.2440002},\n",
       " 11467: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2293,\n",
       "  'pred_knn': 6116,\n",
       "  'sim_real_cl': 0.12495045,\n",
       "  'sim_best_knn': 0.25537318,\n",
       "  'pred_soft': 8805,\n",
       "  'soft_val': 0.22855429351329803,\n",
       "  'sim_soft': 0.19360997,\n",
       "  'pred_max_max': 6116,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2308538258075714,\n",
       "  'sim_max_max': 0.25537318},\n",
       " 11532: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2306,\n",
       "  'pred_knn': 6049,\n",
       "  'sim_real_cl': 0.21167485,\n",
       "  'sim_best_knn': 0.35285765,\n",
       "  'pred_soft': 6203,\n",
       "  'soft_val': 0.2649373710155487,\n",
       "  'sim_soft': 0.30305701,\n",
       "  'pred_max_max': 6049,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3037283420562744,\n",
       "  'sim_max_max': 0.35285765},\n",
       " 11572: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2314,\n",
       "  'pred_knn': 2314,\n",
       "  'sim_real_cl': 0.34399602,\n",
       "  'sim_best_knn': 0.34399602,\n",
       "  'pred_soft': 2426,\n",
       "  'soft_val': 0.2530028820037842,\n",
       "  'sim_soft': 0.25285113,\n",
       "  'pred_max_max': 4036,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2622770071029663,\n",
       "  'sim_max_max': 0.2320166},\n",
       " 11575: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2315,\n",
       "  'pred_knn': 4589,\n",
       "  'sim_real_cl': 0.2351905,\n",
       "  'sim_best_knn': 0.34574148,\n",
       "  'pred_soft': 4589,\n",
       "  'soft_val': 0.27112114429473877,\n",
       "  'sim_soft': 0.34574148,\n",
       "  'pred_max_max': 4589,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.29911327362060547,\n",
       "  'sim_max_max': 0.34574148},\n",
       " 11579: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2315,\n",
       "  'pred_knn': 3916,\n",
       "  'sim_real_cl': 0.35900328,\n",
       "  'sim_best_knn': 0.399975,\n",
       "  'pred_soft': 2315,\n",
       "  'soft_val': 0.3634139895439148,\n",
       "  'sim_soft': array(0.363414, dtype=float32),\n",
       "  'pred_max_max': 2315,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.36069604754447937,\n",
       "  'sim_max_max': 0.399975},\n",
       " 11593: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2318,\n",
       "  'pred_knn': 2318,\n",
       "  'sim_real_cl': 0.41379505,\n",
       "  'sim_best_knn': 0.41379505,\n",
       "  'pred_soft': 3854,\n",
       "  'soft_val': 0.2700730860233307,\n",
       "  'sim_soft': 0.26797694},\n",
       " 11602: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2320,\n",
       "  'pred_knn': 4530,\n",
       "  'sim_real_cl': 0.300414,\n",
       "  'sim_best_knn': 0.3316747,\n",
       "  'pred_soft': 6838,\n",
       "  'soft_val': 0.2774710953235626,\n",
       "  'sim_soft': 0.27669293,\n",
       "  'pred_max_max': 4530,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2869391143321991,\n",
       "  'sim_max_max': 0.3316747},\n",
       " 11624: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2324,\n",
       "  'pred_knn': 984,\n",
       "  'sim_real_cl': 0.3081046,\n",
       "  'sim_best_knn': 0.32598364,\n",
       "  'pred_soft': 2324,\n",
       "  'soft_val': 0.30450546741485596,\n",
       "  'sim_soft': array(0.30450547, dtype=float32),\n",
       "  'pred_max_max': 2324,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3223620355129242,\n",
       "  'sim_max_max': 0.32598364},\n",
       " 11639: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2327,\n",
       "  'pred_knn': 312,\n",
       "  'sim_real_cl': 0.3110438,\n",
       "  'sim_best_knn': 0.31450653,\n",
       "  'pred_soft': 901,\n",
       "  'soft_val': 0.28769180178642273,\n",
       "  'sim_soft': 0.31121433,\n",
       "  'pred_max_max': 901,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.29433101415634155,\n",
       "  'sim_max_max': 0.31121433},\n",
       " 11656: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2331,\n",
       "  'pred_knn': 6895,\n",
       "  'sim_real_cl': 0.1776954,\n",
       "  'sim_best_knn': 0.2839291,\n",
       "  'pred_soft': 6895,\n",
       "  'soft_val': 0.2772262394428253,\n",
       "  'sim_soft': 0.2839291,\n",
       "  'pred_max_max': 6895,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2833590805530548,\n",
       "  'sim_max_max': 0.2839291},\n",
       " 11715: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2343,\n",
       "  'pred_knn': 3790,\n",
       "  'sim_real_cl': 0.22724402,\n",
       "  'sim_best_knn': 0.26585525,\n",
       "  'pred_soft': 2343,\n",
       "  'soft_val': 0.22879576683044434,\n",
       "  'sim_soft': array(0.22879577, dtype=float32),\n",
       "  'pred_max_max': 2343,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.2789953351020813,\n",
       "  'sim_max_max': 0.26585525},\n",
       " 11737: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2347,\n",
       "  'pred_knn': 1379,\n",
       "  'sim_real_cl': 0.251141,\n",
       "  'sim_best_knn': 0.3195933,\n",
       "  'pred_soft': 2347,\n",
       "  'soft_val': 0.4051186442375183,\n",
       "  'sim_soft': array(0.40511864, dtype=float32),\n",
       "  'pred_max_max': 2347,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.31685373187065125,\n",
       "  'sim_max_max': 0.3195933},\n",
       " 11764: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2352,\n",
       "  'pred_knn': 2352,\n",
       "  'sim_real_cl': 0.34240872,\n",
       "  'sim_best_knn': 0.34240872,\n",
       "  'pred_soft': 1980,\n",
       "  'soft_val': 0.28068217635154724,\n",
       "  'sim_soft': 0.28699923},\n",
       " 11790: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2358,\n",
       "  'pred_knn': 3796,\n",
       "  'sim_real_cl': 0.26258257,\n",
       "  'sim_best_knn': 0.39405167,\n",
       "  'pred_soft': 3796,\n",
       "  'soft_val': 0.2590450644493103,\n",
       "  'sim_soft': 0.39405167,\n",
       "  'pred_max_max': 3796,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2873939871788025,\n",
       "  'sim_max_max': 0.39405167},\n",
       " 11874: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2374,\n",
       "  'pred_knn': 2374,\n",
       "  'sim_real_cl': 0.45258242,\n",
       "  'sim_best_knn': 0.45258242,\n",
       "  'pred_soft': 2112,\n",
       "  'soft_val': 0.24586121737957,\n",
       "  'sim_soft': 0.254459},\n",
       " 11878: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2375,\n",
       "  'pred_knn': 2625,\n",
       "  'sim_real_cl': 0.3305023,\n",
       "  'sim_best_knn': 0.39321896,\n",
       "  'pred_soft': 2375,\n",
       "  'soft_val': 0.29375168681144714,\n",
       "  'sim_soft': array(0.2937517, dtype=float32),\n",
       "  'pred_max_max': 2375,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.29245445132255554,\n",
       "  'sim_max_max': 0.39321896},\n",
       " 11987: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2397,\n",
       "  'pred_knn': 8239,\n",
       "  'sim_real_cl': 0.26317328,\n",
       "  'sim_best_knn': 0.3259909,\n",
       "  'pred_soft': 5800,\n",
       "  'soft_val': 0.2493557631969452,\n",
       "  'sim_soft': 0.24088314,\n",
       "  'pred_max_max': 894,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26638928055763245,\n",
       "  'sim_max_max': 0.28562474},\n",
       " 11994: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2398,\n",
       "  'pred_knn': 2398,\n",
       "  'sim_real_cl': 0.35305604,\n",
       "  'sim_best_knn': 0.35305604,\n",
       "  'pred_soft': 3640,\n",
       "  'soft_val': 0.2880832254886627,\n",
       "  'sim_soft': 0.27174285,\n",
       "  'pred_max_max': 3640,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3136777877807617,\n",
       "  'sim_max_max': 0.27174285},\n",
       " 12014: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2402,\n",
       "  'pred_knn': 4473,\n",
       "  'sim_real_cl': 0.3211449,\n",
       "  'sim_best_knn': 0.35077915,\n",
       "  'pred_soft': 8413,\n",
       "  'soft_val': 0.2695496380329132,\n",
       "  'sim_soft': 0.33168626,\n",
       "  'pred_max_max': 4473,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2807808518409729,\n",
       "  'sim_max_max': 0.35077915},\n",
       " 12154: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2430,\n",
       "  'pred_knn': 3880,\n",
       "  'sim_real_cl': 0.31311214,\n",
       "  'sim_best_knn': 0.33941248,\n",
       "  'pred_soft': 1904,\n",
       "  'soft_val': 0.263596773147583,\n",
       "  'sim_soft': 0.26854146,\n",
       "  'pred_max_max': 3880,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2732866704463959,\n",
       "  'sim_max_max': 0.33941248},\n",
       " 12155: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2431,\n",
       "  'pred_knn': 8938,\n",
       "  'sim_real_cl': 0.2843755,\n",
       "  'sim_best_knn': 0.3384603,\n",
       "  'pred_soft': 5497,\n",
       "  'soft_val': 0.30955174565315247,\n",
       "  'sim_soft': 0.29375443,\n",
       "  'pred_max_max': 5497,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2885207235813141,\n",
       "  'sim_max_max': 0.29375443},\n",
       " 12215: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2443,\n",
       "  'pred_knn': 699,\n",
       "  'sim_real_cl': 0.2540654,\n",
       "  'sim_best_knn': 0.2996474,\n",
       "  'pred_soft': 5979,\n",
       "  'soft_val': 0.23588083684444427,\n",
       "  'sim_soft': 0.25578174,\n",
       "  'pred_max_max': 1214,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2726157605648041,\n",
       "  'sim_max_max': 0.28842473},\n",
       " 12222: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2444,\n",
       "  'pred_knn': 3797,\n",
       "  'sim_real_cl': 0.34086236,\n",
       "  'sim_best_knn': 0.3774132,\n",
       "  'pred_soft': 2444,\n",
       "  'soft_val': 0.33788272738456726,\n",
       "  'sim_soft': array(0.33788273, dtype=float32),\n",
       "  'pred_max_max': 2444,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.32341471314430237,\n",
       "  'sim_max_max': 0.3774132},\n",
       " 12430: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2486,\n",
       "  'pred_knn': 2028,\n",
       "  'sim_real_cl': 0.19318251,\n",
       "  'sim_best_knn': 0.36894917,\n",
       "  'pred_soft': 2829,\n",
       "  'soft_val': 0.2703641355037689,\n",
       "  'sim_soft': 0.3345766,\n",
       "  'pred_max_max': 2028,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.29331251978874207,\n",
       "  'sim_max_max': 0.36894917},\n",
       " 12536: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2507,\n",
       "  'pred_knn': 5631,\n",
       "  'sim_real_cl': 0.33547962,\n",
       "  'sim_best_knn': 0.39601234,\n",
       "  'pred_soft': 5631,\n",
       "  'soft_val': 0.2892429232597351,\n",
       "  'sim_soft': 0.39601234,\n",
       "  'pred_max_max': 2507,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3662169873714447,\n",
       "  'sim_max_max': 0.39601234},\n",
       " 12587: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2517,\n",
       "  'pred_knn': 2517,\n",
       "  'sim_real_cl': 0.33001775,\n",
       "  'sim_best_knn': 0.33001775,\n",
       "  'pred_soft': 661,\n",
       "  'soft_val': 0.28152236342430115,\n",
       "  'sim_soft': 0.25155148,\n",
       "  'pred_max_max': 3439,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24054569005966187,\n",
       "  'sim_max_max': 0.25882977},\n",
       " 12646: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2529,\n",
       "  'pred_knn': 2856,\n",
       "  'sim_real_cl': 0.26323333,\n",
       "  'sim_best_knn': 0.28690693,\n",
       "  'pred_soft': 7901,\n",
       "  'soft_val': 0.2776813805103302,\n",
       "  'sim_soft': 0.2468872,\n",
       "  'pred_max_max': 1214,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24936720728874207,\n",
       "  'sim_max_max': 0.23622316},\n",
       " 12701: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2540,\n",
       "  'pred_knn': 2540,\n",
       "  'sim_real_cl': 0.35322958,\n",
       "  'sim_best_knn': 0.35322958,\n",
       "  'pred_soft': 2638,\n",
       "  'soft_val': 0.2557925879955292,\n",
       "  'sim_soft': 0.24971256},\n",
       " 12736: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2547,\n",
       "  'pred_knn': 8319,\n",
       "  'sim_real_cl': 0.33175576,\n",
       "  'sim_best_knn': 0.3471714,\n",
       "  'pred_soft': 2547,\n",
       "  'soft_val': 0.35683122277259827,\n",
       "  'sim_soft': array(0.35683122, dtype=float32),\n",
       "  'pred_max_max': 2547,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.39878177642822266,\n",
       "  'sim_max_max': 0.3471714},\n",
       " 12750: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2550,\n",
       "  'pred_knn': 5484,\n",
       "  'sim_real_cl': 0.24937934,\n",
       "  'sim_best_knn': 0.34495753,\n",
       "  'pred_soft': 2370,\n",
       "  'soft_val': 0.25727787613868713,\n",
       "  'sim_soft': 0.2807144,\n",
       "  'pred_max_max': 5484,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30034786462783813,\n",
       "  'sim_max_max': 0.34495753},\n",
       " 12789: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2557,\n",
       "  'pred_knn': 2604,\n",
       "  'sim_real_cl': 0.31263793,\n",
       "  'sim_best_knn': 0.34202802,\n",
       "  'pred_soft': 5382,\n",
       "  'soft_val': 0.2928623855113983,\n",
       "  'sim_soft': 0.28248936,\n",
       "  'pred_max_max': 2623,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3001919090747833,\n",
       "  'sim_max_max': 0.30122444},\n",
       " 12825: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2565,\n",
       "  'pred_knn': 9794,\n",
       "  'sim_real_cl': 0.20710859,\n",
       "  'sim_best_knn': 0.34441334,\n",
       "  'pred_soft': 5083,\n",
       "  'soft_val': 0.25933927297592163,\n",
       "  'sim_soft': 0.30173647,\n",
       "  'pred_max_max': 5083,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27254822850227356,\n",
       "  'sim_max_max': 0.30173647},\n",
       " 12828: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2565,\n",
       "  'pred_knn': 9794,\n",
       "  'sim_real_cl': 0.27360618,\n",
       "  'sim_best_knn': 0.33988127,\n",
       "  'pred_soft': 2565,\n",
       "  'soft_val': 0.27558067440986633,\n",
       "  'sim_soft': array(0.27558067, dtype=float32),\n",
       "  'pred_max_max': 5083,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2749771475791931,\n",
       "  'sim_max_max': 0.3099892},\n",
       " 12849: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2569,\n",
       "  'pred_knn': 4779,\n",
       "  'sim_real_cl': 0.22088802,\n",
       "  'sim_best_knn': 0.3080901,\n",
       "  'pred_soft': 4779,\n",
       "  'soft_val': 0.27360373735427856,\n",
       "  'sim_soft': 0.3080901,\n",
       "  'pred_max_max': 4779,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28091126680374146,\n",
       "  'sim_max_max': 0.3080901},\n",
       " 12860: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2572,\n",
       "  'pred_knn': 2275,\n",
       "  'sim_real_cl': 0.3250268,\n",
       "  'sim_best_knn': 0.44400635,\n",
       "  'pred_soft': 849,\n",
       "  'soft_val': 0.2744825780391693,\n",
       "  'sim_soft': 0.25102141,\n",
       "  'pred_max_max': 2572,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3042498528957367,\n",
       "  'sim_max_max': 0.44400635},\n",
       " 12885: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2577,\n",
       "  'pred_knn': 2577,\n",
       "  'sim_real_cl': 0.3672686,\n",
       "  'sim_best_knn': 0.3672686,\n",
       "  'pred_soft': 7880,\n",
       "  'soft_val': 0.20717144012451172,\n",
       "  'sim_soft': 0.2521964},\n",
       " 12913: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2582,\n",
       "  'pred_knn': 2582,\n",
       "  'sim_real_cl': 0.5658771,\n",
       "  'sim_best_knn': 0.5658771,\n",
       "  'pred_soft': 2734,\n",
       "  'soft_val': 0.3621284067630768,\n",
       "  'sim_soft': 0.3854841},\n",
       " 12927: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2585,\n",
       "  'pred_knn': 3777,\n",
       "  'sim_real_cl': 0.16768593,\n",
       "  'sim_best_knn': 0.27677763,\n",
       "  'pred_soft': 3750,\n",
       "  'soft_val': 0.23046228289604187,\n",
       "  'sim_soft': 0.22572672,\n",
       "  'pred_max_max': 3750,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2579815685749054,\n",
       "  'sim_max_max': 0.22572672},\n",
       " 12928: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2585,\n",
       "  'pred_knn': 9356,\n",
       "  'sim_real_cl': 0.38746607,\n",
       "  'sim_best_knn': 0.3885445,\n",
       "  'pred_soft': 2585,\n",
       "  'soft_val': 0.3212888538837433,\n",
       "  'sim_soft': array(0.32128885, dtype=float32),\n",
       "  'pred_max_max': 2585,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3496936857700348,\n",
       "  'sim_max_max': 0.3885445},\n",
       " 13011: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2602,\n",
       "  'pred_knn': 7809,\n",
       "  'sim_real_cl': 0.47701627,\n",
       "  'sim_best_knn': 0.5182339,\n",
       "  'pred_soft': 7809,\n",
       "  'soft_val': 0.46792367100715637,\n",
       "  'sim_soft': 0.5182339,\n",
       "  'pred_max_max': 7809,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.46767836809158325,\n",
       "  'sim_max_max': 0.5182339},\n",
       " 13037: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2607,\n",
       "  'pred_knn': 2607,\n",
       "  'sim_real_cl': 0.34967342,\n",
       "  'sim_best_knn': 0.34967342,\n",
       "  'pred_soft': 5251,\n",
       "  'soft_val': 0.2892272472381592,\n",
       "  'sim_soft': 0.340104,\n",
       "  'pred_max_max': 1600,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3307749629020691,\n",
       "  'sim_max_max': 0.34288478},\n",
       " 13044: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2608,\n",
       "  'pred_knn': 2556,\n",
       "  'sim_real_cl': 0.43201017,\n",
       "  'sim_best_knn': 0.44129202,\n",
       "  'pred_soft': 2556,\n",
       "  'soft_val': 0.43110910058021545,\n",
       "  'sim_soft': 0.44129202,\n",
       "  'pred_max_max': 2608,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.37303104996681213,\n",
       "  'sim_max_max': 0.44129202},\n",
       " 13118: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2623,\n",
       "  'pred_knn': 3943,\n",
       "  'sim_real_cl': 0.2776736,\n",
       "  'sim_best_knn': 0.30056113,\n",
       "  'pred_soft': 2623,\n",
       "  'soft_val': 0.2500361204147339,\n",
       "  'sim_soft': array(0.25003612, dtype=float32),\n",
       "  'pred_max_max': 2623,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.26853805780410767,\n",
       "  'sim_max_max': 0.30056113},\n",
       " 13133: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2626,\n",
       "  'pred_knn': 3242,\n",
       "  'sim_real_cl': 0.2620046,\n",
       "  'sim_best_knn': 0.30766428,\n",
       "  'pred_soft': 8507,\n",
       "  'soft_val': 0.29400116205215454,\n",
       "  'sim_soft': 0.28330633,\n",
       "  'pred_max_max': 4882,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2823830246925354,\n",
       "  'sim_max_max': 0.28803524},\n",
       " 13149: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2629,\n",
       "  'pred_knn': 4528,\n",
       "  'sim_real_cl': 0.19637887,\n",
       "  'sim_best_knn': 0.33176947,\n",
       "  'pred_soft': 4528,\n",
       "  'soft_val': 0.30478689074516296,\n",
       "  'sim_soft': 0.33176947,\n",
       "  'pred_max_max': 4528,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2814517915248871,\n",
       "  'sim_max_max': 0.33176947},\n",
       " 13151: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2630,\n",
       "  'pred_knn': 8548,\n",
       "  'sim_real_cl': 0.35155278,\n",
       "  'sim_best_knn': 0.39641562,\n",
       "  'pred_soft': 2630,\n",
       "  'soft_val': 0.3260100483894348,\n",
       "  'sim_soft': array(0.32601005, dtype=float32),\n",
       "  'pred_max_max': 2630,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.31811633706092834,\n",
       "  'sim_max_max': 0.39641562},\n",
       " 13199: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2639,\n",
       "  'pred_knn': 6421,\n",
       "  'sim_real_cl': 0.23694748,\n",
       "  'sim_best_knn': 0.27730703,\n",
       "  'pred_soft': 6421,\n",
       "  'soft_val': 0.25807708501815796,\n",
       "  'sim_soft': 0.27730703,\n",
       "  'pred_max_max': 4728,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2638011574745178,\n",
       "  'sim_max_max': 0.2754938},\n",
       " 13214: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2642,\n",
       "  'pred_knn': 4038,\n",
       "  'sim_real_cl': 0.29535854,\n",
       "  'sim_best_knn': 0.3194344,\n",
       "  'pred_soft': 2642,\n",
       "  'soft_val': 0.25577953457832336,\n",
       "  'sim_soft': array(0.25577953, dtype=float32),\n",
       "  'pred_max_max': 9201,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27885037660598755,\n",
       "  'sim_max_max': 0.26891214},\n",
       " 13268: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2653,\n",
       "  'pred_knn': 4456,\n",
       "  'sim_real_cl': 0.28496036,\n",
       "  'sim_best_knn': 0.30822778,\n",
       "  'pred_soft': 2919,\n",
       "  'soft_val': 0.28483569622039795,\n",
       "  'sim_soft': 0.26992586,\n",
       "  'pred_max_max': 2653,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.28655102849006653,\n",
       "  'sim_max_max': 0.30822778},\n",
       " 13386: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2677,\n",
       "  'pred_knn': 1747,\n",
       "  'sim_real_cl': 0.50782734,\n",
       "  'sim_best_knn': 0.5170976,\n",
       "  'pred_soft': 2677,\n",
       "  'soft_val': 0.3627372086048126,\n",
       "  'sim_soft': array(0.3627372, dtype=float32),\n",
       "  'pred_max_max': 1747,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.44875025749206543,\n",
       "  'sim_max_max': 0.5170976},\n",
       " 13472: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2694,\n",
       "  'pred_knn': 2694,\n",
       "  'sim_real_cl': 0.36014274,\n",
       "  'sim_best_knn': 0.36014274,\n",
       "  'pred_soft': 6938,\n",
       "  'soft_val': 0.32125020027160645,\n",
       "  'sim_soft': 0.3420332},\n",
       " 13482: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2696,\n",
       "  'pred_knn': 1861,\n",
       "  'sim_real_cl': 0.28818858,\n",
       "  'sim_best_knn': 0.32659662,\n",
       "  'pred_soft': 2067,\n",
       "  'soft_val': 0.26158225536346436,\n",
       "  'sim_soft': 0.2544266,\n",
       "  'pred_max_max': 1315,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27062588930130005,\n",
       "  'sim_max_max': 0.265835},\n",
       " 13546: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2709,\n",
       "  'pred_knn': 2709,\n",
       "  'sim_real_cl': 0.37738043,\n",
       "  'sim_best_knn': 0.37738043,\n",
       "  'pred_soft': 5640,\n",
       "  'soft_val': 0.31557586789131165,\n",
       "  'sim_soft': 0.34107387,\n",
       "  'pred_max_max': 5640,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3031126856803894,\n",
       "  'sim_max_max': 0.34107387},\n",
       " 13594: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2718,\n",
       "  'pred_knn': 1677,\n",
       "  'sim_real_cl': 0.23216097,\n",
       "  'sim_best_knn': 0.31484053,\n",
       "  'pred_soft': 155,\n",
       "  'soft_val': 0.25084683299064636,\n",
       "  'sim_soft': 0.2901935,\n",
       "  'pred_max_max': 155,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2879149317741394,\n",
       "  'sim_max_max': 0.2901935},\n",
       " 13640: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2728,\n",
       "  'pred_knn': 399,\n",
       "  'sim_real_cl': 0.4208672,\n",
       "  'sim_best_knn': 0.4211655,\n",
       "  'pred_soft': 2728,\n",
       "  'soft_val': 0.3831741511821747,\n",
       "  'sim_soft': array(0.38317415, dtype=float32),\n",
       "  'pred_max_max': 2728,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.41574105620384216,\n",
       "  'sim_max_max': 0.4211655},\n",
       " 13644: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2728,\n",
       "  'pred_knn': 7441,\n",
       "  'sim_real_cl': 0.27061006,\n",
       "  'sim_best_knn': 0.32545334,\n",
       "  'pred_soft': 7441,\n",
       "  'soft_val': 0.2735622823238373,\n",
       "  'sim_soft': 0.32545334,\n",
       "  'pred_max_max': 7441,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2791421115398407,\n",
       "  'sim_max_max': 0.32545334},\n",
       " 13744: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2748,\n",
       "  'pred_knn': 1491,\n",
       "  'sim_real_cl': 0.34764874,\n",
       "  'sim_best_knn': 0.36675033,\n",
       "  'pred_soft': 2748,\n",
       "  'soft_val': 0.3245488107204437,\n",
       "  'sim_soft': array(0.3245488, dtype=float32),\n",
       "  'pred_max_max': 2748,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.36867618560791016,\n",
       "  'sim_max_max': 0.36675033},\n",
       " 13794: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2758,\n",
       "  'pred_knn': 1371,\n",
       "  'sim_real_cl': 0.26861227,\n",
       "  'sim_best_knn': 0.387542,\n",
       "  'pred_soft': 631,\n",
       "  'soft_val': 0.2625712752342224,\n",
       "  'sim_soft': 0.33152848,\n",
       "  'pred_max_max': 1371,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3112635314464569,\n",
       "  'sim_max_max': 0.387542},\n",
       " 13868: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2773,\n",
       "  'pred_knn': 2773,\n",
       "  'sim_real_cl': 0.4200276,\n",
       "  'sim_best_knn': 0.4200276,\n",
       "  'pred_soft': 926,\n",
       "  'soft_val': 0.30494168400764465,\n",
       "  'sim_soft': 0.30554384},\n",
       " 13892: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2778,\n",
       "  'pred_knn': 2778,\n",
       "  'sim_real_cl': 0.50057214,\n",
       "  'sim_best_knn': 0.50057214,\n",
       "  'pred_soft': 8984,\n",
       "  'soft_val': 0.38587695360183716,\n",
       "  'sim_soft': 0.38127702,\n",
       "  'pred_max_max': 8274,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3640492260456085,\n",
       "  'sim_max_max': 0.43510437},\n",
       " 13904: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2780,\n",
       "  'pred_knn': 3931,\n",
       "  'sim_real_cl': 0.22463356,\n",
       "  'sim_best_knn': 0.33029515,\n",
       "  'pred_soft': 3931,\n",
       "  'soft_val': 0.26932933926582336,\n",
       "  'sim_soft': 0.33029515,\n",
       "  'pred_max_max': 3931,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28607916831970215,\n",
       "  'sim_max_max': 0.33029515},\n",
       " 13927: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2785,\n",
       "  'pred_knn': 2785,\n",
       "  'sim_real_cl': 0.3946734,\n",
       "  'sim_best_knn': 0.3946734,\n",
       "  'pred_soft': 8910,\n",
       "  'soft_val': 0.30381059646606445,\n",
       "  'sim_soft': 0.30655205,\n",
       "  'pred_max_max': 1617,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.32622531056404114,\n",
       "  'sim_max_max': 0.30885246},\n",
       " 13944: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2788,\n",
       "  'pred_knn': 2788,\n",
       "  'sim_real_cl': 0.4588916,\n",
       "  'sim_best_knn': 0.4588916,\n",
       "  'pred_soft': 8834,\n",
       "  'soft_val': 0.39872726798057556,\n",
       "  'sim_soft': 0.40731505,\n",
       "  'pred_max_max': 8834,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.41749557852745056,\n",
       "  'sim_max_max': 0.40731505},\n",
       " 13974: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2794,\n",
       "  'pred_knn': 7703,\n",
       "  'sim_real_cl': 0.19263043,\n",
       "  'sim_best_knn': 0.29258,\n",
       "  'pred_soft': 7703,\n",
       "  'soft_val': 0.2530718147754669,\n",
       "  'sim_soft': 0.29258,\n",
       "  'pred_max_max': 7703,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.23504731059074402,\n",
       "  'sim_max_max': 0.29258},\n",
       " 14031: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2806,\n",
       "  'pred_knn': 2806,\n",
       "  'sim_real_cl': 0.3327844,\n",
       "  'sim_best_knn': 0.3327844,\n",
       "  'pred_soft': 282,\n",
       "  'soft_val': 0.2968311309814453,\n",
       "  'sim_soft': 0.30403322},\n",
       " 14072: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2814,\n",
       "  'pred_knn': 4401,\n",
       "  'sim_real_cl': 0.2575863,\n",
       "  'sim_best_knn': 0.3204553,\n",
       "  'pred_soft': 2814,\n",
       "  'soft_val': 0.26749128103256226,\n",
       "  'sim_soft': array(0.26749128, dtype=float32),\n",
       "  'pred_max_max': 2814,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.2476409524679184,\n",
       "  'sim_max_max': 0.3204553},\n",
       " 14097: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2819,\n",
       "  'pred_knn': 2819,\n",
       "  'sim_real_cl': 0.30859986,\n",
       "  'sim_best_knn': 0.30859986,\n",
       "  'pred_soft': 2024,\n",
       "  'soft_val': 0.2679685354232788,\n",
       "  'sim_soft': 0.28628945,\n",
       "  'pred_max_max': 7530,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2801143229007721,\n",
       "  'sim_max_max': 0.28810403},\n",
       " 14121: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2824,\n",
       "  'pred_knn': 4483,\n",
       "  'sim_real_cl': 0.18137182,\n",
       "  'sim_best_knn': 0.33718818,\n",
       "  'pred_soft': 853,\n",
       "  'soft_val': 0.26674777269363403,\n",
       "  'sim_soft': 0.31917357,\n",
       "  'pred_max_max': 2343,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2638282775878906,\n",
       "  'sim_max_max': 0.27126658},\n",
       " 14157: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2831,\n",
       "  'pred_knn': 4775,\n",
       "  'sim_real_cl': 0.21463174,\n",
       "  'sim_best_knn': 0.27346748,\n",
       "  'pred_soft': 2831,\n",
       "  'soft_val': 0.2230217605829239,\n",
       "  'sim_soft': array(0.22302176, dtype=float32),\n",
       "  'pred_max_max': 1610,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.22894585132598877,\n",
       "  'sim_max_max': 0.19199178},\n",
       " 14264: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2852,\n",
       "  'pred_knn': 4532,\n",
       "  'sim_real_cl': 0.31638718,\n",
       "  'sim_best_knn': 0.32030585,\n",
       "  'pred_soft': 2852,\n",
       "  'soft_val': 0.29039767384529114,\n",
       "  'sim_soft': array(0.29039767, dtype=float32),\n",
       "  'pred_max_max': 2852,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3242122232913971,\n",
       "  'sim_max_max': 0.32030585},\n",
       " 14370: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2874,\n",
       "  'pred_knn': 3713,\n",
       "  'sim_real_cl': 0.30822805,\n",
       "  'sim_best_knn': 0.3540587,\n",
       "  'pred_soft': 2874,\n",
       "  'soft_val': 0.2982615828514099,\n",
       "  'sim_soft': array(0.29826158, dtype=float32),\n",
       "  'pred_max_max': 3713,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2877405881881714,\n",
       "  'sim_max_max': 0.3540587},\n",
       " 14393: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2878,\n",
       "  'pred_knn': 6584,\n",
       "  'sim_real_cl': 0.2735458,\n",
       "  'sim_best_knn': 0.3554824,\n",
       "  'pred_soft': 6584,\n",
       "  'soft_val': 0.37871047854423523,\n",
       "  'sim_soft': 0.3554824,\n",
       "  'pred_max_max': 6584,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.39473462104797363,\n",
       "  'sim_max_max': 0.3554824},\n",
       " 14476: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2895,\n",
       "  'pred_knn': 6420,\n",
       "  'sim_real_cl': 0.31417984,\n",
       "  'sim_best_knn': 0.3331316,\n",
       "  'pred_soft': 3497,\n",
       "  'soft_val': 0.24428802728652954,\n",
       "  'sim_soft': 0.26067883,\n",
       "  'pred_max_max': 6420,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2716841697692871,\n",
       "  'sim_max_max': 0.3331316},\n",
       " 14565: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2913,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.20212522,\n",
       "  'sim_best_knn': 0.3277679,\n",
       "  'pred_soft': 5005,\n",
       "  'soft_val': 0.28255316615104675,\n",
       "  'sim_soft': 0.30363765,\n",
       "  'pred_max_max': 5005,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28105252981185913,\n",
       "  'sim_max_max': 0.30363765},\n",
       " 14569: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2913,\n",
       "  'pred_knn': 9949,\n",
       "  'sim_real_cl': 0.17965797,\n",
       "  'sim_best_knn': 0.34888595,\n",
       "  'pred_soft': 9949,\n",
       "  'soft_val': 0.3380044102668762,\n",
       "  'sim_soft': 0.34888595,\n",
       "  'pred_max_max': 9949,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.33697187900543213,\n",
       "  'sim_max_max': 0.34888595},\n",
       " 14583: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2916,\n",
       "  'pred_knn': 4557,\n",
       "  'sim_real_cl': 0.2639815,\n",
       "  'sim_best_knn': 0.26896727,\n",
       "  'pred_soft': 2916,\n",
       "  'soft_val': 0.292018324136734,\n",
       "  'sim_soft': array(0.29201832, dtype=float32),\n",
       "  'pred_max_max': 2916,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.27326369285583496,\n",
       "  'sim_max_max': 0.26896727},\n",
       " 14693: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2938,\n",
       "  'pred_knn': 7283,\n",
       "  'sim_real_cl': 0.30567074,\n",
       "  'sim_best_knn': 0.33219182,\n",
       "  'pred_soft': 8496,\n",
       "  'soft_val': 0.235208198428154,\n",
       "  'sim_soft': 0.23033306,\n",
       "  'pred_max_max': 8496,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.23552638292312622,\n",
       "  'sim_max_max': 0.23033306},\n",
       " 14705: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2941,\n",
       "  'pred_knn': 786,\n",
       "  'sim_real_cl': 0.19970432,\n",
       "  'sim_best_knn': 0.2639044,\n",
       "  'pred_soft': 9085,\n",
       "  'soft_val': 0.22135132551193237,\n",
       "  'sim_soft': 0.2012102,\n",
       "  'pred_max_max': 9507,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25510281324386597,\n",
       "  'sim_max_max': 0.2571021},\n",
       " 14768: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2953,\n",
       "  'pred_knn': 3197,\n",
       "  'sim_real_cl': 0.1752398,\n",
       "  'sim_best_knn': 0.3412434,\n",
       "  'pred_soft': 2801,\n",
       "  'soft_val': 0.2693197429180145,\n",
       "  'sim_soft': 0.28878236,\n",
       "  'pred_max_max': 1503,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2483653426170349,\n",
       "  'sim_max_max': 0.25464153},\n",
       " 14791: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 2958,\n",
       "  'pred_knn': 2958,\n",
       "  'sim_real_cl': 0.29364696,\n",
       "  'sim_best_knn': 0.29364696,\n",
       "  'pred_soft': 4565,\n",
       "  'soft_val': 0.25849980115890503,\n",
       "  'sim_soft': 0.26348653},\n",
       " 14811: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2962,\n",
       "  'pred_knn': 6229,\n",
       "  'sim_real_cl': 0.46086663,\n",
       "  'sim_best_knn': 0.49661404,\n",
       "  'pred_soft': 2962,\n",
       "  'soft_val': 0.3841734230518341,\n",
       "  'sim_soft': array(0.38417342, dtype=float32),\n",
       "  'pred_max_max': 2962,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.4375700056552887,\n",
       "  'sim_max_max': 0.49661404},\n",
       " 14856: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2971,\n",
       "  'pred_knn': 1452,\n",
       "  'sim_real_cl': 0.2554891,\n",
       "  'sim_best_knn': 0.32541454,\n",
       "  'pred_soft': 2971,\n",
       "  'soft_val': 0.28310176730155945,\n",
       "  'sim_soft': array(0.28310177, dtype=float32),\n",
       "  'pred_max_max': 2971,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.2814202606678009,\n",
       "  'sim_max_max': 0.32541454},\n",
       " 14864: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2972,\n",
       "  'pred_knn': 7273,\n",
       "  'sim_real_cl': 0.28505385,\n",
       "  'sim_best_knn': 0.31778163,\n",
       "  'pred_soft': 7273,\n",
       "  'soft_val': 0.3294390141963959,\n",
       "  'sim_soft': 0.31778163,\n",
       "  'pred_max_max': 2972,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3221137523651123,\n",
       "  'sim_max_max': 0.31778163},\n",
       " 14888: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2977,\n",
       "  'pred_knn': 2114,\n",
       "  'sim_real_cl': 0.30898708,\n",
       "  'sim_best_knn': 0.32679513,\n",
       "  'pred_soft': 2977,\n",
       "  'soft_val': 0.32650724053382874,\n",
       "  'sim_soft': array(0.32650724, dtype=float32),\n",
       "  'pred_max_max': 2977,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3512158989906311,\n",
       "  'sim_max_max': 0.32679513},\n",
       " 14981: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 2996,\n",
       "  'pred_knn': 7222,\n",
       "  'sim_real_cl': 0.30228925,\n",
       "  'sim_best_knn': 0.3132351,\n",
       "  'pred_soft': 2996,\n",
       "  'soft_val': 0.300327330827713,\n",
       "  'sim_soft': array(0.30032733, dtype=float32),\n",
       "  'pred_max_max': 2996,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.29770100116729736,\n",
       "  'sim_max_max': 0.3132351},\n",
       " 15001: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3000,\n",
       "  'pred_knn': 3000,\n",
       "  'sim_real_cl': 0.3503969,\n",
       "  'sim_best_knn': 0.3503969,\n",
       "  'pred_soft': 562,\n",
       "  'soft_val': 0.2700946629047394,\n",
       "  'sim_soft': 0.27077132},\n",
       " 15062: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3012,\n",
       "  'pred_knn': 3012,\n",
       "  'sim_real_cl': 0.40498734,\n",
       "  'sim_best_knn': 0.40498734,\n",
       "  'pred_soft': 8746,\n",
       "  'soft_val': 0.24747952818870544,\n",
       "  'sim_soft': 0.31559262},\n",
       " 15120: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3024,\n",
       "  'pred_knn': 918,\n",
       "  'sim_real_cl': 0.36367074,\n",
       "  'sim_best_knn': 0.3987524,\n",
       "  'pred_soft': 918,\n",
       "  'soft_val': 0.3658157289028168,\n",
       "  'sim_soft': 0.3987524,\n",
       "  'pred_max_max': 918,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3587934970855713,\n",
       "  'sim_max_max': 0.3987524},\n",
       " 15121: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3024,\n",
       "  'pred_knn': 918,\n",
       "  'sim_real_cl': 0.3542598,\n",
       "  'sim_best_knn': 0.35989535,\n",
       "  'pred_soft': 918,\n",
       "  'soft_val': 0.3356476426124573,\n",
       "  'sim_soft': 0.35989535,\n",
       "  'pred_max_max': 918,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3104255795478821,\n",
       "  'sim_max_max': 0.35989535},\n",
       " 15124: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3024,\n",
       "  'pred_knn': 8493,\n",
       "  'sim_real_cl': 0.20399086,\n",
       "  'sim_best_knn': 0.33058763,\n",
       "  'pred_soft': 918,\n",
       "  'soft_val': 0.2967591881752014,\n",
       "  'sim_soft': 0.2877313,\n",
       "  'pred_max_max': 8493,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3031380772590637,\n",
       "  'sim_max_max': 0.33058763},\n",
       " 15132: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3026,\n",
       "  'pred_knn': 9574,\n",
       "  'sim_real_cl': 0.23229241,\n",
       "  'sim_best_knn': 0.4192761,\n",
       "  'pred_soft': 2319,\n",
       "  'soft_val': 0.2797262370586395,\n",
       "  'sim_soft': 0.30538222,\n",
       "  'pred_max_max': 9574,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.29396355152130127,\n",
       "  'sim_max_max': 0.4192761},\n",
       " 15142: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3028,\n",
       "  'pred_knn': 3028,\n",
       "  'sim_real_cl': 0.43691993,\n",
       "  'sim_best_knn': 0.43691993,\n",
       "  'pred_soft': 4930,\n",
       "  'soft_val': 0.37438294291496277,\n",
       "  'sim_soft': 0.37831587,\n",
       "  'pred_max_max': 64,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.36808010935783386,\n",
       "  'sim_max_max': 0.42407978},\n",
       " 15157: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3031,\n",
       "  'pred_knn': 3031,\n",
       "  'sim_real_cl': 0.38208652,\n",
       "  'sim_best_knn': 0.38208652,\n",
       "  'pred_soft': 4841,\n",
       "  'soft_val': 0.28258347511291504,\n",
       "  'sim_soft': 0.31981918,\n",
       "  'pred_max_max': 4841,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28331032395362854,\n",
       "  'sim_max_max': 0.31981918},\n",
       " 15178: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3035,\n",
       "  'pred_knn': 3035,\n",
       "  'sim_real_cl': 0.4390872,\n",
       "  'sim_best_knn': 0.4390872,\n",
       "  'pred_soft': 2176,\n",
       "  'soft_val': 0.2667163014411926,\n",
       "  'sim_soft': 0.26821238,\n",
       "  'pred_max_max': 6490,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3039569854736328,\n",
       "  'sim_max_max': 0.30354473},\n",
       " 15180: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3036,\n",
       "  'pred_knn': 3036,\n",
       "  'sim_real_cl': 0.36408317,\n",
       "  'sim_best_knn': 0.36408317,\n",
       "  'pred_soft': 3819,\n",
       "  'soft_val': 0.2791517376899719,\n",
       "  'sim_soft': 0.34173042,\n",
       "  'pred_max_max': 3819,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.315677672624588,\n",
       "  'sim_max_max': 0.34173042},\n",
       " 15182: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3036,\n",
       "  'pred_knn': 3036,\n",
       "  'sim_real_cl': 0.50452626,\n",
       "  'sim_best_knn': 0.50452626,\n",
       "  'pred_soft': 2170,\n",
       "  'soft_val': 0.2533077299594879,\n",
       "  'sim_soft': 0.29962337},\n",
       " 15183: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3036,\n",
       "  'pred_knn': 3036,\n",
       "  'sim_real_cl': 0.4759128,\n",
       "  'sim_best_knn': 0.4759128,\n",
       "  'pred_soft': 5141,\n",
       "  'soft_val': 0.28106728196144104,\n",
       "  'sim_soft': 0.29160506,\n",
       "  'pred_max_max': 5141,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2816779911518097,\n",
       "  'sim_max_max': 0.29160506},\n",
       " 15206: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3041,\n",
       "  'pred_knn': 9969,\n",
       "  'sim_real_cl': 0.24525718,\n",
       "  'sim_best_knn': 0.3279211,\n",
       "  'pred_soft': 930,\n",
       "  'soft_val': 0.2608616352081299,\n",
       "  'sim_soft': 0.28581294,\n",
       "  'pred_max_max': 930,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28364112973213196,\n",
       "  'sim_max_max': 0.28581294},\n",
       " 15249: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3049,\n",
       "  'pred_knn': 3049,\n",
       "  'sim_real_cl': 0.5035988,\n",
       "  'sim_best_knn': 0.5035988,\n",
       "  'pred_soft': 1445,\n",
       "  'soft_val': 0.3262426257133484,\n",
       "  'sim_soft': 0.336959},\n",
       " 15303: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3060,\n",
       "  'pred_knn': 4489,\n",
       "  'sim_real_cl': 0.26861507,\n",
       "  'sim_best_knn': 0.3322159,\n",
       "  'pred_soft': 2018,\n",
       "  'soft_val': 0.2654955983161926,\n",
       "  'sim_soft': 0.31928337,\n",
       "  'pred_max_max': 4489,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2937585115432739,\n",
       "  'sim_max_max': 0.3322159},\n",
       " 15309: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3061,\n",
       "  'pred_knn': 9955,\n",
       "  'sim_real_cl': 0.36420688,\n",
       "  'sim_best_knn': 0.39322013,\n",
       "  'pred_soft': 9955,\n",
       "  'soft_val': 0.322064071893692,\n",
       "  'sim_soft': 0.39322013,\n",
       "  'pred_max_max': 9955,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3539828062057495,\n",
       "  'sim_max_max': 0.39322013},\n",
       " 15332: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3066,\n",
       "  'pred_knn': 9139,\n",
       "  'sim_real_cl': 0.26872122,\n",
       "  'sim_best_knn': 0.33626103,\n",
       "  'pred_soft': 8234,\n",
       "  'soft_val': 0.2546844482421875,\n",
       "  'sim_soft': 0.21091652,\n",
       "  'pred_max_max': 3424,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2700124979019165,\n",
       "  'sim_max_max': 0.28518146},\n",
       " 15395: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3079,\n",
       "  'pred_knn': 4422,\n",
       "  'sim_real_cl': 0.20759866,\n",
       "  'sim_best_knn': 0.33614758,\n",
       "  'pred_soft': 5636,\n",
       "  'soft_val': 0.22254323959350586,\n",
       "  'sim_soft': 0.21944295,\n",
       "  'pred_max_max': 3523,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.22821131348609924,\n",
       "  'sim_max_max': 0.22493446},\n",
       " 15417: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3083,\n",
       "  'pred_knn': 2983,\n",
       "  'sim_real_cl': 0.20903113,\n",
       "  'sim_best_knn': 0.26980916,\n",
       "  'pred_soft': 1346,\n",
       "  'soft_val': 0.2500050961971283,\n",
       "  'sim_soft': 0.2604161,\n",
       "  'pred_max_max': 1346,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.257839173078537,\n",
       "  'sim_max_max': 0.2604161},\n",
       " 15425: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3085,\n",
       "  'pred_knn': 3085,\n",
       "  'sim_real_cl': 0.40276754,\n",
       "  'sim_best_knn': 0.40276754,\n",
       "  'pred_soft': 5939,\n",
       "  'soft_val': 0.2812596559524536,\n",
       "  'sim_soft': 0.30142218},\n",
       " 15473: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3094,\n",
       "  'pred_knn': 1277,\n",
       "  'sim_real_cl': 0.014138211,\n",
       "  'sim_best_knn': 0.34311634,\n",
       "  'pred_soft': 7480,\n",
       "  'soft_val': 0.26001960039138794,\n",
       "  'sim_soft': 0.21015474,\n",
       "  'pred_max_max': 1277,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25693726539611816,\n",
       "  'sim_max_max': 0.34311634},\n",
       " 15514: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3102,\n",
       "  'pred_knn': 3102,\n",
       "  'sim_real_cl': 0.46220368,\n",
       "  'sim_best_knn': 0.46220368,\n",
       "  'pred_soft': 1073,\n",
       "  'soft_val': 0.2772504687309265,\n",
       "  'sim_soft': 0.38314998},\n",
       " 15548: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3109,\n",
       "  'pred_knn': 9794,\n",
       "  'sim_real_cl': 0.1040348,\n",
       "  'sim_best_knn': 0.34558198,\n",
       "  'pred_soft': 5810,\n",
       "  'soft_val': 0.2629329562187195,\n",
       "  'sim_soft': 0.3193384,\n",
       "  'pred_max_max': 5810,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24790161848068237,\n",
       "  'sim_max_max': 0.3193384},\n",
       " 15557: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3111,\n",
       "  'pred_knn': 3111,\n",
       "  'sim_real_cl': 0.4507947,\n",
       "  'sim_best_knn': 0.4507947,\n",
       "  'pred_soft': 4244,\n",
       "  'soft_val': 0.3503456711769104,\n",
       "  'sim_soft': 0.41670442,\n",
       "  'pred_max_max': 4244,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3717711269855499,\n",
       "  'sim_max_max': 0.41670442},\n",
       " 15558: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3111,\n",
       "  'pred_knn': 3039,\n",
       "  'sim_real_cl': 0.31883544,\n",
       "  'sim_best_knn': 0.32261565,\n",
       "  'pred_soft': 3039,\n",
       "  'soft_val': 0.2916510999202728,\n",
       "  'sim_soft': 0.32261565,\n",
       "  'pred_max_max': 8260,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.32249149680137634,\n",
       "  'sim_max_max': 0.30924594},\n",
       " 15561: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3112,\n",
       "  'pred_knn': 3613,\n",
       "  'sim_real_cl': 0.03492943,\n",
       "  'sim_best_knn': 0.30138457,\n",
       "  'pred_soft': 9459,\n",
       "  'soft_val': 0.22481180727481842,\n",
       "  'sim_soft': 0.214753,\n",
       "  'pred_max_max': 8306,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.21502996981143951,\n",
       "  'sim_max_max': 0.24836105},\n",
       " 15567: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3113,\n",
       "  'pred_knn': 2593,\n",
       "  'sim_real_cl': 0.22207937,\n",
       "  'sim_best_knn': 0.31386888,\n",
       "  'pred_soft': 7222,\n",
       "  'soft_val': 0.24959184229373932,\n",
       "  'sim_soft': 0.30012518,\n",
       "  'pred_max_max': 7222,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2769281566143036,\n",
       "  'sim_max_max': 0.30012518},\n",
       " 15615: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3123,\n",
       "  'pred_knn': 3123,\n",
       "  'sim_real_cl': 0.3576606,\n",
       "  'sim_best_knn': 0.3576606,\n",
       "  'pred_soft': 9898,\n",
       "  'soft_val': 0.26839908957481384,\n",
       "  'sim_soft': 0.29140088},\n",
       " 15616: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3123,\n",
       "  'pred_knn': 9136,\n",
       "  'sim_real_cl': 0.14253347,\n",
       "  'sim_best_knn': 0.24632832,\n",
       "  'pred_soft': 7420,\n",
       "  'soft_val': 0.22908258438110352,\n",
       "  'sim_soft': 0.22886707,\n",
       "  'pred_max_max': 7420,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2643643021583557,\n",
       "  'sim_max_max': 0.22886707},\n",
       " 15657: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3131,\n",
       "  'pred_knn': 4007,\n",
       "  'sim_real_cl': 0.3178575,\n",
       "  'sim_best_knn': 0.3589337,\n",
       "  'pred_soft': 4007,\n",
       "  'soft_val': 0.2972865402698517,\n",
       "  'sim_soft': 0.3589337,\n",
       "  'pred_max_max': 4007,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3691120743751526,\n",
       "  'sim_max_max': 0.3589337},\n",
       " 15690: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3138,\n",
       "  'pred_knn': 2680,\n",
       "  'sim_real_cl': 0.3093272,\n",
       "  'sim_best_knn': 0.33944917,\n",
       "  'pred_soft': 3138,\n",
       "  'soft_val': 0.28604400157928467,\n",
       "  'sim_soft': array(0.286044, dtype=float32),\n",
       "  'pred_max_max': 2680,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.31146419048309326,\n",
       "  'sim_max_max': 0.33944917},\n",
       " 15724: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3144,\n",
       "  'pred_knn': 3144,\n",
       "  'sim_real_cl': 0.5609969,\n",
       "  'sim_best_knn': 0.5609969,\n",
       "  'pred_soft': 3931,\n",
       "  'soft_val': 0.3481532335281372,\n",
       "  'sim_soft': 0.41906637},\n",
       " 15764: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3152,\n",
       "  'pred_knn': 6466,\n",
       "  'sim_real_cl': 0.29128262,\n",
       "  'sim_best_knn': 0.35019523,\n",
       "  'pred_soft': 3152,\n",
       "  'soft_val': 0.259371817111969,\n",
       "  'sim_soft': array(0.25937182, dtype=float32),\n",
       "  'pred_max_max': 6387,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28408342599868774,\n",
       "  'sim_max_max': 0.25312144},\n",
       " 15789: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3157,\n",
       "  'pred_knn': 3157,\n",
       "  'sim_real_cl': 0.5007311,\n",
       "  'sim_best_knn': 0.5007311,\n",
       "  'pred_soft': 1076,\n",
       "  'soft_val': 0.41884520649909973,\n",
       "  'sim_soft': 0.44991624},\n",
       " 15822: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3164,\n",
       "  'pred_knn': 3164,\n",
       "  'sim_real_cl': 0.52546597,\n",
       "  'sim_best_knn': 0.52546597,\n",
       "  'pred_soft': 9424,\n",
       "  'soft_val': 0.3159000277519226,\n",
       "  'sim_soft': 0.3762917,\n",
       "  'pred_max_max': 9424,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3961735665798187,\n",
       "  'sim_max_max': 0.3762917},\n",
       " 15824: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3164,\n",
       "  'pred_knn': 3164,\n",
       "  'sim_real_cl': 0.6455319,\n",
       "  'sim_best_knn': 0.6455319,\n",
       "  'pred_soft': 4670,\n",
       "  'soft_val': 0.31729039549827576,\n",
       "  'sim_soft': 0.4017313},\n",
       " 15989: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3197,\n",
       "  'pred_knn': 9794,\n",
       "  'sim_real_cl': 0.2755384,\n",
       "  'sim_best_knn': 0.3571616,\n",
       "  'pred_soft': 2091,\n",
       "  'soft_val': 0.21516640484333038,\n",
       "  'sim_soft': 0.26109445,\n",
       "  'pred_max_max': 2091,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24261026084423065,\n",
       "  'sim_max_max': 0.26109445},\n",
       " 16027: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3205,\n",
       "  'pred_knn': 1141,\n",
       "  'sim_real_cl': 0.27806562,\n",
       "  'sim_best_knn': 0.2929126,\n",
       "  'pred_soft': 3205,\n",
       "  'soft_val': 0.27800610661506653,\n",
       "  'sim_soft': array(0.2780061, dtype=float32),\n",
       "  'pred_max_max': 2805,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26935386657714844,\n",
       "  'sim_max_max': 0.26267856},\n",
       " 16044: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3208,\n",
       "  'pred_knn': 3208,\n",
       "  'sim_real_cl': 0.5733762,\n",
       "  'sim_best_knn': 0.5733762,\n",
       "  'pred_soft': 2315,\n",
       "  'soft_val': 0.2811715602874756,\n",
       "  'sim_soft': 0.31288853},\n",
       " 16083: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3216,\n",
       "  'pred_knn': 3216,\n",
       "  'sim_real_cl': 0.40368262,\n",
       "  'sim_best_knn': 0.40368262,\n",
       "  'pred_soft': 5570,\n",
       "  'soft_val': 0.31317660212516785,\n",
       "  'sim_soft': 0.35056943,\n",
       "  'pred_max_max': 5570,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3149474859237671,\n",
       "  'sim_max_max': 0.35056943},\n",
       " 16102: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3220,\n",
       "  'pred_knn': 3220,\n",
       "  'sim_real_cl': 0.35373336,\n",
       "  'sim_best_knn': 0.35373336,\n",
       "  'pred_soft': 2655,\n",
       "  'soft_val': 0.25803130865097046,\n",
       "  'sim_soft': 0.2452874},\n",
       " 16112: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3222,\n",
       "  'pred_knn': 3222,\n",
       "  'sim_real_cl': 0.5045876,\n",
       "  'sim_best_knn': 0.5045876,\n",
       "  'pred_soft': 2997,\n",
       "  'soft_val': 0.46879127621650696,\n",
       "  'sim_soft': 0.45052004,\n",
       "  'pred_max_max': 2997,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.4529709815979004,\n",
       "  'sim_max_max': 0.45052004},\n",
       " 16168: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3233,\n",
       "  'pred_knn': 3233,\n",
       "  'sim_real_cl': 0.5054751,\n",
       "  'sim_best_knn': 0.5054751,\n",
       "  'pred_soft': 2497,\n",
       "  'soft_val': 0.32430151104927063,\n",
       "  'sim_soft': 0.3671943},\n",
       " 16180: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3236,\n",
       "  'pred_knn': 9200,\n",
       "  'sim_real_cl': 0.2510268,\n",
       "  'sim_best_knn': 0.32843757,\n",
       "  'pred_soft': 3236,\n",
       "  'soft_val': 0.27903878688812256,\n",
       "  'sim_soft': array(0.2790388, dtype=float32),\n",
       "  'pred_max_max': 3096,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2577158510684967,\n",
       "  'sim_max_max': 0.25255176},\n",
       " 16212: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3242,\n",
       "  'pred_knn': 3242,\n",
       "  'sim_real_cl': 0.36934453,\n",
       "  'sim_best_knn': 0.36934453,\n",
       "  'pred_soft': 4000,\n",
       "  'soft_val': 0.3218306601047516,\n",
       "  'sim_soft': 0.3648705},\n",
       " 16244: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3248,\n",
       "  'pred_knn': 3248,\n",
       "  'sim_real_cl': 0.34045902,\n",
       "  'sim_best_knn': 0.34045902,\n",
       "  'pred_soft': 203,\n",
       "  'soft_val': 0.2689216732978821,\n",
       "  'sim_soft': 0.3026145},\n",
       " 16323: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3264,\n",
       "  'pred_knn': 3264,\n",
       "  'sim_real_cl': 0.44076133,\n",
       "  'sim_best_knn': 0.44076133,\n",
       "  'pred_soft': 7803,\n",
       "  'soft_val': 0.32717466354370117,\n",
       "  'sim_soft': 0.35408443},\n",
       " 16324: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3264,\n",
       "  'pred_knn': 3264,\n",
       "  'sim_real_cl': 0.4047134,\n",
       "  'sim_best_knn': 0.4047134,\n",
       "  'pred_soft': 5052,\n",
       "  'soft_val': 0.32865384221076965,\n",
       "  'sim_soft': 0.37541664,\n",
       "  'pred_max_max': 4726,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3165968060493469,\n",
       "  'sim_max_max': 0.31504232},\n",
       " 16338: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3267,\n",
       "  'pred_knn': 3267,\n",
       "  'sim_real_cl': 0.42376736,\n",
       "  'sim_best_knn': 0.42376736,\n",
       "  'pred_soft': 1754,\n",
       "  'soft_val': 0.3023644983768463,\n",
       "  'sim_soft': 0.33901665},\n",
       " 16434: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3286,\n",
       "  'pred_knn': 3286,\n",
       "  'sim_real_cl': 0.38677606,\n",
       "  'sim_best_knn': 0.38677606,\n",
       "  'pred_soft': 6367,\n",
       "  'soft_val': 0.2521853744983673,\n",
       "  'sim_soft': 0.36768925},\n",
       " 16452: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3290,\n",
       "  'pred_knn': 9360,\n",
       "  'sim_real_cl': 0.26385367,\n",
       "  'sim_best_knn': 0.31013268,\n",
       "  'pred_soft': 2991,\n",
       "  'soft_val': 0.281991571187973,\n",
       "  'sim_soft': 0.29949376,\n",
       "  'pred_max_max': 2991,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28051379323005676,\n",
       "  'sim_max_max': 0.29949376},\n",
       " 16517: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3303,\n",
       "  'pred_knn': 3303,\n",
       "  'sim_real_cl': 0.3620284,\n",
       "  'sim_best_knn': 0.3620284,\n",
       "  'pred_soft': 5383,\n",
       "  'soft_val': 0.2642320990562439,\n",
       "  'sim_soft': 0.31966275,\n",
       "  'pred_max_max': 5383,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27279067039489746,\n",
       "  'sim_max_max': 0.31966275},\n",
       " 16520: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3304,\n",
       "  'pred_knn': 3304,\n",
       "  'sim_real_cl': 0.3515054,\n",
       "  'sim_best_knn': 0.3515054,\n",
       "  'pred_soft': 5738,\n",
       "  'soft_val': 0.26836898922920227,\n",
       "  'sim_soft': 0.31400335,\n",
       "  'pred_max_max': 7764,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26484185457229614,\n",
       "  'sim_max_max': 0.29411557},\n",
       " 16553: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3310,\n",
       "  'pred_knn': 8735,\n",
       "  'sim_real_cl': 0.20769578,\n",
       "  'sim_best_knn': 0.37946558,\n",
       "  'pred_soft': 8904,\n",
       "  'soft_val': 0.2995110750198364,\n",
       "  'sim_soft': 0.32775548,\n",
       "  'pred_max_max': 1799,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2761354446411133,\n",
       "  'sim_max_max': 0.24684352},\n",
       " 16578: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3315,\n",
       "  'pred_knn': 3315,\n",
       "  'sim_real_cl': 0.3023721,\n",
       "  'sim_best_knn': 0.3023721,\n",
       "  'pred_soft': 7296,\n",
       "  'soft_val': 0.28995946049690247,\n",
       "  'sim_soft': 0.26156342,\n",
       "  'pred_max_max': 8384,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2881093919277191,\n",
       "  'sim_max_max': 0.27127212},\n",
       " 16623: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3324,\n",
       "  'pred_knn': 3324,\n",
       "  'sim_real_cl': 0.34825683,\n",
       "  'sim_best_knn': 0.34825683,\n",
       "  'pred_soft': 9315,\n",
       "  'soft_val': 0.295886754989624,\n",
       "  'sim_soft': 0.27523518},\n",
       " 16635: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3327,\n",
       "  'pred_knn': 3327,\n",
       "  'sim_real_cl': 0.43002743,\n",
       "  'sim_best_knn': 0.43002743,\n",
       "  'pred_soft': 5593,\n",
       "  'soft_val': 0.2924172282218933,\n",
       "  'sim_soft': 0.3092131},\n",
       " 16638: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3327,\n",
       "  'pred_knn': 1250,\n",
       "  'sim_real_cl': 0.43094957,\n",
       "  'sim_best_knn': 0.44256923,\n",
       "  'pred_soft': 4401,\n",
       "  'soft_val': 0.27225860953330994,\n",
       "  'sim_soft': 0.30499163,\n",
       "  'pred_max_max': 3327,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3108903169631958,\n",
       "  'sim_max_max': 0.44256923},\n",
       " 16655: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3331,\n",
       "  'pred_knn': 1835,\n",
       "  'sim_real_cl': 0.26301962,\n",
       "  'sim_best_knn': 0.31586418,\n",
       "  'pred_soft': 3331,\n",
       "  'soft_val': 0.2649892568588257,\n",
       "  'sim_soft': array(0.26498926, dtype=float32),\n",
       "  'pred_max_max': 2770,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2874191403388977,\n",
       "  'sim_max_max': 0.26685825},\n",
       " 16661: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3332,\n",
       "  'pred_knn': 3332,\n",
       "  'sim_real_cl': 0.38328266,\n",
       "  'sim_best_knn': 0.38328266,\n",
       "  'pred_soft': 1882,\n",
       "  'soft_val': 0.29319342970848083,\n",
       "  'sim_soft': 0.31160688,\n",
       "  'pred_max_max': 1882,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2871943712234497,\n",
       "  'sim_max_max': 0.31160688},\n",
       " 16664: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3332,\n",
       "  'pred_knn': 3332,\n",
       "  'sim_real_cl': 0.39368445,\n",
       "  'sim_best_knn': 0.39368445,\n",
       "  'pred_soft': 9653,\n",
       "  'soft_val': 0.273632675409317,\n",
       "  'sim_soft': 0.26762444,\n",
       "  'pred_max_max': 7420,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28700917959213257,\n",
       "  'sim_max_max': 0.2806307},\n",
       " 16701: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3340,\n",
       "  'pred_knn': 3340,\n",
       "  'sim_real_cl': 0.35399026,\n",
       "  'sim_best_knn': 0.35399026,\n",
       "  'pred_soft': 5515,\n",
       "  'soft_val': 0.27150627970695496,\n",
       "  'sim_soft': 0.28469655},\n",
       " 16702: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3340,\n",
       "  'pred_knn': 3340,\n",
       "  'sim_real_cl': 0.43895912,\n",
       "  'sim_best_knn': 0.43895912,\n",
       "  'pred_soft': 5118,\n",
       "  'soft_val': 0.29371294379234314,\n",
       "  'sim_soft': 0.39215553},\n",
       " 16709: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3341,\n",
       "  'pred_knn': 3341,\n",
       "  'sim_real_cl': 0.35329467,\n",
       "  'sim_best_knn': 0.35329467,\n",
       "  'pred_soft': 5800,\n",
       "  'soft_val': 0.25614914298057556,\n",
       "  'sim_soft': 0.24938448},\n",
       " 16757: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3351,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.1503548,\n",
       "  'sim_best_knn': 0.29488674,\n",
       "  'pred_soft': 2997,\n",
       "  'soft_val': 0.261385977268219,\n",
       "  'sim_soft': 0.23260292,\n",
       "  'pred_max_max': 2997,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28568774461746216,\n",
       "  'sim_max_max': 0.23260292},\n",
       " 16772: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3354,\n",
       "  'pred_knn': 3354,\n",
       "  'sim_real_cl': 0.48426917,\n",
       "  'sim_best_knn': 0.48426917,\n",
       "  'pred_soft': 2319,\n",
       "  'soft_val': 0.31950655579566956,\n",
       "  'sim_soft': 0.31257313},\n",
       " 16891: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3378,\n",
       "  'pred_knn': 9926,\n",
       "  'sim_real_cl': 0.24358757,\n",
       "  'sim_best_knn': 0.3650893,\n",
       "  'pred_soft': 5542,\n",
       "  'soft_val': 0.2899344265460968,\n",
       "  'sim_soft': 0.31823242,\n",
       "  'pred_max_max': 3581,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28311076760292053,\n",
       "  'sim_max_max': 0.2973789},\n",
       " 16892: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3378,\n",
       "  'pred_knn': 4898,\n",
       "  'sim_real_cl': 0.3318736,\n",
       "  'sim_best_knn': 0.34861624,\n",
       "  'pred_soft': 986,\n",
       "  'soft_val': 0.27021822333335876,\n",
       "  'sim_soft': 0.24451643,\n",
       "  'pred_max_max': 886,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2702327370643616,\n",
       "  'sim_max_max': 0.34778914},\n",
       " 16894: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3378,\n",
       "  'pred_knn': 5851,\n",
       "  'sim_real_cl': 0.28637624,\n",
       "  'sim_best_knn': 0.28787968,\n",
       "  'pred_soft': 6785,\n",
       "  'soft_val': 0.23916006088256836,\n",
       "  'sim_soft': 0.25673652,\n",
       "  'pred_max_max': 8731,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25704970955848694,\n",
       "  'sim_max_max': 0.18692356},\n",
       " 16924: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3384,\n",
       "  'pred_knn': 3384,\n",
       "  'sim_real_cl': 0.39294523,\n",
       "  'sim_best_knn': 0.39294523,\n",
       "  'pred_soft': 1898,\n",
       "  'soft_val': 0.29522109031677246,\n",
       "  'sim_soft': 0.308487},\n",
       " 16933: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3386,\n",
       "  'pred_knn': 7870,\n",
       "  'sim_real_cl': 0.221265,\n",
       "  'sim_best_knn': 0.35681474,\n",
       "  'pred_soft': 5368,\n",
       "  'soft_val': 0.2950988709926605,\n",
       "  'sim_soft': 0.2677798,\n",
       "  'pred_max_max': 5368,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2586139738559723,\n",
       "  'sim_max_max': 0.2677798},\n",
       " 16934: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3386,\n",
       "  'pred_knn': 2103,\n",
       "  'sim_real_cl': 0.26434776,\n",
       "  'sim_best_knn': 0.41987896,\n",
       "  'pred_soft': 2103,\n",
       "  'soft_val': 0.28566762804985046,\n",
       "  'sim_soft': 0.41987896,\n",
       "  'pred_max_max': 2103,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.33648523688316345,\n",
       "  'sim_max_max': 0.41987896},\n",
       " 17040: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3408,\n",
       "  'pred_knn': 4557,\n",
       "  'sim_real_cl': 0.19372824,\n",
       "  'sim_best_knn': 0.3131996,\n",
       "  'pred_soft': 2063,\n",
       "  'soft_val': 0.25920218229293823,\n",
       "  'sim_soft': 0.22438896,\n",
       "  'pred_max_max': 4557,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3242090940475464,\n",
       "  'sim_max_max': 0.3131996},\n",
       " 17046: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3409,\n",
       "  'pred_knn': 3931,\n",
       "  'sim_real_cl': 0.3909698,\n",
       "  'sim_best_knn': 0.41554248,\n",
       "  'pred_soft': 3931,\n",
       "  'soft_val': 0.36418628692626953,\n",
       "  'sim_soft': 0.41554248,\n",
       "  'pred_max_max': 3931,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.368256151676178,\n",
       "  'sim_max_max': 0.41554248},\n",
       " 17052: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3410,\n",
       "  'pred_knn': 3410,\n",
       "  'sim_real_cl': 0.43709815,\n",
       "  'sim_best_knn': 0.43709815,\n",
       "  'pred_soft': 8034,\n",
       "  'soft_val': 0.306615948677063,\n",
       "  'sim_soft': 0.31294787},\n",
       " 17136: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3427,\n",
       "  'pred_knn': 3427,\n",
       "  'sim_real_cl': 0.35918623,\n",
       "  'sim_best_knn': 0.35918623,\n",
       "  'pred_soft': 2309,\n",
       "  'soft_val': 0.23414954543113708,\n",
       "  'sim_soft': 0.25305074},\n",
       " 17139: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3427,\n",
       "  'pred_knn': 3427,\n",
       "  'sim_real_cl': 0.35903555,\n",
       "  'sim_best_knn': 0.35903555,\n",
       "  'pred_soft': 4425,\n",
       "  'soft_val': 0.2424851655960083,\n",
       "  'sim_soft': 0.2909823},\n",
       " 17165: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3433,\n",
       "  'pred_knn': 3433,\n",
       "  'sim_real_cl': 0.5919688,\n",
       "  'sim_best_knn': 0.5919688,\n",
       "  'pred_soft': 742,\n",
       "  'soft_val': 0.28691011667251587,\n",
       "  'sim_soft': 0.24317728},\n",
       " 17188: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3437,\n",
       "  'pred_knn': 3303,\n",
       "  'sim_real_cl': 0.30927318,\n",
       "  'sim_best_knn': 0.37495583,\n",
       "  'pred_soft': 9992,\n",
       "  'soft_val': 0.23684963583946228,\n",
       "  'sim_soft': 0.29587966,\n",
       "  'pred_max_max': 5303,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3016088306903839,\n",
       "  'sim_max_max': 0.33619565},\n",
       " 17198: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3439,\n",
       "  'pred_knn': 186,\n",
       "  'sim_real_cl': 0.29227525,\n",
       "  'sim_best_knn': 0.30812442,\n",
       "  'pred_soft': 2344,\n",
       "  'soft_val': 0.2624810039997101,\n",
       "  'sim_soft': 0.28205746,\n",
       "  'pred_max_max': 7487,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25600510835647583,\n",
       "  'sim_max_max': 0.2651207},\n",
       " 17224: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3444,\n",
       "  'pred_knn': 3444,\n",
       "  'sim_real_cl': 0.607484,\n",
       "  'sim_best_knn': 0.607484,\n",
       "  'pred_soft': 5820,\n",
       "  'soft_val': 0.3248341679573059,\n",
       "  'sim_soft': 0.33939725},\n",
       " 17248: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3449,\n",
       "  'pred_knn': 3449,\n",
       "  'sim_real_cl': 0.3989334,\n",
       "  'sim_best_knn': 0.3989334,\n",
       "  'pred_soft': 5676,\n",
       "  'soft_val': 0.266786128282547,\n",
       "  'sim_soft': 0.2729519},\n",
       " 17267: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3453,\n",
       "  'pred_knn': 1445,\n",
       "  'sim_real_cl': 0.29259866,\n",
       "  'sim_best_knn': 0.2927906,\n",
       "  'pred_soft': 3183,\n",
       "  'soft_val': 0.23095625638961792,\n",
       "  'sim_soft': 0.26326936,\n",
       "  'pred_max_max': 1445,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2687070071697235,\n",
       "  'sim_max_max': 0.2927906},\n",
       " 17274: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3454,\n",
       "  'pred_knn': 3454,\n",
       "  'sim_real_cl': 0.40020463,\n",
       "  'sim_best_knn': 0.40020463,\n",
       "  'pred_soft': 1866,\n",
       "  'soft_val': 0.2620173692703247,\n",
       "  'sim_soft': 0.2378995},\n",
       " 17342: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3468,\n",
       "  'pred_knn': 8561,\n",
       "  'sim_real_cl': 0.3388989,\n",
       "  'sim_best_knn': 0.44403195,\n",
       "  'pred_soft': 7573,\n",
       "  'soft_val': 0.2775457799434662,\n",
       "  'sim_soft': 0.34278327,\n",
       "  'pred_max_max': 7573,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28329917788505554,\n",
       "  'sim_max_max': 0.34278327},\n",
       " 17347: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3469,\n",
       "  'pred_knn': 3469,\n",
       "  'sim_real_cl': 0.34255692,\n",
       "  'sim_best_knn': 0.34255692,\n",
       "  'pred_soft': 4867,\n",
       "  'soft_val': 0.24206385016441345,\n",
       "  'sim_soft': 0.22684053},\n",
       " 17425: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3485,\n",
       "  'pred_knn': 3048,\n",
       "  'sim_real_cl': 0.17583512,\n",
       "  'sim_best_knn': 0.30596676,\n",
       "  'pred_soft': 7863,\n",
       "  'soft_val': 0.2394474595785141,\n",
       "  'sim_soft': 0.2890486,\n",
       "  'pred_max_max': 7863,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26503124833106995,\n",
       "  'sim_max_max': 0.2890486},\n",
       " 17429: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3485,\n",
       "  'pred_knn': 9835,\n",
       "  'sim_real_cl': 0.18198888,\n",
       "  'sim_best_knn': 0.30158472,\n",
       "  'pred_soft': 9835,\n",
       "  'soft_val': 0.2563716173171997,\n",
       "  'sim_soft': 0.30158472,\n",
       "  'pred_max_max': 4627,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27367985248565674,\n",
       "  'sim_max_max': 0.24444672},\n",
       " 17467: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3493,\n",
       "  'pred_knn': 3493,\n",
       "  'sim_real_cl': 0.3569228,\n",
       "  'sim_best_knn': 0.3569228,\n",
       "  'pred_soft': 606,\n",
       "  'soft_val': 0.3091162443161011,\n",
       "  'sim_soft': 0.31829342,\n",
       "  'pred_max_max': 9524,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.31350651383399963,\n",
       "  'sim_max_max': 0.31944394},\n",
       " 17468: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3493,\n",
       "  'pred_knn': 3493,\n",
       "  'sim_real_cl': 0.5556954,\n",
       "  'sim_best_knn': 0.5556954,\n",
       "  'pred_soft': 7542,\n",
       "  'soft_val': 0.3921178877353668,\n",
       "  'sim_soft': 0.4178871,\n",
       "  'pred_max_max': 7542,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.42353326082229614,\n",
       "  'sim_max_max': 0.4178871},\n",
       " 17470: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3494,\n",
       "  'pred_knn': 5408,\n",
       "  'sim_real_cl': 0.10955407,\n",
       "  'sim_best_knn': 0.2508006,\n",
       "  'pred_soft': 4029,\n",
       "  'soft_val': 0.24411243200302124,\n",
       "  'sim_soft': 0.2064611,\n",
       "  'pred_max_max': 5408,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2586096525192261,\n",
       "  'sim_max_max': 0.2508006},\n",
       " 17481: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3496,\n",
       "  'pred_knn': 1981,\n",
       "  'sim_real_cl': -0.083287135,\n",
       "  'sim_best_knn': 0.21948761,\n",
       "  'pred_soft': 8550,\n",
       "  'soft_val': 0.1996697038412094,\n",
       "  'sim_soft': 0.2113796,\n",
       "  'pred_max_max': 8550,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.21023035049438477,\n",
       "  'sim_max_max': 0.2113796},\n",
       " 17541: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3508,\n",
       "  'pred_knn': 3508,\n",
       "  'sim_real_cl': 0.46717733,\n",
       "  'sim_best_knn': 0.46717733,\n",
       "  'pred_soft': 7936,\n",
       "  'soft_val': 0.2910924553871155,\n",
       "  'sim_soft': 0.3466273},\n",
       " 17587: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3517,\n",
       "  'pred_knn': 3517,\n",
       "  'sim_real_cl': 0.4483294,\n",
       "  'sim_best_knn': 0.4483294,\n",
       "  'pred_soft': 6588,\n",
       "  'soft_val': 0.2644746005535126,\n",
       "  'sim_soft': 0.24483807,\n",
       "  'pred_max_max': 4244,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28751999139785767,\n",
       "  'sim_max_max': 0.29852462},\n",
       " 17589: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3517,\n",
       "  'pred_knn': 3517,\n",
       "  'sim_real_cl': 0.50417846,\n",
       "  'sim_best_knn': 0.50417846,\n",
       "  'pred_soft': 6742,\n",
       "  'soft_val': 0.25909703969955444,\n",
       "  'sim_soft': 0.30566376},\n",
       " 17619: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3523,\n",
       "  'pred_knn': 7193,\n",
       "  'sim_real_cl': 0.2478517,\n",
       "  'sim_best_knn': 0.3224249,\n",
       "  'pred_soft': 6159,\n",
       "  'soft_val': 0.2491108477115631,\n",
       "  'sim_soft': 0.3184901,\n",
       "  'pred_max_max': 2246,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2915426790714264,\n",
       "  'sim_max_max': 0.32055002},\n",
       " 17631: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3526,\n",
       "  'pred_knn': 3526,\n",
       "  'sim_real_cl': 0.38684493,\n",
       "  'sim_best_knn': 0.38684493,\n",
       "  'pred_soft': 2587,\n",
       "  'soft_val': 0.29580986499786377,\n",
       "  'sim_soft': 0.3595537},\n",
       " 17677: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3535,\n",
       "  'pred_knn': 7787,\n",
       "  'sim_real_cl': 0.2772646,\n",
       "  'sim_best_knn': 0.32388604,\n",
       "  'pred_soft': 3535,\n",
       "  'soft_val': 0.2551519572734833,\n",
       "  'sim_soft': array(0.25515196, dtype=float32),\n",
       "  'pred_max_max': 3535,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.2754998207092285,\n",
       "  'sim_max_max': 0.32388604},\n",
       " 17703: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3540,\n",
       "  'pred_knn': 4743,\n",
       "  'sim_real_cl': 0.30776143,\n",
       "  'sim_best_knn': 0.34979504,\n",
       "  'pred_soft': 4743,\n",
       "  'soft_val': 0.28671208024024963,\n",
       "  'sim_soft': 0.34979504,\n",
       "  'pred_max_max': 4743,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.32779815793037415,\n",
       "  'sim_max_max': 0.34979504},\n",
       " 17778: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3555,\n",
       "  'pred_knn': 62,\n",
       "  'sim_real_cl': 0.29168507,\n",
       "  'sim_best_knn': 0.35965216,\n",
       "  'pred_soft': 3555,\n",
       "  'soft_val': 0.2294609248638153,\n",
       "  'sim_soft': array(0.22946092, dtype=float32),\n",
       "  'pred_max_max': 8168,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2669266164302826,\n",
       "  'sim_max_max': 0.26073506},\n",
       " 17789: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3557,\n",
       "  'pred_knn': 3557,\n",
       "  'sim_real_cl': 0.31555462,\n",
       "  'sim_best_knn': 0.31555462,\n",
       "  'pred_soft': 4627,\n",
       "  'soft_val': 0.26950332522392273,\n",
       "  'sim_soft': 0.2800395,\n",
       "  'pred_max_max': 4627,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2837899625301361,\n",
       "  'sim_max_max': 0.2800395},\n",
       " 17877: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3575,\n",
       "  'pred_knn': 3575,\n",
       "  'sim_real_cl': 0.40656096,\n",
       "  'sim_best_knn': 0.40656096,\n",
       "  'pred_soft': 5448,\n",
       "  'soft_val': 0.26019373536109924,\n",
       "  'sim_soft': 0.2549764},\n",
       " 17927: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3585,\n",
       "  'pred_knn': 9474,\n",
       "  'sim_real_cl': 0.30255264,\n",
       "  'sim_best_knn': 0.3454162,\n",
       "  'pred_soft': 9474,\n",
       "  'soft_val': 0.25442588329315186,\n",
       "  'sim_soft': 0.3454162,\n",
       "  'pred_max_max': 9474,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28405335545539856,\n",
       "  'sim_max_max': 0.3454162},\n",
       " 17944: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3588,\n",
       "  'pred_knn': 3588,\n",
       "  'sim_real_cl': 0.35951728,\n",
       "  'sim_best_knn': 0.35951728,\n",
       "  'pred_soft': 6726,\n",
       "  'soft_val': 0.26758676767349243,\n",
       "  'sim_soft': 0.31919295,\n",
       "  'pred_max_max': 3951,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2680720090866089,\n",
       "  'sim_max_max': 0.29100192},\n",
       " 17959: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3591,\n",
       "  'pred_knn': 3591,\n",
       "  'sim_real_cl': 0.39702672,\n",
       "  'sim_best_knn': 0.39702672,\n",
       "  'pred_soft': 8851,\n",
       "  'soft_val': 0.3135709762573242,\n",
       "  'sim_soft': 0.31929046,\n",
       "  'pred_max_max': 1862,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2981197237968445,\n",
       "  'sim_max_max': 0.30546078},\n",
       " 18044: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3608,\n",
       "  'pred_knn': 3608,\n",
       "  'sim_real_cl': 0.3542971,\n",
       "  'sim_best_knn': 0.3542971,\n",
       "  'pred_soft': 859,\n",
       "  'soft_val': 0.2758542597293854,\n",
       "  'sim_soft': 0.28199047},\n",
       " 18062: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3612,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.20873903,\n",
       "  'sim_best_knn': 0.4386685,\n",
       "  'pred_soft': 8844,\n",
       "  'soft_val': 0.2614750862121582,\n",
       "  'sim_soft': 0.30914515,\n",
       "  'pred_max_max': 8844,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27638763189315796,\n",
       "  'sim_max_max': 0.30914515},\n",
       " 18064: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3612,\n",
       "  'pred_knn': 9837,\n",
       "  'sim_real_cl': 0.36801755,\n",
       "  'sim_best_knn': 0.3881187,\n",
       "  'pred_soft': 8844,\n",
       "  'soft_val': 0.266785204410553,\n",
       "  'sim_soft': 0.3226986,\n",
       "  'pred_max_max': 579,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3026707172393799,\n",
       "  'sim_max_max': 0.38268143},\n",
       " 18101: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3620,\n",
       "  'pred_knn': 1933,\n",
       "  'sim_real_cl': 0.27824926,\n",
       "  'sim_best_knn': 0.2835514,\n",
       "  'pred_soft': 3620,\n",
       "  'soft_val': 0.2711479663848877,\n",
       "  'sim_soft': array(0.27114797, dtype=float32),\n",
       "  'pred_max_max': 1933,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2904149293899536,\n",
       "  'sim_max_max': 0.2835514},\n",
       " 18102: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3620,\n",
       "  'pred_knn': 1933,\n",
       "  'sim_real_cl': 0.2729613,\n",
       "  'sim_best_knn': 0.34145874,\n",
       "  'pred_soft': 1933,\n",
       "  'soft_val': 0.2989664375782013,\n",
       "  'sim_soft': 0.34145874,\n",
       "  'pred_max_max': 1933,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.33577996492385864,\n",
       "  'sim_max_max': 0.34145874},\n",
       " 18152: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3630,\n",
       "  'pred_knn': 1963,\n",
       "  'sim_real_cl': 0.31942445,\n",
       "  'sim_best_knn': 0.35196212,\n",
       "  'pred_soft': 1963,\n",
       "  'soft_val': 0.2961674928665161,\n",
       "  'sim_soft': 0.35196212,\n",
       "  'pred_max_max': 1963,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3139939308166504,\n",
       "  'sim_max_max': 0.35196212},\n",
       " 18157: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3631,\n",
       "  'pred_knn': 6683,\n",
       "  'sim_real_cl': 0.23943552,\n",
       "  'sim_best_knn': 0.24635811,\n",
       "  'pred_soft': 3631,\n",
       "  'soft_val': 0.23455622792243958,\n",
       "  'sim_soft': array(0.23455623, dtype=float32),\n",
       "  'pred_max_max': 9282,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.23666790127754211,\n",
       "  'sim_max_max': 0.21391052},\n",
       " 18168: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3633,\n",
       "  'pred_knn': 2231,\n",
       "  'sim_real_cl': 0.143925,\n",
       "  'sim_best_knn': 0.26899272,\n",
       "  'pred_soft': 6092,\n",
       "  'soft_val': 0.24529512226581573,\n",
       "  'sim_soft': 0.24196362,\n",
       "  'pred_max_max': 6092,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27406415343284607,\n",
       "  'sim_max_max': 0.24196362},\n",
       " 18198: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3639,\n",
       "  'pred_knn': 3835,\n",
       "  'sim_real_cl': 0.3465281,\n",
       "  'sim_best_knn': 0.377248,\n",
       "  'pred_soft': 867,\n",
       "  'soft_val': 0.35318171977996826,\n",
       "  'sim_soft': 0.3405545,\n",
       "  'pred_max_max': 867,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.34362101554870605,\n",
       "  'sim_max_max': 0.3405545},\n",
       " 18266: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3653,\n",
       "  'pred_knn': 3653,\n",
       "  'sim_real_cl': 0.57105017,\n",
       "  'sim_best_knn': 0.57105017,\n",
       "  'pred_soft': 5368,\n",
       "  'soft_val': 0.3140544295310974,\n",
       "  'sim_soft': 0.3485002},\n",
       " 18297: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3659,\n",
       "  'pred_knn': 12,\n",
       "  'sim_real_cl': 0.30305374,\n",
       "  'sim_best_knn': 0.3271253,\n",
       "  'pred_soft': 3659,\n",
       "  'soft_val': 0.3055895268917084,\n",
       "  'sim_soft': array(0.30558953, dtype=float32),\n",
       "  'pred_max_max': 12,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30236589908599854,\n",
       "  'sim_max_max': 0.3271253},\n",
       " 18301: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3660,\n",
       "  'pred_knn': 1250,\n",
       "  'sim_real_cl': 0.30483192,\n",
       "  'sim_best_knn': 0.4104663,\n",
       "  'pred_soft': 3660,\n",
       "  'soft_val': 0.38291800022125244,\n",
       "  'sim_soft': array(0.382918, dtype=float32),\n",
       "  'pred_max_max': 3660,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3105127513408661,\n",
       "  'sim_max_max': 0.4104663},\n",
       " 18313: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3662,\n",
       "  'pred_knn': 4459,\n",
       "  'sim_real_cl': 0.2682068,\n",
       "  'sim_best_knn': 0.31324914,\n",
       "  'pred_soft': 9219,\n",
       "  'soft_val': 0.24462270736694336,\n",
       "  'sim_soft': 0.2520916,\n",
       "  'pred_max_max': 3662,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.31272318959236145,\n",
       "  'sim_max_max': 0.31324914},\n",
       " 18356: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3671,\n",
       "  'pred_knn': 8609,\n",
       "  'sim_real_cl': 0.30790207,\n",
       "  'sim_best_knn': 0.30882037,\n",
       "  'pred_soft': 6932,\n",
       "  'soft_val': 0.2650853097438812,\n",
       "  'sim_soft': 0.25993407,\n",
       "  'pred_max_max': 3184,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2463633418083191,\n",
       "  'sim_max_max': 0.2704122},\n",
       " 18455: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3691,\n",
       "  'pred_knn': 3691,\n",
       "  'sim_real_cl': 0.58474237,\n",
       "  'sim_best_knn': 0.58474237,\n",
       "  'pred_soft': 2501,\n",
       "  'soft_val': 0.42721477150917053,\n",
       "  'sim_soft': 0.51849365},\n",
       " 18474: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3694,\n",
       "  'pred_knn': 3694,\n",
       "  'sim_real_cl': 0.36668333,\n",
       "  'sim_best_knn': 0.36668333,\n",
       "  'pred_soft': 2923,\n",
       "  'soft_val': 0.3044041395187378,\n",
       "  'sim_soft': 0.2766574},\n",
       " 18495: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3699,\n",
       "  'pred_knn': 3699,\n",
       "  'sim_real_cl': 0.4216623,\n",
       "  'sim_best_knn': 0.4216623,\n",
       "  'pred_soft': 2148,\n",
       "  'soft_val': 0.2535565197467804,\n",
       "  'sim_soft': 0.25368667,\n",
       "  'pred_max_max': 1399,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28858858346939087,\n",
       "  'sim_max_max': 0.3352281},\n",
       " 18502: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3700,\n",
       "  'pred_knn': 5515,\n",
       "  'sim_real_cl': 0.46258754,\n",
       "  'sim_best_knn': 0.49284625,\n",
       "  'pred_soft': 5515,\n",
       "  'soft_val': 0.4587480127811432,\n",
       "  'sim_soft': 0.49284625,\n",
       "  'pred_max_max': 3700,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.4569759964942932,\n",
       "  'sim_max_max': 0.49284625},\n",
       " 18526: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3705,\n",
       "  'pred_knn': 3685,\n",
       "  'sim_real_cl': 0.2996756,\n",
       "  'sim_best_knn': 0.45632586,\n",
       "  'pred_soft': 5561,\n",
       "  'soft_val': 0.36972174048423767,\n",
       "  'sim_soft': 0.4385287,\n",
       "  'pred_max_max': 5561,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.40168267488479614,\n",
       "  'sim_max_max': 0.4385287},\n",
       " 18528: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3705,\n",
       "  'pred_knn': 3705,\n",
       "  'sim_real_cl': 0.445485,\n",
       "  'sim_best_knn': 0.445485,\n",
       "  'pred_soft': 2354,\n",
       "  'soft_val': 0.3500101864337921,\n",
       "  'sim_soft': 0.3953805,\n",
       "  'pred_max_max': 6480,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.360528826713562,\n",
       "  'sim_max_max': 0.3418998},\n",
       " 18530: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3706,\n",
       "  'pred_knn': 3706,\n",
       "  'sim_real_cl': 0.4129691,\n",
       "  'sim_best_knn': 0.4129691,\n",
       "  'pred_soft': 2963,\n",
       "  'soft_val': 0.24073012173175812,\n",
       "  'sim_soft': 0.26326782},\n",
       " 18534: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3706,\n",
       "  'pred_knn': 3706,\n",
       "  'sim_real_cl': 0.4701079,\n",
       "  'sim_best_knn': 0.4701079,\n",
       "  'pred_soft': 9769,\n",
       "  'soft_val': 0.2401048094034195,\n",
       "  'sim_soft': 0.32886523},\n",
       " 18539: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3707,\n",
       "  'pred_knn': 3707,\n",
       "  'sim_real_cl': 0.37502164,\n",
       "  'sim_best_knn': 0.37502164,\n",
       "  'pred_soft': 2316,\n",
       "  'soft_val': 0.2654007375240326,\n",
       "  'sim_soft': 0.27756864},\n",
       " 18649: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3729,\n",
       "  'pred_knn': 9794,\n",
       "  'sim_real_cl': 0.25255063,\n",
       "  'sim_best_knn': 0.3467114,\n",
       "  'pred_soft': 3506,\n",
       "  'soft_val': 0.2721846103668213,\n",
       "  'sim_soft': 0.28883827,\n",
       "  'pred_max_max': 3506,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2535221576690674,\n",
       "  'sim_max_max': 0.28883827},\n",
       " 18758: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3751,\n",
       "  'pred_knn': 3751,\n",
       "  'sim_real_cl': 0.42051476,\n",
       "  'sim_best_knn': 0.42051476,\n",
       "  'pred_soft': 7809,\n",
       "  'soft_val': 0.3185475766658783,\n",
       "  'sim_soft': 0.30828947},\n",
       " 18775: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3755,\n",
       "  'pred_knn': 7870,\n",
       "  'sim_real_cl': 0.34124517,\n",
       "  'sim_best_knn': 0.38050342,\n",
       "  'pred_soft': 3755,\n",
       "  'soft_val': 0.3134748935699463,\n",
       "  'sim_soft': array(0.3134749, dtype=float32),\n",
       "  'pred_max_max': 3755,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.34932801127433777,\n",
       "  'sim_max_max': 0.38050342},\n",
       " 18776: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3755,\n",
       "  'pred_knn': 2725,\n",
       "  'sim_real_cl': 0.2544526,\n",
       "  'sim_best_knn': 0.30511993,\n",
       "  'pred_soft': 1476,\n",
       "  'soft_val': 0.2369932234287262,\n",
       "  'sim_soft': 0.26113033,\n",
       "  'pred_max_max': 9167,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.271467387676239,\n",
       "  'sim_max_max': 0.23515138},\n",
       " 18788: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3757,\n",
       "  'pred_knn': 3757,\n",
       "  'sim_real_cl': 0.57893705,\n",
       "  'sim_best_knn': 0.57893705,\n",
       "  'pred_soft': 9496,\n",
       "  'soft_val': 0.3371051251888275,\n",
       "  'sim_soft': 0.3566535},\n",
       " 18826: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3765,\n",
       "  'pred_knn': 2581,\n",
       "  'sim_real_cl': 0.26892054,\n",
       "  'sim_best_knn': 0.28843942,\n",
       "  'pred_soft': 2581,\n",
       "  'soft_val': 0.29142504930496216,\n",
       "  'sim_soft': 0.28843942,\n",
       "  'pred_max_max': 2581,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25048360228538513,\n",
       "  'sim_max_max': 0.28843942},\n",
       " 18827: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3765,\n",
       "  'pred_knn': 5484,\n",
       "  'sim_real_cl': 0.17368717,\n",
       "  'sim_best_knn': 0.32781422,\n",
       "  'pred_soft': 5356,\n",
       "  'soft_val': 0.2260635495185852,\n",
       "  'sim_soft': 0.2725821,\n",
       "  'pred_max_max': 6320,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24032381176948547,\n",
       "  'sim_max_max': 0.24534321},\n",
       " 18887: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3777,\n",
       "  'pred_knn': 6570,\n",
       "  'sim_real_cl': 0.18969789,\n",
       "  'sim_best_knn': 0.25332975,\n",
       "  'pred_soft': 42,\n",
       "  'soft_val': 0.23860737681388855,\n",
       "  'sim_soft': 0.13876833,\n",
       "  'pred_max_max': 6570,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2498205453157425,\n",
       "  'sim_max_max': 0.25332975},\n",
       " 18914: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3782,\n",
       "  'pred_knn': 3782,\n",
       "  'sim_real_cl': 0.4776625,\n",
       "  'sim_best_knn': 0.4776625,\n",
       "  'pred_soft': 5493,\n",
       "  'soft_val': 0.32458677887916565,\n",
       "  'sim_soft': 0.41412234},\n",
       " 18926: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3785,\n",
       "  'pred_knn': 3785,\n",
       "  'sim_real_cl': 0.30555642,\n",
       "  'sim_best_knn': 0.30555642,\n",
       "  'pred_soft': 7681,\n",
       "  'soft_val': 0.23272471129894257,\n",
       "  'sim_soft': 0.2891686,\n",
       "  'pred_max_max': 7681,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27762892842292786,\n",
       "  'sim_max_max': 0.2891686},\n",
       " 18932: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3786,\n",
       "  'pred_knn': 5918,\n",
       "  'sim_real_cl': 0.23251578,\n",
       "  'sim_best_knn': 0.29712656,\n",
       "  'pred_soft': 4931,\n",
       "  'soft_val': 0.23704186081886292,\n",
       "  'sim_soft': 0.24702573,\n",
       "  'pred_max_max': 4931,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2703412175178528,\n",
       "  'sim_max_max': 0.24702573},\n",
       " 18972: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3794,\n",
       "  'pred_knn': 742,\n",
       "  'sim_real_cl': 0.31693873,\n",
       "  'sim_best_knn': 0.31888792,\n",
       "  'pred_soft': 742,\n",
       "  'soft_val': 0.27615034580230713,\n",
       "  'sim_soft': 0.31888792,\n",
       "  'pred_max_max': 742,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2888570725917816,\n",
       "  'sim_max_max': 0.31888792},\n",
       " 19012: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3802,\n",
       "  'pred_knn': 3802,\n",
       "  'sim_real_cl': 0.48589325,\n",
       "  'sim_best_knn': 0.48589325,\n",
       "  'pred_soft': 6788,\n",
       "  'soft_val': 0.3980424106121063,\n",
       "  'sim_soft': 0.42129648},\n",
       " 19013: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3802,\n",
       "  'pred_knn': 3802,\n",
       "  'sim_real_cl': 0.4124924,\n",
       "  'sim_best_knn': 0.4124924,\n",
       "  'pred_soft': 6788,\n",
       "  'soft_val': 0.33863362669944763,\n",
       "  'sim_soft': 0.3837799},\n",
       " 19076: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3815,\n",
       "  'pred_knn': 3815,\n",
       "  'sim_real_cl': 0.4255623,\n",
       "  'sim_best_knn': 0.4255623,\n",
       "  'pred_soft': 4207,\n",
       "  'soft_val': 0.28454336524009705,\n",
       "  'sim_soft': 0.2762918},\n",
       " 19102: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3820,\n",
       "  'pred_knn': 1201,\n",
       "  'sim_real_cl': 0.34527707,\n",
       "  'sim_best_knn': 0.36993366,\n",
       "  'pred_soft': 3820,\n",
       "  'soft_val': 0.3173643946647644,\n",
       "  'sim_soft': array(0.3173644, dtype=float32),\n",
       "  'pred_max_max': 3820,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3407018780708313,\n",
       "  'sim_max_max': 0.36993366},\n",
       " 19118: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3823,\n",
       "  'pred_knn': 3823,\n",
       "  'sim_real_cl': 0.3740618,\n",
       "  'sim_best_knn': 0.3740618,\n",
       "  'pred_soft': 5556,\n",
       "  'soft_val': 0.3438495099544525,\n",
       "  'sim_soft': 0.31223464,\n",
       "  'pred_max_max': 5556,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3250678777694702,\n",
       "  'sim_max_max': 0.31223464},\n",
       " 19144: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3828,\n",
       "  'pred_knn': 9574,\n",
       "  'sim_real_cl': 0.38475692,\n",
       "  'sim_best_knn': 0.38962403,\n",
       "  'pred_soft': 3828,\n",
       "  'soft_val': 0.3935515284538269,\n",
       "  'sim_soft': array(0.39355153, dtype=float32),\n",
       "  'pred_max_max': 3828,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.38043269515037537,\n",
       "  'sim_max_max': 0.38962403},\n",
       " 19154: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3830,\n",
       "  'pred_knn': 3294,\n",
       "  'sim_real_cl': 0.050441936,\n",
       "  'sim_best_knn': 0.30925012,\n",
       "  'pred_soft': 9458,\n",
       "  'soft_val': 0.26408833265304565,\n",
       "  'sim_soft': 0.26316997,\n",
       "  'pred_max_max': 3294,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2543136775493622,\n",
       "  'sim_max_max': 0.30925012},\n",
       " 19228: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3845,\n",
       "  'pred_knn': 3845,\n",
       "  'sim_real_cl': 0.47894844,\n",
       "  'sim_best_knn': 0.47894844,\n",
       "  'pred_soft': 6241,\n",
       "  'soft_val': 0.3473946154117584,\n",
       "  'sim_soft': 0.40205395,\n",
       "  'pred_max_max': 6241,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.35399991273880005,\n",
       "  'sim_max_max': 0.40205395},\n",
       " 19341: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3868,\n",
       "  'pred_knn': 3868,\n",
       "  'sim_real_cl': 0.46619046,\n",
       "  'sim_best_knn': 0.46619046,\n",
       "  'pred_soft': 5999,\n",
       "  'soft_val': 0.3311089277267456,\n",
       "  'sim_soft': 0.38382268},\n",
       " 19352: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3870,\n",
       "  'pred_knn': 8263,\n",
       "  'sim_real_cl': 0.3326062,\n",
       "  'sim_best_knn': 0.3681618,\n",
       "  'pred_soft': 3870,\n",
       "  'soft_val': 0.4425603449344635,\n",
       "  'sim_soft': array(0.44256034, dtype=float32),\n",
       "  'pred_max_max': 3870,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3854145407676697,\n",
       "  'sim_max_max': 0.3681618},\n",
       " 19373: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3874,\n",
       "  'pred_knn': 1743,\n",
       "  'sim_real_cl': 0.22998808,\n",
       "  'sim_best_knn': 0.33857837,\n",
       "  'pred_soft': 1610,\n",
       "  'soft_val': 0.2872146964073181,\n",
       "  'sim_soft': 0.2797972,\n",
       "  'pred_max_max': 1610,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2826364040374756,\n",
       "  'sim_max_max': 0.2797972},\n",
       " 19400: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3880,\n",
       "  'pred_knn': 5480,\n",
       "  'sim_real_cl': 0.38467014,\n",
       "  'sim_best_knn': 0.4189089,\n",
       "  'pred_soft': 5480,\n",
       "  'soft_val': 0.3332488238811493,\n",
       "  'sim_soft': 0.4189089,\n",
       "  'pred_max_max': 3880,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.30981263518333435,\n",
       "  'sim_max_max': 0.4189089},\n",
       " 19563: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3912,\n",
       "  'pred_knn': 3912,\n",
       "  'sim_real_cl': 0.33182117,\n",
       "  'sim_best_knn': 0.33182117,\n",
       "  'pred_soft': 8798,\n",
       "  'soft_val': 0.23866230249404907,\n",
       "  'sim_soft': 0.28489992,\n",
       "  'pred_max_max': 8354,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28244030475616455,\n",
       "  'sim_max_max': 0.26211423},\n",
       " 19723: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3944,\n",
       "  'pred_knn': 1270,\n",
       "  'sim_real_cl': 0.16162074,\n",
       "  'sim_best_knn': 0.33917633,\n",
       "  'pred_soft': 5848,\n",
       "  'soft_val': 0.2634679675102234,\n",
       "  'sim_soft': 0.29170376,\n",
       "  'pred_max_max': 2063,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2715691924095154,\n",
       "  'sim_max_max': 0.25512695},\n",
       " 19821: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3964,\n",
       "  'pred_knn': 4057,\n",
       "  'sim_real_cl': 0.33847672,\n",
       "  'sim_best_knn': 0.34984076,\n",
       "  'pred_soft': 4057,\n",
       "  'soft_val': 0.3117380738258362,\n",
       "  'sim_soft': 0.34984076,\n",
       "  'pred_max_max': 4057,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30778178572654724,\n",
       "  'sim_max_max': 0.34984076},\n",
       " 19823: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 3964,\n",
       "  'pred_knn': 3964,\n",
       "  'sim_real_cl': 0.33831227,\n",
       "  'sim_best_knn': 0.33831227,\n",
       "  'pred_soft': 9663,\n",
       "  'soft_val': 0.28200244903564453,\n",
       "  'sim_soft': 0.28943956,\n",
       "  'pred_max_max': 9663,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2638154625892639,\n",
       "  'sim_max_max': 0.28943956},\n",
       " 19842: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3968,\n",
       "  'pred_knn': 6205,\n",
       "  'sim_real_cl': 0.27790096,\n",
       "  'sim_best_knn': 0.3729003,\n",
       "  'pred_soft': 5923,\n",
       "  'soft_val': 0.262933611869812,\n",
       "  'sim_soft': 0.31487468,\n",
       "  'pred_max_max': 6205,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2807025909423828,\n",
       "  'sim_max_max': 0.3729003},\n",
       " 19878: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3975,\n",
       "  'pred_knn': 691,\n",
       "  'sim_real_cl': 0.23394987,\n",
       "  'sim_best_knn': 0.31783134,\n",
       "  'pred_soft': 691,\n",
       "  'soft_val': 0.3222188651561737,\n",
       "  'sim_soft': 0.31783134,\n",
       "  'pred_max_max': 691,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2784292697906494,\n",
       "  'sim_max_max': 0.31783134},\n",
       " 19884: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3976,\n",
       "  'pred_knn': 9731,\n",
       "  'sim_real_cl': 0.40830404,\n",
       "  'sim_best_knn': 0.43579257,\n",
       "  'pred_soft': 3976,\n",
       "  'soft_val': 0.4779724180698395,\n",
       "  'sim_soft': array(0.47797242, dtype=float32),\n",
       "  'pred_max_max': 3976,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.46373388171195984,\n",
       "  'sim_max_max': 0.43579257},\n",
       " 19888: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3977,\n",
       "  'pred_knn': 639,\n",
       "  'sim_real_cl': 0.20052683,\n",
       "  'sim_best_knn': 0.43171105,\n",
       "  'pred_soft': 2990,\n",
       "  'soft_val': 0.2269420027732849,\n",
       "  'sim_soft': 0.20843524,\n",
       "  'pred_max_max': 4562,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25382542610168457,\n",
       "  'sim_max_max': 0.3622524},\n",
       " 19896: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3979,\n",
       "  'pred_knn': 1014,\n",
       "  'sim_real_cl': 0.31242052,\n",
       "  'sim_best_knn': 0.3969326,\n",
       "  'pred_soft': 3979,\n",
       "  'soft_val': 0.32044413685798645,\n",
       "  'sim_soft': array(0.32044414, dtype=float32),\n",
       "  'pred_max_max': 3979,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.36703673005104065,\n",
       "  'sim_max_max': 0.3969326},\n",
       " 19918: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3983,\n",
       "  'pred_knn': 6723,\n",
       "  'sim_real_cl': 0.10640319,\n",
       "  'sim_best_knn': 0.2652968,\n",
       "  'pred_soft': 2763,\n",
       "  'soft_val': 0.24470725655555725,\n",
       "  'sim_soft': 0.24360837,\n",
       "  'pred_max_max': 3752,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26041409373283386,\n",
       "  'sim_max_max': 0.24249975},\n",
       " 19943: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 3988,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.22183459,\n",
       "  'sim_best_knn': 0.304958,\n",
       "  'pred_soft': 8525,\n",
       "  'soft_val': 0.22140252590179443,\n",
       "  'sim_soft': 0.2590882,\n",
       "  'pred_max_max': 4438,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2466905266046524,\n",
       "  'sim_max_max': 0.2664804},\n",
       " 20045: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4009,\n",
       "  'pred_knn': 4009,\n",
       "  'sim_real_cl': 0.41312853,\n",
       "  'sim_best_knn': 0.41312853,\n",
       "  'pred_soft': 6729,\n",
       "  'soft_val': 0.2797079086303711,\n",
       "  'sim_soft': 0.34492305},\n",
       " 20078: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4015,\n",
       "  'pred_knn': 894,\n",
       "  'sim_real_cl': 0.12669623,\n",
       "  'sim_best_knn': 0.26852778,\n",
       "  'pred_soft': 894,\n",
       "  'soft_val': 0.2380608767271042,\n",
       "  'sim_soft': 0.26852778,\n",
       "  'pred_max_max': 894,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24432167410850525,\n",
       "  'sim_max_max': 0.26852778},\n",
       " 20090: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4018,\n",
       "  'pred_knn': 9537,\n",
       "  'sim_real_cl': 0.2638144,\n",
       "  'sim_best_knn': 0.3029062,\n",
       "  'pred_soft': 607,\n",
       "  'soft_val': 0.24465778470039368,\n",
       "  'sim_soft': 0.2797212,\n",
       "  'pred_max_max': 607,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2859874665737152,\n",
       "  'sim_max_max': 0.2797212},\n",
       " 20185: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4037,\n",
       "  'pred_knn': 5342,\n",
       "  'sim_real_cl': 0.2530144,\n",
       "  'sim_best_knn': 0.29472214,\n",
       "  'pred_soft': 3952,\n",
       "  'soft_val': 0.2506519854068756,\n",
       "  'sim_soft': 0.25787795,\n",
       "  'pred_max_max': 1862,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24363520741462708,\n",
       "  'sim_max_max': 0.26597822},\n",
       " 20189: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4037,\n",
       "  'pred_knn': 1016,\n",
       "  'sim_real_cl': 0.2641933,\n",
       "  'sim_best_knn': 0.31277192,\n",
       "  'pred_soft': 5417,\n",
       "  'soft_val': 0.27383187413215637,\n",
       "  'sim_soft': 0.28100166,\n",
       "  'pred_max_max': 3620,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2695121169090271,\n",
       "  'sim_max_max': 0.275693},\n",
       " 20227: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4045,\n",
       "  'pred_knn': 4045,\n",
       "  'sim_real_cl': 0.35075793,\n",
       "  'sim_best_knn': 0.35075793,\n",
       "  'pred_soft': 2481,\n",
       "  'soft_val': 0.2864517271518707,\n",
       "  'sim_soft': 0.31837413},\n",
       " 20239: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4047,\n",
       "  'pred_knn': 853,\n",
       "  'sim_real_cl': 0.13867056,\n",
       "  'sim_best_knn': 0.2987849,\n",
       "  'pred_soft': 853,\n",
       "  'soft_val': 0.2858303487300873,\n",
       "  'sim_soft': 0.2987849,\n",
       "  'pred_max_max': 9209,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25243112444877625,\n",
       "  'sim_max_max': 0.22215304},\n",
       " 20289: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4057,\n",
       "  'pred_knn': 4057,\n",
       "  'sim_real_cl': 0.425418,\n",
       "  'sim_best_knn': 0.425418,\n",
       "  'pred_soft': 8572,\n",
       "  'soft_val': 0.27496638894081116,\n",
       "  'sim_soft': 0.28818333},\n",
       " 20302: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4060,\n",
       "  'pred_knn': 4060,\n",
       "  'sim_real_cl': 0.3937989,\n",
       "  'sim_best_knn': 0.3937989,\n",
       "  'pred_soft': 7644,\n",
       "  'soft_val': 0.27482983469963074,\n",
       "  'sim_soft': 0.31524518},\n",
       " 20316: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4063,\n",
       "  'pred_knn': 4063,\n",
       "  'sim_real_cl': 0.4623094,\n",
       "  'sim_best_knn': 0.4623094,\n",
       "  'pred_soft': 2881,\n",
       "  'soft_val': 0.29975903034210205,\n",
       "  'sim_soft': 0.31314465},\n",
       " 20340: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4068,\n",
       "  'pred_knn': 6427,\n",
       "  'sim_real_cl': 0.25051823,\n",
       "  'sim_best_knn': 0.26481074,\n",
       "  'pred_soft': 2476,\n",
       "  'soft_val': 0.23236028850078583,\n",
       "  'sim_soft': 0.25057858,\n",
       "  'pred_max_max': 4068,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.2502274215221405,\n",
       "  'sim_max_max': 0.26481074},\n",
       " 20367: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4073,\n",
       "  'pred_knn': 4073,\n",
       "  'sim_real_cl': 0.35920462,\n",
       "  'sim_best_knn': 0.35920462,\n",
       "  'pred_soft': 2867,\n",
       "  'soft_val': 0.27320602536201477,\n",
       "  'sim_soft': 0.26591617},\n",
       " 20394: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4078,\n",
       "  'pred_knn': 5631,\n",
       "  'sim_real_cl': 0.29537714,\n",
       "  'sim_best_knn': 0.3203466,\n",
       "  'pred_soft': 5631,\n",
       "  'soft_val': 0.2690321207046509,\n",
       "  'sim_soft': 0.3203466,\n",
       "  'pred_max_max': 5631,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2751155197620392,\n",
       "  'sim_max_max': 0.3203466},\n",
       " 20421: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4084,\n",
       "  'pred_knn': 4084,\n",
       "  'sim_real_cl': 0.49086618,\n",
       "  'sim_best_knn': 0.49086618,\n",
       "  'pred_soft': 909,\n",
       "  'soft_val': 0.2514236867427826,\n",
       "  'sim_soft': 0.28139457},\n",
       " 20463: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4092,\n",
       "  'pred_knn': 4092,\n",
       "  'sim_real_cl': 0.5058651,\n",
       "  'sim_best_knn': 0.5058651,\n",
       "  'pred_soft': 9138,\n",
       "  'soft_val': 0.4132475256919861,\n",
       "  'sim_soft': 0.43676275,\n",
       "  'pred_max_max': 9138,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.4507635831832886,\n",
       "  'sim_max_max': 0.43676275},\n",
       " 20467: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4093,\n",
       "  'pred_knn': 7037,\n",
       "  'sim_real_cl': 0.13186948,\n",
       "  'sim_best_knn': 0.27669513,\n",
       "  'pred_soft': 2373,\n",
       "  'soft_val': 0.2129666805267334,\n",
       "  'sim_soft': 0.20874459,\n",
       "  'pred_max_max': 4683,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.22171561419963837,\n",
       "  'sim_max_max': 0.24108875},\n",
       " 20524: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4104,\n",
       "  'pred_knn': 4104,\n",
       "  'sim_real_cl': 0.32675815,\n",
       "  'sim_best_knn': 0.32675815,\n",
       "  'pred_soft': 5191,\n",
       "  'soft_val': 0.23473499715328217,\n",
       "  'sim_soft': 0.23253167},\n",
       " 20543: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4108,\n",
       "  'pred_knn': 8582,\n",
       "  'sim_real_cl': 0.229711,\n",
       "  'sim_best_knn': 0.2885541,\n",
       "  'pred_soft': 2395,\n",
       "  'soft_val': 0.2645157277584076,\n",
       "  'sim_soft': 0.25765628,\n",
       "  'pred_max_max': 8582,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27381187677383423,\n",
       "  'sim_max_max': 0.2885541},\n",
       " 20563: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4112,\n",
       "  'pred_knn': 4112,\n",
       "  'sim_real_cl': 0.40831876,\n",
       "  'sim_best_knn': 0.40831876,\n",
       "  'pred_soft': 5556,\n",
       "  'soft_val': 0.2815634310245514,\n",
       "  'sim_soft': 0.28911185},\n",
       " 20585: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4117,\n",
       "  'pred_knn': 4117,\n",
       "  'sim_real_cl': 0.46677893,\n",
       "  'sim_best_knn': 0.46677893,\n",
       "  'pred_soft': 5820,\n",
       "  'soft_val': 0.3188234269618988,\n",
       "  'sim_soft': 0.3432135},\n",
       " 20591: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4118,\n",
       "  'pred_knn': 4118,\n",
       "  'sim_real_cl': 0.5360196,\n",
       "  'sim_best_knn': 0.5360196,\n",
       "  'pred_soft': 5770,\n",
       "  'soft_val': 0.295198529958725,\n",
       "  'sim_soft': 0.3301385},\n",
       " 20592: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4118,\n",
       "  'pred_knn': 4118,\n",
       "  'sim_real_cl': 0.5611099,\n",
       "  'sim_best_knn': 0.5611099,\n",
       "  'pred_soft': 6648,\n",
       "  'soft_val': 0.25997135043144226,\n",
       "  'sim_soft': 0.29162657},\n",
       " 20599: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4119,\n",
       "  'pred_knn': 3024,\n",
       "  'sim_real_cl': 0.30672485,\n",
       "  'sim_best_knn': 0.37169865,\n",
       "  'pred_soft': 8709,\n",
       "  'soft_val': 0.2544131577014923,\n",
       "  'sim_soft': 0.32688004,\n",
       "  'pred_max_max': 8259,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27121734619140625,\n",
       "  'sim_max_max': 0.30928254},\n",
       " 20614: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4122,\n",
       "  'pred_knn': 2858,\n",
       "  'sim_real_cl': 0.08968119,\n",
       "  'sim_best_knn': 0.32565418,\n",
       "  'pred_soft': 2858,\n",
       "  'soft_val': 0.2803741991519928,\n",
       "  'sim_soft': 0.32565418,\n",
       "  'pred_max_max': 2858,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.281735897064209,\n",
       "  'sim_max_max': 0.32565418},\n",
       " 20679: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4135,\n",
       "  'pred_knn': 4135,\n",
       "  'sim_real_cl': 0.49333206,\n",
       "  'sim_best_knn': 0.49333206,\n",
       "  'pred_soft': 8517,\n",
       "  'soft_val': 0.26472511887550354,\n",
       "  'sim_soft': 0.26807743},\n",
       " 20692: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4138,\n",
       "  'pred_knn': 4138,\n",
       "  'sim_real_cl': 0.5121912,\n",
       "  'sim_best_knn': 0.5121912,\n",
       "  'pred_soft': 2496,\n",
       "  'soft_val': 0.2699156105518341,\n",
       "  'sim_soft': 0.2727738},\n",
       " 20693: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4138,\n",
       "  'pred_knn': 7584,\n",
       "  'sim_real_cl': 0.012214635,\n",
       "  'sim_best_knn': 0.26440403,\n",
       "  'pred_soft': 2316,\n",
       "  'soft_val': 0.2102838009595871,\n",
       "  'sim_soft': 0.20277765,\n",
       "  'pred_max_max': 8826,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.22516530752182007,\n",
       "  'sim_max_max': 0.18723726},\n",
       " 20698: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4139,\n",
       "  'pred_knn': 4139,\n",
       "  'sim_real_cl': 0.2940391,\n",
       "  'sim_best_knn': 0.2940391,\n",
       "  'pred_soft': 912,\n",
       "  'soft_val': 0.2832716107368469,\n",
       "  'sim_soft': 0.27769625,\n",
       "  'pred_max_max': 7361,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28210750222206116,\n",
       "  'sim_max_max': 0.24813822},\n",
       " 20699: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4139,\n",
       "  'pred_knn': 4139,\n",
       "  'sim_real_cl': 0.3058501,\n",
       "  'sim_best_knn': 0.3058501,\n",
       "  'pred_soft': 5004,\n",
       "  'soft_val': 0.317634254693985,\n",
       "  'sim_soft': 0.30130303},\n",
       " 20700: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4140,\n",
       "  'pred_knn': 2829,\n",
       "  'sim_real_cl': 0.15164529,\n",
       "  'sim_best_knn': 0.2976213,\n",
       "  'pred_soft': 2829,\n",
       "  'soft_val': 0.27028965950012207,\n",
       "  'sim_soft': 0.2976213,\n",
       "  'pred_max_max': 2829,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2754353880882263,\n",
       "  'sim_max_max': 0.2976213},\n",
       " 20704: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4140,\n",
       "  'pred_knn': 5848,\n",
       "  'sim_real_cl': 0.03782279,\n",
       "  'sim_best_knn': 0.24057838,\n",
       "  'pred_soft': 6092,\n",
       "  'soft_val': 0.22556211054325104,\n",
       "  'sim_soft': 0.21283397,\n",
       "  'pred_max_max': 2059,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24882599711418152,\n",
       "  'sim_max_max': 0.22202022},\n",
       " 20733: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4146,\n",
       "  'pred_knn': 4146,\n",
       "  'sim_real_cl': 0.35710463,\n",
       "  'sim_best_knn': 0.35710463,\n",
       "  'pred_soft': 8801,\n",
       "  'soft_val': 0.30616408586502075,\n",
       "  'sim_soft': 0.3332597,\n",
       "  'pred_max_max': 8801,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2972642481327057,\n",
       "  'sim_max_max': 0.3332597},\n",
       " 20778: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4155,\n",
       "  'pred_knn': 8786,\n",
       "  'sim_real_cl': 0.3762977,\n",
       "  'sim_best_knn': 0.45451516,\n",
       "  'pred_soft': 8885,\n",
       "  'soft_val': 0.3027622699737549,\n",
       "  'sim_soft': 0.35653526,\n",
       "  'pred_max_max': 8885,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.36380138993263245,\n",
       "  'sim_max_max': 0.35653526},\n",
       " 20804: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4160,\n",
       "  'pred_knn': 4160,\n",
       "  'sim_real_cl': 0.49049586,\n",
       "  'sim_best_knn': 0.49049586,\n",
       "  'pred_soft': 9678,\n",
       "  'soft_val': 0.3034321963787079,\n",
       "  'sim_soft': 0.37254995,\n",
       "  'pred_max_max': 9678,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3679790198802948,\n",
       "  'sim_max_max': 0.37254995},\n",
       " 20808: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4161,\n",
       "  'pred_knn': 4161,\n",
       "  'sim_real_cl': 0.49351323,\n",
       "  'sim_best_knn': 0.49351323,\n",
       "  'pred_soft': 1721,\n",
       "  'soft_val': 0.27858439087867737,\n",
       "  'sim_soft': 0.29842412},\n",
       " 20814: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4162,\n",
       "  'pred_knn': 4162,\n",
       "  'sim_real_cl': 0.34533373,\n",
       "  'sim_best_knn': 0.34533373,\n",
       "  'pred_soft': 4866,\n",
       "  'soft_val': 0.27574971318244934,\n",
       "  'sim_soft': 0.31080157},\n",
       " 20899: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4179,\n",
       "  'pred_knn': 4179,\n",
       "  'sim_real_cl': 0.5181758,\n",
       "  'sim_best_knn': 0.5181758,\n",
       "  'pred_soft': 1697,\n",
       "  'soft_val': 0.29365479946136475,\n",
       "  'sim_soft': 0.40618932},\n",
       " 20922: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4184,\n",
       "  'pred_knn': 4184,\n",
       "  'sim_real_cl': 0.3534354,\n",
       "  'sim_best_knn': 0.3534354,\n",
       "  'pred_soft': 7797,\n",
       "  'soft_val': 0.22136946022510529,\n",
       "  'sim_soft': 0.23146734,\n",
       "  'pred_max_max': 3518,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24835585057735443,\n",
       "  'sim_max_max': 0.28295475},\n",
       " 20924: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4184,\n",
       "  'pred_knn': 3038,\n",
       "  'sim_real_cl': 0.37431806,\n",
       "  'sim_best_knn': 0.40323314,\n",
       "  'pred_soft': 4829,\n",
       "  'soft_val': 0.25856077671051025,\n",
       "  'sim_soft': 0.28154486,\n",
       "  'pred_max_max': 4184,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.32236671447753906,\n",
       "  'sim_max_max': 0.40323314},\n",
       " 20937: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4187,\n",
       "  'pred_knn': 6569,\n",
       "  'sim_real_cl': 0.13630939,\n",
       "  'sim_best_knn': 0.27498585,\n",
       "  'pred_soft': 1444,\n",
       "  'soft_val': 0.25809958577156067,\n",
       "  'sim_soft': 0.24364507,\n",
       "  'pred_max_max': 1444,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.21836397051811218,\n",
       "  'sim_max_max': 0.24364507},\n",
       " 20951: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4190,\n",
       "  'pred_knn': 4190,\n",
       "  'sim_real_cl': 0.34209433,\n",
       "  'sim_best_knn': 0.34209433,\n",
       "  'pred_soft': 6624,\n",
       "  'soft_val': 0.29500046372413635,\n",
       "  'sim_soft': 0.2626909},\n",
       " 20956: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4191,\n",
       "  'pred_knn': 4191,\n",
       "  'sim_real_cl': 0.46137506,\n",
       "  'sim_best_knn': 0.46137506,\n",
       "  'pred_soft': 2829,\n",
       "  'soft_val': 0.2557871639728546,\n",
       "  'sim_soft': 0.2868162},\n",
       " 20974: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4194,\n",
       "  'pred_knn': 4194,\n",
       "  'sim_real_cl': 0.48478943,\n",
       "  'sim_best_knn': 0.48478943,\n",
       "  'pred_soft': 2484,\n",
       "  'soft_val': 0.36502453684806824,\n",
       "  'sim_soft': 0.3740648},\n",
       " 21028: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4205,\n",
       "  'pred_knn': 4205,\n",
       "  'sim_real_cl': 0.46277463,\n",
       "  'sim_best_knn': 0.46277463,\n",
       "  'pred_soft': 8828,\n",
       "  'soft_val': 0.26963090896606445,\n",
       "  'sim_soft': 0.2968536},\n",
       " 21037: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4207,\n",
       "  'pred_knn': 4360,\n",
       "  'sim_real_cl': 0.29351857,\n",
       "  'sim_best_knn': 0.30751655,\n",
       "  'pred_soft': 4207,\n",
       "  'soft_val': 0.2861574590206146,\n",
       "  'sim_soft': array(0.28615746, dtype=float32),\n",
       "  'pred_max_max': 4207,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.2835001051425934,\n",
       "  'sim_max_max': 0.30751655},\n",
       " 21067: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4213,\n",
       "  'pred_knn': 7630,\n",
       "  'sim_real_cl': 0.277862,\n",
       "  'sim_best_knn': 0.3014865,\n",
       "  'pred_soft': 2316,\n",
       "  'soft_val': 0.2483004331588745,\n",
       "  'sim_soft': 0.23888493,\n",
       "  'pred_max_max': 1443,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25847432017326355,\n",
       "  'sim_max_max': 0.2533055},\n",
       " 21138: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4227,\n",
       "  'pred_knn': 183,\n",
       "  'sim_real_cl': 0.25625587,\n",
       "  'sim_best_knn': 0.35391593,\n",
       "  'pred_soft': 8985,\n",
       "  'soft_val': 0.26783719658851624,\n",
       "  'sim_soft': 0.2956251,\n",
       "  'pred_max_max': 3242,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.31860387325286865,\n",
       "  'sim_max_max': 0.34130067},\n",
       " 21158: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4231,\n",
       "  'pred_knn': 7455,\n",
       "  'sim_real_cl': 0.31055272,\n",
       "  'sim_best_knn': 0.39323628,\n",
       "  'pred_soft': 8693,\n",
       "  'soft_val': 0.28357329964637756,\n",
       "  'sim_soft': 0.33535522,\n",
       "  'pred_max_max': 8801,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26966577768325806,\n",
       "  'sim_max_max': 0.29285762},\n",
       " 21243: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4248,\n",
       "  'pred_knn': 6127,\n",
       "  'sim_real_cl': 0.40775007,\n",
       "  'sim_best_knn': 0.45772052,\n",
       "  'pred_soft': 6127,\n",
       "  'soft_val': 0.2879788875579834,\n",
       "  'sim_soft': 0.45772052,\n",
       "  'pred_max_max': 6127,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3743472993373871,\n",
       "  'sim_max_max': 0.45772052},\n",
       " 21316: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4263,\n",
       "  'pred_knn': 4263,\n",
       "  'sim_real_cl': 0.576291,\n",
       "  'sim_best_knn': 0.576291,\n",
       "  'pred_soft': 4196,\n",
       "  'soft_val': 0.3001076579093933,\n",
       "  'sim_soft': 0.40397513},\n",
       " 21317: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4263,\n",
       "  'pred_knn': 4263,\n",
       "  'sim_real_cl': 0.4823063,\n",
       "  'sim_best_knn': 0.4823063,\n",
       "  'pred_soft': 2886,\n",
       "  'soft_val': 0.2827056050300598,\n",
       "  'sim_soft': 0.24134907},\n",
       " 21369: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4273,\n",
       "  'pred_knn': 8414,\n",
       "  'sim_real_cl': 0.17998719,\n",
       "  'sim_best_knn': 0.30565506,\n",
       "  'pred_soft': 2691,\n",
       "  'soft_val': 0.27766066789627075,\n",
       "  'sim_soft': 0.2788481,\n",
       "  'pred_max_max': 2691,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26841869950294495,\n",
       "  'sim_max_max': 0.2788481},\n",
       " 21389: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4277,\n",
       "  'pred_knn': 5525,\n",
       "  'sim_real_cl': 0.32688254,\n",
       "  'sim_best_knn': 0.33763927,\n",
       "  'pred_soft': 5525,\n",
       "  'soft_val': 0.30363428592681885,\n",
       "  'sim_soft': 0.33763927,\n",
       "  'pred_max_max': 5525,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3161645531654358,\n",
       "  'sim_max_max': 0.33763927},\n",
       " 21420: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4284,\n",
       "  'pred_knn': 4284,\n",
       "  'sim_real_cl': 0.528005,\n",
       "  'sim_best_knn': 0.528005,\n",
       "  'pred_soft': 2923,\n",
       "  'soft_val': 0.3116750121116638,\n",
       "  'sim_soft': 0.32516086},\n",
       " 21422: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4284,\n",
       "  'pred_knn': 4284,\n",
       "  'sim_real_cl': 0.33624274,\n",
       "  'sim_best_knn': 0.33624274,\n",
       "  'pred_soft': 6194,\n",
       "  'soft_val': 0.2658330798149109,\n",
       "  'sim_soft': 0.27832675,\n",
       "  'pred_max_max': 6194,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2496279925107956,\n",
       "  'sim_max_max': 0.27832675},\n",
       " 21424: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4284,\n",
       "  'pred_knn': 4284,\n",
       "  'sim_real_cl': 0.31016338,\n",
       "  'sim_best_knn': 0.31016338,\n",
       "  'pred_soft': 5629,\n",
       "  'soft_val': 0.24844804406166077,\n",
       "  'sim_soft': 0.27116227,\n",
       "  'pred_max_max': 9683,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2487977147102356,\n",
       "  'sim_max_max': 0.26296675},\n",
       " 21459: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4291,\n",
       "  'pred_knn': 4291,\n",
       "  'sim_real_cl': 0.37568858,\n",
       "  'sim_best_knn': 0.37568858,\n",
       "  'pred_soft': 2197,\n",
       "  'soft_val': 0.33890336751937866,\n",
       "  'sim_soft': 0.3355737},\n",
       " 21478: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4295,\n",
       "  'pred_knn': 4295,\n",
       "  'sim_real_cl': 0.33395475,\n",
       "  'sim_best_knn': 0.33395475,\n",
       "  'pred_soft': 2852,\n",
       "  'soft_val': 0.26964402198791504,\n",
       "  'sim_soft': 0.27019987},\n",
       " 21481: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4296,\n",
       "  'pred_knn': 5092,\n",
       "  'sim_real_cl': 0.27425534,\n",
       "  'sim_best_knn': 0.32798922,\n",
       "  'pred_soft': 9317,\n",
       "  'soft_val': 0.26707062125205994,\n",
       "  'sim_soft': 0.32703114,\n",
       "  'pred_max_max': 9317,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30241358280181885,\n",
       "  'sim_max_max': 0.32703114},\n",
       " 21535: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4307,\n",
       "  'pred_knn': 4307,\n",
       "  'sim_real_cl': 0.40920722,\n",
       "  'sim_best_knn': 0.40920722,\n",
       "  'pred_soft': 116,\n",
       "  'soft_val': 0.2125004678964615,\n",
       "  'sim_soft': 0.24350013},\n",
       " 21536: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4307,\n",
       "  'pred_knn': 4307,\n",
       "  'sim_real_cl': 0.42883688,\n",
       "  'sim_best_knn': 0.42883688,\n",
       "  'pred_soft': 5972,\n",
       "  'soft_val': 0.24674095213413239,\n",
       "  'sim_soft': 0.26483822},\n",
       " 21537: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4307,\n",
       "  'pred_knn': 4307,\n",
       "  'sim_real_cl': 0.4147225,\n",
       "  'sim_best_knn': 0.4147225,\n",
       "  'pred_soft': 5972,\n",
       "  'soft_val': 0.22106266021728516,\n",
       "  'sim_soft': 0.21601753},\n",
       " 21646: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4329,\n",
       "  'pred_knn': 4329,\n",
       "  'sim_real_cl': 0.3632689,\n",
       "  'sim_best_knn': 0.3632689,\n",
       "  'pred_soft': 4882,\n",
       "  'soft_val': 0.28827473521232605,\n",
       "  'sim_soft': 0.3324865},\n",
       " 21654: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4330,\n",
       "  'pred_knn': 4330,\n",
       "  'sim_real_cl': 0.3388161,\n",
       "  'sim_best_knn': 0.3388161,\n",
       "  'pred_soft': 8605,\n",
       "  'soft_val': 0.25136566162109375,\n",
       "  'sim_soft': 0.26352763,\n",
       "  'pred_max_max': 9579,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2570701539516449,\n",
       "  'sim_max_max': 0.28400815},\n",
       " 21722: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4344,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.33653438,\n",
       "  'sim_best_knn': 0.38419423,\n",
       "  'pred_soft': 1097,\n",
       "  'soft_val': 0.30436643958091736,\n",
       "  'sim_soft': 0.2610611,\n",
       "  'pred_max_max': 8026,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2598627507686615,\n",
       "  'sim_max_max': 0.3189112},\n",
       " 21724: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4344,\n",
       "  'pred_knn': 4344,\n",
       "  'sim_real_cl': 0.46170345,\n",
       "  'sim_best_knn': 0.46170345,\n",
       "  'pred_soft': 2593,\n",
       "  'soft_val': 0.2260824739933014,\n",
       "  'sim_soft': 0.33645165},\n",
       " 21753: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4350,\n",
       "  'pred_knn': 9549,\n",
       "  'sim_real_cl': 0.29199544,\n",
       "  'sim_best_knn': 0.34556153,\n",
       "  'pred_soft': 5686,\n",
       "  'soft_val': 0.28051066398620605,\n",
       "  'sim_soft': 0.32455558,\n",
       "  'pred_max_max': 4350,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.28684306144714355,\n",
       "  'sim_max_max': 0.34556153},\n",
       " 21883: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4376,\n",
       "  'pred_knn': 1008,\n",
       "  'sim_real_cl': 0.21362525,\n",
       "  'sim_best_knn': 0.37163988,\n",
       "  'pred_soft': 4789,\n",
       "  'soft_val': 0.3186528980731964,\n",
       "  'sim_soft': 0.36416,\n",
       "  'pred_max_max': 1008,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30669084191322327,\n",
       "  'sim_max_max': 0.37163988},\n",
       " 21934: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4386,\n",
       "  'pred_knn': 9197,\n",
       "  'sim_real_cl': 0.33269235,\n",
       "  'sim_best_knn': 0.34694588,\n",
       "  'pred_soft': 5560,\n",
       "  'soft_val': 0.26300516724586487,\n",
       "  'sim_soft': 0.23450583,\n",
       "  'pred_max_max': 4386,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.31668519973754883,\n",
       "  'sim_max_max': 0.34694588},\n",
       " 21970: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4394,\n",
       "  'pred_knn': 4394,\n",
       "  'sim_real_cl': 0.3835597,\n",
       "  'sim_best_knn': 0.3835597,\n",
       "  'pred_soft': 7608,\n",
       "  'soft_val': 0.283593088388443,\n",
       "  'sim_soft': 0.34161168},\n",
       " 21973: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4394,\n",
       "  'pred_knn': 9136,\n",
       "  'sim_real_cl': 0.23406474,\n",
       "  'sim_best_knn': 0.32662365,\n",
       "  'pred_soft': 7880,\n",
       "  'soft_val': 0.18543140590190887,\n",
       "  'sim_soft': 0.22259748,\n",
       "  'pred_max_max': 6018,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.20436707139015198,\n",
       "  'sim_max_max': 0.023023678},\n",
       " 21974: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4394,\n",
       "  'pred_knn': 2784,\n",
       "  'sim_real_cl': 0.25396365,\n",
       "  'sim_best_knn': 0.332216,\n",
       "  'pred_soft': 912,\n",
       "  'soft_val': 0.27495619654655457,\n",
       "  'sim_soft': 0.29201812,\n",
       "  'pred_max_max': 1066,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3088992238044739,\n",
       "  'sim_max_max': 0.314201},\n",
       " 21996: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4399,\n",
       "  'pred_knn': 2071,\n",
       "  'sim_real_cl': 0.044510156,\n",
       "  'sim_best_knn': 0.24855286,\n",
       "  'pred_soft': 2754,\n",
       "  'soft_val': 0.24162332713603973,\n",
       "  'sim_soft': 0.23060477,\n",
       "  'pred_max_max': 9493,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2237788885831833,\n",
       "  'sim_max_max': 0.15132983},\n",
       " 21997: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4399,\n",
       "  'pred_knn': 8852,\n",
       "  'sim_real_cl': 0.22715002,\n",
       "  'sim_best_knn': 0.33341584,\n",
       "  'pred_soft': 6419,\n",
       "  'soft_val': 0.26067498326301575,\n",
       "  'sim_soft': 0.3055513,\n",
       "  'pred_max_max': 6419,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2749732732772827,\n",
       "  'sim_max_max': 0.3055513},\n",
       " 22048: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4409,\n",
       "  'pred_knn': 2555,\n",
       "  'sim_real_cl': 0.21681646,\n",
       "  'sim_best_knn': 0.31695706,\n",
       "  'pred_soft': 2555,\n",
       "  'soft_val': 0.2751433849334717,\n",
       "  'sim_soft': 0.31695706,\n",
       "  'pred_max_max': 1174,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25229302048683167,\n",
       "  'sim_max_max': 0.26445436},\n",
       " 22174: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4434,\n",
       "  'pred_knn': 3546,\n",
       "  'sim_real_cl': 0.1300875,\n",
       "  'sim_best_knn': 0.3244292,\n",
       "  'pred_soft': 7887,\n",
       "  'soft_val': 0.2532918155193329,\n",
       "  'sim_soft': 0.2625891,\n",
       "  'pred_max_max': 3546,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.282787948846817,\n",
       "  'sim_max_max': 0.3244292},\n",
       " 22211: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4442,\n",
       "  'pred_knn': 4442,\n",
       "  'sim_real_cl': 0.4401312,\n",
       "  'sim_best_knn': 0.4401312,\n",
       "  'pred_soft': 67,\n",
       "  'soft_val': 0.2658784091472626,\n",
       "  'sim_soft': 0.33510184},\n",
       " 22286: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4457,\n",
       "  'pred_knn': 9921,\n",
       "  'sim_real_cl': 0.28331542,\n",
       "  'sim_best_knn': 0.31114995,\n",
       "  'pred_soft': 5542,\n",
       "  'soft_val': 0.26252681016921997,\n",
       "  'sim_soft': 0.2895735,\n",
       "  'pred_max_max': 3743,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26790302991867065,\n",
       "  'sim_max_max': 0.24082594},\n",
       " 22288: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4457,\n",
       "  'pred_knn': 3986,\n",
       "  'sim_real_cl': 0.36595494,\n",
       "  'sim_best_knn': 0.3767932,\n",
       "  'pred_soft': 3986,\n",
       "  'soft_val': 0.3387226462364197,\n",
       "  'sim_soft': 0.3767932,\n",
       "  'pred_max_max': 3986,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.32162535190582275,\n",
       "  'sim_max_max': 0.3767932},\n",
       " 22294: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4458,\n",
       "  'pred_knn': 4458,\n",
       "  'sim_real_cl': 0.48302186,\n",
       "  'sim_best_knn': 0.48302186,\n",
       "  'pred_soft': 5979,\n",
       "  'soft_val': 0.3457040786743164,\n",
       "  'sim_soft': 0.40889814},\n",
       " 22299: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4459,\n",
       "  'pred_knn': 4459,\n",
       "  'sim_real_cl': 0.47525042,\n",
       "  'sim_best_knn': 0.47525042,\n",
       "  'pred_soft': 2316,\n",
       "  'soft_val': 0.38601717352867126,\n",
       "  'sim_soft': 0.3675872,\n",
       "  'pred_max_max': 2316,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3886859714984894,\n",
       "  'sim_max_max': 0.3675872},\n",
       " 22328: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4465,\n",
       "  'pred_knn': 4465,\n",
       "  'sim_real_cl': 0.4370502,\n",
       "  'sim_best_knn': 0.4370502,\n",
       "  'pred_soft': 3820,\n",
       "  'soft_val': 0.2476118505001068,\n",
       "  'sim_soft': 0.25545308},\n",
       " 22337: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4467,\n",
       "  'pred_knn': 4880,\n",
       "  'sim_real_cl': 0.21135202,\n",
       "  'sim_best_knn': 0.31775585,\n",
       "  'pred_soft': 2661,\n",
       "  'soft_val': 0.2529492974281311,\n",
       "  'sim_soft': 0.24895765,\n",
       "  'pred_max_max': 2124,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2580585777759552,\n",
       "  'sim_max_max': 0.27146626},\n",
       " 22347: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4469,\n",
       "  'pred_knn': 4469,\n",
       "  'sim_real_cl': 0.36822262,\n",
       "  'sim_best_knn': 0.36822262,\n",
       "  'pred_soft': 1922,\n",
       "  'soft_val': 0.2730109989643097,\n",
       "  'sim_soft': 0.34117982},\n",
       " 22354: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4470,\n",
       "  'pred_knn': 7638,\n",
       "  'sim_real_cl': 0.26989278,\n",
       "  'sim_best_knn': 0.4026085,\n",
       "  'pred_soft': 7638,\n",
       "  'soft_val': 0.31955522298812866,\n",
       "  'sim_soft': 0.4026085,\n",
       "  'pred_max_max': 7638,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.4006667733192444,\n",
       "  'sim_max_max': 0.4026085},\n",
       " 22446: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4489,\n",
       "  'pred_knn': 1081,\n",
       "  'sim_real_cl': 0.10370466,\n",
       "  'sim_best_knn': 0.35819092,\n",
       "  'pred_soft': 4292,\n",
       "  'soft_val': 0.3224586248397827,\n",
       "  'sim_soft': 0.33373296,\n",
       "  'pred_max_max': 4292,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.33927708864212036,\n",
       "  'sim_max_max': 0.33373296},\n",
       " 22489: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4497,\n",
       "  'pred_knn': 962,\n",
       "  'sim_real_cl': 0.19593132,\n",
       "  'sim_best_knn': 0.29435048,\n",
       "  'pred_soft': 962,\n",
       "  'soft_val': 0.24798831343650818,\n",
       "  'sim_soft': 0.29435048,\n",
       "  'pred_max_max': 3359,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2533843219280243,\n",
       "  'sim_max_max': 0.26590896},\n",
       " 22506: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4501,\n",
       "  'pred_knn': 2381,\n",
       "  'sim_real_cl': 0.32343853,\n",
       "  'sim_best_knn': 0.36776862,\n",
       "  'pred_soft': 4501,\n",
       "  'soft_val': 0.2515876889228821,\n",
       "  'sim_soft': array(0.2515877, dtype=float32),\n",
       "  'pred_max_max': 4501,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.32372307777404785,\n",
       "  'sim_max_max': 0.36776862},\n",
       " 22513: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4502,\n",
       "  'pred_knn': 6470,\n",
       "  'sim_real_cl': 0.3559902,\n",
       "  'sim_best_knn': 0.4753304,\n",
       "  'pred_soft': 6470,\n",
       "  'soft_val': 0.29542040824890137,\n",
       "  'sim_soft': 0.4753304,\n",
       "  'pred_max_max': 4502,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.348843514919281,\n",
       "  'sim_max_max': 0.4753304},\n",
       " 22516: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4503,\n",
       "  'pred_knn': 4503,\n",
       "  'sim_real_cl': 0.36472014,\n",
       "  'sim_best_knn': 0.36472014,\n",
       "  'pred_soft': 4182,\n",
       "  'soft_val': 0.23592214286327362,\n",
       "  'sim_soft': 0.24445418},\n",
       " 22527: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4505,\n",
       "  'pred_knn': 1836,\n",
       "  'sim_real_cl': 0.3156528,\n",
       "  'sim_best_knn': 0.353274,\n",
       "  'pred_soft': 1445,\n",
       "  'soft_val': 0.2784919738769531,\n",
       "  'sim_soft': 0.280235,\n",
       "  'pred_max_max': 4505,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3153192400932312,\n",
       "  'sim_max_max': 0.353274},\n",
       " 22539: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4507,\n",
       "  'pred_knn': 8358,\n",
       "  'sim_real_cl': 0.18239668,\n",
       "  'sim_best_knn': 0.24013382,\n",
       "  'pred_soft': 8834,\n",
       "  'soft_val': 0.2301349937915802,\n",
       "  'sim_soft': 0.21691035,\n",
       "  'pred_max_max': 8834,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24700260162353516,\n",
       "  'sim_max_max': 0.21691035},\n",
       " 22611: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4522,\n",
       "  'pred_knn': 4522,\n",
       "  'sim_real_cl': 0.40670896,\n",
       "  'sim_best_knn': 0.40670896,\n",
       "  'pred_soft': 2829,\n",
       "  'soft_val': 0.2877058684825897,\n",
       "  'sim_soft': 0.31046826,\n",
       "  'pred_max_max': 2829,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2763690650463104,\n",
       "  'sim_max_max': 0.31046826},\n",
       " 22647: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4529,\n",
       "  'pred_knn': 4529,\n",
       "  'sim_real_cl': 0.38357514,\n",
       "  'sim_best_knn': 0.38357514,\n",
       "  'pred_soft': 2315,\n",
       "  'soft_val': 0.3427136540412903,\n",
       "  'sim_soft': 0.35865813},\n",
       " 22678: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4535,\n",
       "  'pred_knn': 4535,\n",
       "  'sim_real_cl': 0.44723225,\n",
       "  'sim_best_knn': 0.44723225,\n",
       "  'pred_soft': 9251,\n",
       "  'soft_val': 0.3364199101924896,\n",
       "  'sim_soft': 0.43517226,\n",
       "  'pred_max_max': 9251,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3282482326030731,\n",
       "  'sim_max_max': 0.43517226},\n",
       " 22679: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4535,\n",
       "  'pred_knn': 4535,\n",
       "  'sim_real_cl': 0.39595604,\n",
       "  'sim_best_knn': 0.39595604,\n",
       "  'pred_soft': 5492,\n",
       "  'soft_val': 0.26539337635040283,\n",
       "  'sim_soft': 0.33334273,\n",
       "  'pred_max_max': 6716,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27706378698349,\n",
       "  'sim_max_max': 0.37252453},\n",
       " 22688: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4537,\n",
       "  'pred_knn': 9583,\n",
       "  'sim_real_cl': 0.3724956,\n",
       "  'sim_best_knn': 0.47551924,\n",
       "  'pred_soft': 9583,\n",
       "  'soft_val': 0.3768729269504547,\n",
       "  'sim_soft': 0.47551924,\n",
       "  'pred_max_max': 9583,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.39662298560142517,\n",
       "  'sim_max_max': 0.47551924},\n",
       " 22734: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4546,\n",
       "  'pred_knn': 4546,\n",
       "  'sim_real_cl': 0.5053837,\n",
       "  'sim_best_knn': 0.5053837,\n",
       "  'pred_soft': 2609,\n",
       "  'soft_val': 0.2995222210884094,\n",
       "  'sim_soft': 0.3765791},\n",
       " 22777: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4555,\n",
       "  'pred_knn': 795,\n",
       "  'sim_real_cl': 0.44080073,\n",
       "  'sim_best_knn': 0.4560753,\n",
       "  'pred_soft': 795,\n",
       "  'soft_val': 0.4066469371318817,\n",
       "  'sim_soft': 0.4560753,\n",
       "  'pred_max_max': 4555,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.4002089500427246,\n",
       "  'sim_max_max': 0.4560753},\n",
       " 22864: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4572,\n",
       "  'pred_knn': 4572,\n",
       "  'sim_real_cl': 0.35618454,\n",
       "  'sim_best_knn': 0.35618454,\n",
       "  'pred_soft': 951,\n",
       "  'soft_val': 0.3142898678779602,\n",
       "  'sim_soft': 0.3156268,\n",
       "  'pred_max_max': 951,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30196887254714966,\n",
       "  'sim_max_max': 0.3156268},\n",
       " 22883: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4576,\n",
       "  'pred_knn': 4576,\n",
       "  'sim_real_cl': 0.4094695,\n",
       "  'sim_best_knn': 0.4094695,\n",
       "  'pred_soft': 8656,\n",
       "  'soft_val': 0.30402475595474243,\n",
       "  'sim_soft': 0.27607298},\n",
       " 22884: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4576,\n",
       "  'pred_knn': 4576,\n",
       "  'sim_real_cl': 0.4803031,\n",
       "  'sim_best_knn': 0.4803031,\n",
       "  'pred_soft': 9921,\n",
       "  'soft_val': 0.3047142028808594,\n",
       "  'sim_soft': 0.39153892},\n",
       " 23102: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4620,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.20211877,\n",
       "  'sim_best_knn': 0.2994393,\n",
       "  'pred_soft': 4433,\n",
       "  'soft_val': 0.21240192651748657,\n",
       "  'sim_soft': 0.18837401,\n",
       "  'pred_max_max': 8168,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24913303554058075,\n",
       "  'sim_max_max': 0.26974088},\n",
       " 23105: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4621,\n",
       "  'pred_knn': 2410,\n",
       "  'sim_real_cl': 0.21668813,\n",
       "  'sim_best_knn': 0.38506642,\n",
       "  'pred_soft': 2410,\n",
       "  'soft_val': 0.24623803794384003,\n",
       "  'sim_soft': 0.38506642,\n",
       "  'pred_max_max': 2410,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25574183464050293,\n",
       "  'sim_max_max': 0.38506642},\n",
       " 23108: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4621,\n",
       "  'pred_knn': 2410,\n",
       "  'sim_real_cl': 0.22832322,\n",
       "  'sim_best_knn': 0.3844924,\n",
       "  'pred_soft': 2410,\n",
       "  'soft_val': 0.2730146646499634,\n",
       "  'sim_soft': 0.3844924,\n",
       "  'pred_max_max': 2410,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27082422375679016,\n",
       "  'sim_max_max': 0.3844924},\n",
       " 23127: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4625,\n",
       "  'pred_knn': 4625,\n",
       "  'sim_real_cl': 0.6743851,\n",
       "  'sim_best_knn': 0.6743851,\n",
       "  'pred_soft': 9916,\n",
       "  'soft_val': 0.3218027949333191,\n",
       "  'sim_soft': 0.3439678},\n",
       " 23128: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4625,\n",
       "  'pred_knn': 4625,\n",
       "  'sim_real_cl': 0.8417259,\n",
       "  'sim_best_knn': 0.8417259,\n",
       "  'pred_soft': 5417,\n",
       "  'soft_val': 0.3057997524738312,\n",
       "  'sim_soft': 0.33311915},\n",
       " 23147: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4629,\n",
       "  'pred_knn': 4621,\n",
       "  'sim_real_cl': 0.113193855,\n",
       "  'sim_best_knn': 0.27888143,\n",
       "  'pred_soft': 2159,\n",
       "  'soft_val': 0.2314990907907486,\n",
       "  'sim_soft': 0.26013732,\n",
       "  'pred_max_max': 6078,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24388465285301208,\n",
       "  'sim_max_max': 0.27414036},\n",
       " 23159: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4631,\n",
       "  'pred_knn': 4141,\n",
       "  'sim_real_cl': 0.27954435,\n",
       "  'sim_best_knn': 0.27960315,\n",
       "  'pred_soft': 2997,\n",
       "  'soft_val': 0.2807336151599884,\n",
       "  'sim_soft': 0.27470595,\n",
       "  'pred_max_max': 2997,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2919733226299286,\n",
       "  'sim_max_max': 0.27470595},\n",
       " 23292: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4658,\n",
       "  'pred_knn': 1300,\n",
       "  'sim_real_cl': 0.31120828,\n",
       "  'sim_best_knn': 0.4495355,\n",
       "  'pred_soft': 4658,\n",
       "  'soft_val': 0.293111115694046,\n",
       "  'sim_soft': array(0.29311112, dtype=float32),\n",
       "  'pred_max_max': 4658,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3395332992076874,\n",
       "  'sim_max_max': 0.4495355},\n",
       " 23320: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4664,\n",
       "  'pred_knn': 4664,\n",
       "  'sim_real_cl': 0.3000186,\n",
       "  'sim_best_knn': 0.3000186,\n",
       "  'pred_soft': 4544,\n",
       "  'soft_val': 0.21437062323093414,\n",
       "  'sim_soft': 0.24099085,\n",
       "  'pred_max_max': 4544,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26747941970825195,\n",
       "  'sim_max_max': 0.24099085},\n",
       " 23328: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4665,\n",
       "  'pred_knn': 7485,\n",
       "  'sim_real_cl': 0.28110325,\n",
       "  'sim_best_knn': 0.32856238,\n",
       "  'pred_soft': 5480,\n",
       "  'soft_val': 0.27558690309524536,\n",
       "  'sim_soft': 0.30524144,\n",
       "  'pred_max_max': 4665,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.31089842319488525,\n",
       "  'sim_max_max': 0.32856238},\n",
       " 23391: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4678,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.27152365,\n",
       "  'sim_best_knn': 0.39405447,\n",
       "  'pred_soft': 3931,\n",
       "  'soft_val': 0.2071874588727951,\n",
       "  'sim_soft': 0.24326527,\n",
       "  'pred_max_max': 4678,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3013685345649719,\n",
       "  'sim_max_max': 0.39405447},\n",
       " 23419: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4683,\n",
       "  'pred_knn': 4683,\n",
       "  'sim_real_cl': 0.43941242,\n",
       "  'sim_best_knn': 0.43941242,\n",
       "  'pred_soft': 2896,\n",
       "  'soft_val': 0.33670172095298767,\n",
       "  'sim_soft': 0.3513563},\n",
       " 23465: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4693,\n",
       "  'pred_knn': 4693,\n",
       "  'sim_real_cl': 0.40260744,\n",
       "  'sim_best_knn': 0.40260744,\n",
       "  'pred_soft': 5812,\n",
       "  'soft_val': 0.27086880803108215,\n",
       "  'sim_soft': 0.2762559},\n",
       " 23467: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4693,\n",
       "  'pred_knn': 4693,\n",
       "  'sim_real_cl': 0.4267552,\n",
       "  'sim_best_knn': 0.4267552,\n",
       "  'pred_soft': 7998,\n",
       "  'soft_val': 0.28662949800491333,\n",
       "  'sim_soft': 0.33511013},\n",
       " 23469: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4693,\n",
       "  'pred_knn': 4693,\n",
       "  'sim_real_cl': 0.40579647,\n",
       "  'sim_best_knn': 0.40579647,\n",
       "  'pred_soft': 5812,\n",
       "  'soft_val': 0.28164002299308777,\n",
       "  'sim_soft': 0.28369844},\n",
       " 23497: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4699,\n",
       "  'pred_knn': 9882,\n",
       "  'sim_real_cl': 0.36777,\n",
       "  'sim_best_knn': 0.39519882,\n",
       "  'pred_soft': 2583,\n",
       "  'soft_val': 0.29415255784988403,\n",
       "  'sim_soft': 0.31455588,\n",
       "  'pred_max_max': 9882,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2996356189250946,\n",
       "  'sim_max_max': 0.39519882},\n",
       " 23506: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4701,\n",
       "  'pred_knn': 696,\n",
       "  'sim_real_cl': 0.36787182,\n",
       "  'sim_best_knn': 0.40339115,\n",
       "  'pred_soft': 696,\n",
       "  'soft_val': 0.2919628322124481,\n",
       "  'sim_soft': 0.40339115,\n",
       "  'pred_max_max': 4701,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3204120099544525,\n",
       "  'sim_max_max': 0.40339115},\n",
       " 23554: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4710,\n",
       "  'pred_knn': 4710,\n",
       "  'sim_real_cl': 0.4254895,\n",
       "  'sim_best_knn': 0.4254895,\n",
       "  'pred_soft': 5674,\n",
       "  'soft_val': 0.3396153450012207,\n",
       "  'sim_soft': 0.36948103},\n",
       " 23679: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4735,\n",
       "  'pred_knn': 4735,\n",
       "  'sim_real_cl': 0.375032,\n",
       "  'sim_best_knn': 0.375032,\n",
       "  'pred_soft': 5753,\n",
       "  'soft_val': 0.26037269830703735,\n",
       "  'sim_soft': 0.2606451,\n",
       "  'pred_max_max': 1572,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2601979970932007,\n",
       "  'sim_max_max': 0.3214904},\n",
       " 23741: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4748,\n",
       "  'pred_knn': 4748,\n",
       "  'sim_real_cl': 0.3429888,\n",
       "  'sim_best_knn': 0.3429888,\n",
       "  'pred_soft': 940,\n",
       "  'soft_val': 0.2906447649002075,\n",
       "  'sim_soft': 0.32691133},\n",
       " 23764: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4752,\n",
       "  'pred_knn': 8620,\n",
       "  'sim_real_cl': 0.32863852,\n",
       "  'sim_best_knn': 0.37081707,\n",
       "  'pred_soft': 4752,\n",
       "  'soft_val': 0.3133162260055542,\n",
       "  'sim_soft': array(0.31331623, dtype=float32),\n",
       "  'pred_max_max': 4985,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30884718894958496,\n",
       "  'sim_max_max': 0.2816672},\n",
       " 23769: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4753,\n",
       "  'pred_knn': 9110,\n",
       "  'sim_real_cl': 0.25263572,\n",
       "  'sim_best_knn': 0.29070768,\n",
       "  'pred_soft': 9360,\n",
       "  'soft_val': 0.2564482092857361,\n",
       "  'sim_soft': 0.2777984,\n",
       "  'pred_max_max': 4753,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.24081067740917206,\n",
       "  'sim_max_max': 0.29070768},\n",
       " 23852: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4770,\n",
       "  'pred_knn': 4770,\n",
       "  'sim_real_cl': 0.28432575,\n",
       "  'sim_best_knn': 0.28432575,\n",
       "  'pred_soft': 8569,\n",
       "  'soft_val': 0.23480689525604248,\n",
       "  'sim_soft': 0.2353974},\n",
       " 23855: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4771,\n",
       "  'pred_knn': 1593,\n",
       "  'sim_real_cl': 0.2744648,\n",
       "  'sim_best_knn': 0.3730088,\n",
       "  'pred_soft': 1593,\n",
       "  'soft_val': 0.3126591145992279,\n",
       "  'sim_soft': 0.3730088,\n",
       "  'pred_max_max': 1593,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.327086865901947,\n",
       "  'sim_max_max': 0.3730088},\n",
       " 23913: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4782,\n",
       "  'pred_knn': 98,\n",
       "  'sim_real_cl': 0.24657276,\n",
       "  'sim_best_knn': 0.31229833,\n",
       "  'pred_soft': 5817,\n",
       "  'soft_val': 0.22271381318569183,\n",
       "  'sim_soft': 0.2145459,\n",
       "  'pred_max_max': 98,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28834637999534607,\n",
       "  'sim_max_max': 0.31229833},\n",
       " 23923: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4784,\n",
       "  'pred_knn': 2182,\n",
       "  'sim_real_cl': 0.3157993,\n",
       "  'sim_best_knn': 0.34459168,\n",
       "  'pred_soft': 4784,\n",
       "  'soft_val': 0.30136677622795105,\n",
       "  'sim_soft': array(0.30136678, dtype=float32),\n",
       "  'pred_max_max': 4784,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.34574174880981445,\n",
       "  'sim_max_max': 0.34459168},\n",
       " 23949: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4789,\n",
       "  'pred_knn': 5433,\n",
       "  'sim_real_cl': 0.26845813,\n",
       "  'sim_best_knn': 0.32682428,\n",
       "  'pred_soft': 5433,\n",
       "  'soft_val': 0.25166529417037964,\n",
       "  'sim_soft': 0.32682428,\n",
       "  'pred_max_max': 3242,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28168851137161255,\n",
       "  'sim_max_max': 0.32061502},\n",
       " 23958: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4791,\n",
       "  'pred_knn': 4791,\n",
       "  'sim_real_cl': 0.44088584,\n",
       "  'sim_best_knn': 0.44088584,\n",
       "  'pred_soft': 2084,\n",
       "  'soft_val': 0.3870372772216797,\n",
       "  'sim_soft': 0.35907602,\n",
       "  'pred_max_max': 2084,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.39310675859451294,\n",
       "  'sim_max_max': 0.35907602},\n",
       " 23982: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4796,\n",
       "  'pred_knn': 6078,\n",
       "  'sim_real_cl': 0.18242064,\n",
       "  'sim_best_knn': 0.32640532,\n",
       "  'pred_soft': 7670,\n",
       "  'soft_val': 0.2611809968948364,\n",
       "  'sim_soft': 0.28136745,\n",
       "  'pred_max_max': 7670,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2659534811973572,\n",
       "  'sim_max_max': 0.28136745},\n",
       " 24090: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4818,\n",
       "  'pred_knn': 6055,\n",
       "  'sim_real_cl': 0.30667323,\n",
       "  'sim_best_knn': 0.35916567,\n",
       "  'pred_soft': 6740,\n",
       "  'soft_val': 0.26739808917045593,\n",
       "  'sim_soft': 0.28989848,\n",
       "  'pred_max_max': 8640,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27440595626831055,\n",
       "  'sim_max_max': 0.3035932},\n",
       " 24162: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4832,\n",
       "  'pred_knn': 7190,\n",
       "  'sim_real_cl': 0.21336624,\n",
       "  'sim_best_knn': 0.3104638,\n",
       "  'pred_soft': 4992,\n",
       "  'soft_val': 0.24315986037254333,\n",
       "  'sim_soft': 0.2747507,\n",
       "  'pred_max_max': 352,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2867775857448578,\n",
       "  'sim_max_max': 0.30560803},\n",
       " 24198: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4839,\n",
       "  'pred_knn': 4839,\n",
       "  'sim_real_cl': 0.51364,\n",
       "  'sim_best_knn': 0.51364,\n",
       "  'pred_soft': 6321,\n",
       "  'soft_val': 0.2719683051109314,\n",
       "  'sim_soft': 0.30271798},\n",
       " 24230: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4846,\n",
       "  'pred_knn': 5277,\n",
       "  'sim_real_cl': 0.3042211,\n",
       "  'sim_best_knn': 0.40265375,\n",
       "  'pred_soft': 5277,\n",
       "  'soft_val': 0.36233246326446533,\n",
       "  'sim_soft': 0.40265375,\n",
       "  'pred_max_max': 8354,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.36345386505126953,\n",
       "  'sim_max_max': 0.3792914},\n",
       " 24263: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4852,\n",
       "  'pred_knn': 4852,\n",
       "  'sim_real_cl': 0.5097574,\n",
       "  'sim_best_knn': 0.5097574,\n",
       "  'pred_soft': 9961,\n",
       "  'soft_val': 0.24356761574745178,\n",
       "  'sim_soft': 0.3156038},\n",
       " 24280: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4856,\n",
       "  'pred_knn': 1954,\n",
       "  'sim_real_cl': 0.12476061,\n",
       "  'sim_best_knn': 0.29062033,\n",
       "  'pred_soft': 7965,\n",
       "  'soft_val': 0.20952942967414856,\n",
       "  'sim_soft': 0.22117871,\n",
       "  'pred_max_max': 6018,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2163991928100586,\n",
       "  'sim_max_max': 0.004028165},\n",
       " 24298: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4859,\n",
       "  'pred_knn': 4859,\n",
       "  'sim_real_cl': 0.5983263,\n",
       "  'sim_best_knn': 0.5983263,\n",
       "  'pred_soft': 5382,\n",
       "  'soft_val': 0.4122723340988159,\n",
       "  'sim_soft': 0.40566355},\n",
       " 24307: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4861,\n",
       "  'pred_knn': 4861,\n",
       "  'sim_real_cl': 0.583158,\n",
       "  'sim_best_knn': 0.583158,\n",
       "  'pred_soft': 13,\n",
       "  'soft_val': 0.36183053255081177,\n",
       "  'sim_soft': 0.38568592},\n",
       " 24308: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4861,\n",
       "  'pred_knn': 4861,\n",
       "  'sim_real_cl': 0.60265005,\n",
       "  'sim_best_knn': 0.60265005,\n",
       "  'pred_soft': 13,\n",
       "  'soft_val': 0.2991417944431305,\n",
       "  'sim_soft': 0.26492944},\n",
       " 24357: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4871,\n",
       "  'pred_knn': 2873,\n",
       "  'sim_real_cl': 0.23902854,\n",
       "  'sim_best_knn': 0.30467656,\n",
       "  'pred_soft': 2998,\n",
       "  'soft_val': 0.25798019766807556,\n",
       "  'sim_soft': 0.2602617,\n",
       "  'pred_max_max': 8582,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28360462188720703,\n",
       "  'sim_max_max': 0.3045436},\n",
       " 24376: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4875,\n",
       "  'pred_knn': 4875,\n",
       "  'sim_real_cl': 0.4288702,\n",
       "  'sim_best_knn': 0.4288702,\n",
       "  'pred_soft': 2344,\n",
       "  'soft_val': 0.3123423159122467,\n",
       "  'sim_soft': 0.30575764,\n",
       "  'pred_max_max': 7956,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27389803528785706,\n",
       "  'sim_max_max': 0.34997562},\n",
       " 24456: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4891,\n",
       "  'pred_knn': 6327,\n",
       "  'sim_real_cl': 0.33656153,\n",
       "  'sim_best_knn': 0.39292654,\n",
       "  'pred_soft': 2531,\n",
       "  'soft_val': 0.30269142985343933,\n",
       "  'sim_soft': 0.35681403,\n",
       "  'pred_max_max': 4891,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.35095450282096863,\n",
       "  'sim_max_max': 0.39292654},\n",
       " 24485: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4897,\n",
       "  'pred_knn': 7870,\n",
       "  'sim_real_cl': 0.3470079,\n",
       "  'sim_best_knn': 0.3656634,\n",
       "  'pred_soft': 6788,\n",
       "  'soft_val': 0.2606951594352722,\n",
       "  'sim_soft': 0.30123332,\n",
       "  'pred_max_max': 4897,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.27643343806266785,\n",
       "  'sim_max_max': 0.3656634},\n",
       " 24547: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4909,\n",
       "  'pred_knn': 4451,\n",
       "  'sim_real_cl': 0.37439418,\n",
       "  'sim_best_knn': 0.41372555,\n",
       "  'pred_soft': 8977,\n",
       "  'soft_val': 0.27607059478759766,\n",
       "  'sim_soft': 0.29437655,\n",
       "  'pred_max_max': 4451,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2981756627559662,\n",
       "  'sim_max_max': 0.41372555},\n",
       " 24549: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4909,\n",
       "  'pred_knn': 8438,\n",
       "  'sim_real_cl': 0.34888953,\n",
       "  'sim_best_knn': 0.35501054,\n",
       "  'pred_soft': 8599,\n",
       "  'soft_val': 0.27324527502059937,\n",
       "  'sim_soft': 0.33077163,\n",
       "  'pred_max_max': 8599,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.29396694898605347,\n",
       "  'sim_max_max': 0.33077163},\n",
       " 24595: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4919,\n",
       "  'pred_knn': 4919,\n",
       "  'sim_real_cl': 0.3518002,\n",
       "  'sim_best_knn': 0.3518002,\n",
       "  'pred_soft': 2445,\n",
       "  'soft_val': 0.2975938022136688,\n",
       "  'sim_soft': 0.3119639},\n",
       " 24622: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4924,\n",
       "  'pred_knn': 1308,\n",
       "  'sim_real_cl': 0.2845639,\n",
       "  'sim_best_knn': 0.42305517,\n",
       "  'pred_soft': 1308,\n",
       "  'soft_val': 0.3308973014354706,\n",
       "  'sim_soft': 0.42305517,\n",
       "  'pred_max_max': 1308,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.36857670545578003,\n",
       "  'sim_max_max': 0.42305517},\n",
       " 24628: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4925,\n",
       "  'pred_knn': 4959,\n",
       "  'sim_real_cl': 0.314525,\n",
       "  'sim_best_knn': 0.32140195,\n",
       "  'pred_soft': 4925,\n",
       "  'soft_val': 0.3650827705860138,\n",
       "  'sim_soft': array(0.36508277, dtype=float32),\n",
       "  'pred_max_max': 4925,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.32438939809799194,\n",
       "  'sim_max_max': 0.32140195},\n",
       " 24674: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4934,\n",
       "  'pred_knn': 2713,\n",
       "  'sim_real_cl': 0.3719561,\n",
       "  'sim_best_knn': 0.39076746,\n",
       "  'pred_soft': 2713,\n",
       "  'soft_val': 0.3749403655529022,\n",
       "  'sim_soft': 0.39076746,\n",
       "  'pred_max_max': 2713,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3215642273426056,\n",
       "  'sim_max_max': 0.39076746},\n",
       " 24769: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4953,\n",
       "  'pred_knn': 5585,\n",
       "  'sim_real_cl': 0.2702698,\n",
       "  'sim_best_knn': 0.29486674,\n",
       "  'pred_soft': 5585,\n",
       "  'soft_val': 0.27895089983940125,\n",
       "  'sim_soft': 0.29486674,\n",
       "  'pred_max_max': 7011,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2517675757408142,\n",
       "  'sim_max_max': 0.25409806},\n",
       " 24866: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4973,\n",
       "  'pred_knn': 6360,\n",
       "  'sim_real_cl': 0.24740776,\n",
       "  'sim_best_knn': 0.41969544,\n",
       "  'pred_soft': 6360,\n",
       "  'soft_val': 0.292181134223938,\n",
       "  'sim_soft': 0.41969544,\n",
       "  'pred_max_max': 6360,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3137459456920624,\n",
       "  'sim_max_max': 0.41969544},\n",
       " 24901: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 4980,\n",
       "  'pred_knn': 4980,\n",
       "  'sim_real_cl': 0.26947224,\n",
       "  'sim_best_knn': 0.26947224,\n",
       "  'pred_soft': 2018,\n",
       "  'soft_val': 0.25289762020111084,\n",
       "  'sim_soft': 0.18912804},\n",
       " 24909: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4981,\n",
       "  'pred_knn': 8569,\n",
       "  'sim_real_cl': 0.3475972,\n",
       "  'sim_best_knn': 0.47612485,\n",
       "  'pred_soft': 8569,\n",
       "  'soft_val': 0.4001852571964264,\n",
       "  'sim_soft': 0.47612485,\n",
       "  'pred_max_max': 8569,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.4066252112388611,\n",
       "  'sim_max_max': 0.47612485},\n",
       " 24941: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 4988,\n",
       "  'pred_knn': 1836,\n",
       "  'sim_real_cl': 0.34052324,\n",
       "  'sim_best_knn': 0.39493132,\n",
       "  'pred_soft': 4988,\n",
       "  'soft_val': 0.3622179627418518,\n",
       "  'sim_soft': array(0.36221796, dtype=float32),\n",
       "  'pred_max_max': 4988,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.4355822801589966,\n",
       "  'sim_max_max': 0.39493132},\n",
       " 25067: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5013,\n",
       "  'pred_knn': 6390,\n",
       "  'sim_real_cl': 0.3383906,\n",
       "  'sim_best_knn': 0.37417215,\n",
       "  'pred_soft': 5937,\n",
       "  'soft_val': 0.27130600810050964,\n",
       "  'sim_soft': 0.34131354,\n",
       "  'pred_max_max': 5013,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3104875683784485,\n",
       "  'sim_max_max': 0.37417215},\n",
       " 25077: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5015,\n",
       "  'pred_knn': 5015,\n",
       "  'sim_real_cl': 0.3722751,\n",
       "  'sim_best_knn': 0.3722751,\n",
       "  'pred_soft': 2809,\n",
       "  'soft_val': 0.3400440216064453,\n",
       "  'sim_soft': 0.34240538},\n",
       " 25094: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5018,\n",
       "  'pred_knn': 1144,\n",
       "  'sim_real_cl': 0.33147395,\n",
       "  'sim_best_knn': 0.33439788,\n",
       "  'pred_soft': 5018,\n",
       "  'soft_val': 0.3074277937412262,\n",
       "  'sim_soft': array(0.3074278, dtype=float32),\n",
       "  'pred_max_max': 3425,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2734031677246094,\n",
       "  'sim_max_max': 0.32278275},\n",
       " 25129: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5025,\n",
       "  'pred_knn': 5025,\n",
       "  'sim_real_cl': 0.3438071,\n",
       "  'sim_best_knn': 0.3438071,\n",
       "  'pred_soft': 820,\n",
       "  'soft_val': 0.2470470368862152,\n",
       "  'sim_soft': 0.24805981},\n",
       " 25137: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5027,\n",
       "  'pred_knn': 9980,\n",
       "  'sim_real_cl': 0.30738375,\n",
       "  'sim_best_knn': 0.44793934,\n",
       "  'pred_soft': 2091,\n",
       "  'soft_val': 0.29068678617477417,\n",
       "  'sim_soft': 0.3628897,\n",
       "  'pred_max_max': 2091,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3075387477874756,\n",
       "  'sim_max_max': 0.3628897},\n",
       " 25139: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5027,\n",
       "  'pred_knn': 9980,\n",
       "  'sim_real_cl': 0.31709284,\n",
       "  'sim_best_knn': 0.42500204,\n",
       "  'pred_soft': 2091,\n",
       "  'soft_val': 0.2601793110370636,\n",
       "  'sim_soft': 0.32332557,\n",
       "  'pred_max_max': 3278,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2876276969909668,\n",
       "  'sim_max_max': 0.29448336},\n",
       " 25244: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5048,\n",
       "  'pred_knn': 639,\n",
       "  'sim_real_cl': 0.22879618,\n",
       "  'sim_best_knn': 0.42363557,\n",
       "  'pred_soft': 8797,\n",
       "  'soft_val': 0.20520547032356262,\n",
       "  'sim_soft': 0.20199189,\n",
       "  'pred_max_max': 4562,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.23243962228298187,\n",
       "  'sim_max_max': 0.3469607},\n",
       " 25278: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5055,\n",
       "  'pred_knn': 3038,\n",
       "  'sim_real_cl': 0.34968328,\n",
       "  'sim_best_knn': 0.42520452,\n",
       "  'pred_soft': 7113,\n",
       "  'soft_val': 0.2919614911079407,\n",
       "  'sim_soft': 0.29190844,\n",
       "  'pred_max_max': 3038,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3769099712371826,\n",
       "  'sim_max_max': 0.42520452},\n",
       " 25284: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5056,\n",
       "  'pred_knn': 9658,\n",
       "  'sim_real_cl': 0.10051687,\n",
       "  'sim_best_knn': 0.3521108,\n",
       "  'pred_soft': 2081,\n",
       "  'soft_val': 0.27675846219062805,\n",
       "  'sim_soft': 0.29871345,\n",
       "  'pred_max_max': 1789,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2764585614204407,\n",
       "  'sim_max_max': 0.32365423},\n",
       " 25302: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5060,\n",
       "  'pred_knn': 2381,\n",
       "  'sim_real_cl': 0.41779476,\n",
       "  'sim_best_knn': 0.48673156,\n",
       "  'pred_soft': 5515,\n",
       "  'soft_val': 0.40292665362358093,\n",
       "  'sim_soft': 0.47262534,\n",
       "  'pred_max_max': 2381,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.40943604707717896,\n",
       "  'sim_max_max': 0.48673156},\n",
       " 25338: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5067,\n",
       "  'pred_knn': 5067,\n",
       "  'sim_real_cl': 0.42465603,\n",
       "  'sim_best_knn': 0.42465603,\n",
       "  'pred_soft': 2091,\n",
       "  'soft_val': 0.36089926958084106,\n",
       "  'sim_soft': 0.39985466,\n",
       "  'pred_max_max': 2091,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.40321823954582214,\n",
       "  'sim_max_max': 0.39985466},\n",
       " 25449: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5089,\n",
       "  'pred_knn': 1922,\n",
       "  'sim_real_cl': 0.35246217,\n",
       "  'sim_best_knn': 0.3841834,\n",
       "  'pred_soft': 2316,\n",
       "  'soft_val': 0.3485947847366333,\n",
       "  'sim_soft': 0.31904316,\n",
       "  'pred_max_max': 2316,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.34138375520706177,\n",
       "  'sim_max_max': 0.31904316},\n",
       " 25456: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5091,\n",
       "  'pred_knn': 5772,\n",
       "  'sim_real_cl': 0.32148212,\n",
       "  'sim_best_knn': 0.32999346,\n",
       "  'pred_soft': 2937,\n",
       "  'soft_val': 0.2835385203361511,\n",
       "  'sim_soft': 0.28701422,\n",
       "  'pred_max_max': 5091,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.297986775636673,\n",
       "  'sim_max_max': 0.32999346},\n",
       " 25566: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5113,\n",
       "  'pred_knn': 5113,\n",
       "  'sim_real_cl': 0.32366943,\n",
       "  'sim_best_knn': 0.32366943,\n",
       "  'pred_soft': 2764,\n",
       "  'soft_val': 0.2743872106075287,\n",
       "  'sim_soft': 0.25439805},\n",
       " 25581: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5116,\n",
       "  'pred_knn': 5116,\n",
       "  'sim_real_cl': 0.26060832,\n",
       "  'sim_best_knn': 0.26060832,\n",
       "  'pred_soft': 7639,\n",
       "  'soft_val': 0.22312894463539124,\n",
       "  'sim_soft': 0.23329544},\n",
       " 25609: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5121,\n",
       "  'pred_knn': 4103,\n",
       "  'sim_real_cl': 0.28928584,\n",
       "  'sim_best_knn': 0.36058065,\n",
       "  'pred_soft': 4103,\n",
       "  'soft_val': 0.259513795375824,\n",
       "  'sim_soft': 0.36058065,\n",
       "  'pred_max_max': 7965,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3055344223976135,\n",
       "  'sim_max_max': 0.2949974},\n",
       " 25637: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5127,\n",
       "  'pred_knn': 3995,\n",
       "  'sim_real_cl': 0.16783546,\n",
       "  'sim_best_knn': 0.23847573,\n",
       "  'pred_soft': 4562,\n",
       "  'soft_val': 0.20322446525096893,\n",
       "  'sim_soft': 0.23312539,\n",
       "  'pred_max_max': 4562,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.20175695419311523,\n",
       "  'sim_max_max': 0.23312539},\n",
       " 25657: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5131,\n",
       "  'pred_knn': 2834,\n",
       "  'sim_real_cl': 0.19703695,\n",
       "  'sim_best_knn': 0.32919526,\n",
       "  'pred_soft': 2834,\n",
       "  'soft_val': 0.2590548098087311,\n",
       "  'sim_soft': 0.32919526,\n",
       "  'pred_max_max': 2834,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2970215082168579,\n",
       "  'sim_max_max': 0.32919526},\n",
       " 25659: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5131,\n",
       "  'pred_knn': 5117,\n",
       "  'sim_real_cl': 0.22805966,\n",
       "  'sim_best_knn': 0.3815177,\n",
       "  'pred_soft': 302,\n",
       "  'soft_val': 0.23077847063541412,\n",
       "  'sim_soft': 0.30658862,\n",
       "  'pred_max_max': 5117,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26669812202453613,\n",
       "  'sim_max_max': 0.3815177},\n",
       " 25674: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5134,\n",
       "  'pred_knn': 8821,\n",
       "  'sim_real_cl': 0.22074206,\n",
       "  'sim_best_knn': 0.3707201,\n",
       "  'pred_soft': 8821,\n",
       "  'soft_val': 0.28566044569015503,\n",
       "  'sim_soft': 0.3707201,\n",
       "  'pred_max_max': 8821,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2992768883705139,\n",
       "  'sim_max_max': 0.3707201},\n",
       " 25720: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5144,\n",
       "  'pred_knn': 2860,\n",
       "  'sim_real_cl': 0.22724335,\n",
       "  'sim_best_knn': 0.3148101,\n",
       "  'pred_soft': 2484,\n",
       "  'soft_val': 0.2519644796848297,\n",
       "  'sim_soft': 0.23619862,\n",
       "  'pred_max_max': 94,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2592780292034149,\n",
       "  'sim_max_max': 0.2809695},\n",
       " 25721: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5144,\n",
       "  'pred_knn': 6819,\n",
       "  'sim_real_cl': 0.24630472,\n",
       "  'sim_best_knn': 0.27971047,\n",
       "  'pred_soft': 5745,\n",
       "  'soft_val': 0.2615985870361328,\n",
       "  'sim_soft': 0.26670617,\n",
       "  'pred_max_max': 6819,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27380990982055664,\n",
       "  'sim_max_max': 0.27971047},\n",
       " 25786: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5157,\n",
       "  'pred_knn': 5157,\n",
       "  'sim_real_cl': 0.47767895,\n",
       "  'sim_best_knn': 0.47767895,\n",
       "  'pred_soft': 9328,\n",
       "  'soft_val': 0.3177882134914398,\n",
       "  'sim_soft': 0.34704727},\n",
       " 25815: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5163,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.28792801,\n",
       "  'sim_best_knn': 0.34852546,\n",
       "  'pred_soft': 8834,\n",
       "  'soft_val': 0.27852845191955566,\n",
       "  'sim_soft': 0.24518758,\n",
       "  'pred_max_max': 5484,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2737857699394226,\n",
       "  'sim_max_max': 0.3480235},\n",
       " 25818: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5163,\n",
       "  'pred_knn': 806,\n",
       "  'sim_real_cl': 0.31353596,\n",
       "  'sim_best_knn': 0.37937677,\n",
       "  'pred_soft': 5163,\n",
       "  'soft_val': 0.25301164388656616,\n",
       "  'sim_soft': array(0.25301164, dtype=float32),\n",
       "  'pred_max_max': 806,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2849459648132324,\n",
       "  'sim_max_max': 0.37937677},\n",
       " 25827: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5165,\n",
       "  'pred_knn': 9102,\n",
       "  'sim_real_cl': 0.29779792,\n",
       "  'sim_best_knn': 0.34688526,\n",
       "  'pred_soft': 7019,\n",
       "  'soft_val': 0.2612403333187103,\n",
       "  'sim_soft': 0.1941348,\n",
       "  'pred_max_max': 5165,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.2642722725868225,\n",
       "  'sim_max_max': 0.34688526},\n",
       " 25828: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5165,\n",
       "  'pred_knn': 6655,\n",
       "  'sim_real_cl': 0.15027204,\n",
       "  'sim_best_knn': 0.3247269,\n",
       "  'pred_soft': 6655,\n",
       "  'soft_val': 0.2372140884399414,\n",
       "  'sim_soft': 0.3247269,\n",
       "  'pred_max_max': 4373,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.217670276761055,\n",
       "  'sim_max_max': 0.16647545},\n",
       " 25890: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5178,\n",
       "  'pred_knn': 1944,\n",
       "  'sim_real_cl': 0.3075143,\n",
       "  'sim_best_knn': 0.3590513,\n",
       "  'pred_soft': 5556,\n",
       "  'soft_val': 0.3057482838630676,\n",
       "  'sim_soft': 0.2569636,\n",
       "  'pred_max_max': 5556,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27785372734069824,\n",
       "  'sim_max_max': 0.2569636},\n",
       " 25919: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5183,\n",
       "  'pred_knn': 8583,\n",
       "  'sim_real_cl': 0.34412968,\n",
       "  'sim_best_knn': 0.37963527,\n",
       "  'pred_soft': 5183,\n",
       "  'soft_val': 0.2820022404193878,\n",
       "  'sim_soft': array(0.28200224, dtype=float32),\n",
       "  'pred_max_max': 1550,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26931050419807434,\n",
       "  'sim_max_max': 0.3013792},\n",
       " 25985: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5197,\n",
       "  'pred_knn': 8959,\n",
       "  'sim_real_cl': 0.25663733,\n",
       "  'sim_best_knn': 0.30041692,\n",
       "  'pred_soft': 8959,\n",
       "  'soft_val': 0.29368165135383606,\n",
       "  'sim_soft': 0.30041692,\n",
       "  'pred_max_max': 8959,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3186340034008026,\n",
       "  'sim_max_max': 0.30041692},\n",
       " 25987: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5197,\n",
       "  'pred_knn': 2091,\n",
       "  'sim_real_cl': 0.058435917,\n",
       "  'sim_best_knn': 0.33307567,\n",
       "  'pred_soft': 2091,\n",
       "  'soft_val': 0.2865838408470154,\n",
       "  'sim_soft': 0.33307567,\n",
       "  'pred_max_max': 2091,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.31389206647872925,\n",
       "  'sim_max_max': 0.33307567},\n",
       " 26036: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5207,\n",
       "  'pred_knn': 6083,\n",
       "  'sim_real_cl': 0.20660804,\n",
       "  'sim_best_knn': 0.33932492,\n",
       "  'pred_soft': 5415,\n",
       "  'soft_val': 0.2662312388420105,\n",
       "  'sim_soft': 0.30719572,\n",
       "  'pred_max_max': 6083,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3028234839439392,\n",
       "  'sim_max_max': 0.33932492},\n",
       " 26038: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5207,\n",
       "  'pred_knn': 576,\n",
       "  'sim_real_cl': 0.23922849,\n",
       "  'sim_best_knn': 0.2787014,\n",
       "  'pred_soft': 9259,\n",
       "  'soft_val': 0.23935005068778992,\n",
       "  'sim_soft': 0.26120186,\n",
       "  'pred_max_max': 9259,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2457604557275772,\n",
       "  'sim_max_max': 0.26120186},\n",
       " 26045: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5209,\n",
       "  'pred_knn': 5209,\n",
       "  'sim_real_cl': 0.331474,\n",
       "  'sim_best_knn': 0.331474,\n",
       "  'pred_soft': 8502,\n",
       "  'soft_val': 0.2655078172683716,\n",
       "  'sim_soft': 0.28649992},\n",
       " 26058: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5211,\n",
       "  'pred_knn': 4074,\n",
       "  'sim_real_cl': 0.33797723,\n",
       "  'sim_best_knn': 0.36670405,\n",
       "  'pred_soft': 5211,\n",
       "  'soft_val': 0.28678983449935913,\n",
       "  'sim_soft': array(0.28678983, dtype=float32),\n",
       "  'pred_max_max': 5211,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.35112446546554565,\n",
       "  'sim_max_max': 0.36670405},\n",
       " 26059: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5211,\n",
       "  'pred_knn': 2443,\n",
       "  'sim_real_cl': 0.27929047,\n",
       "  'sim_best_knn': 0.28008747,\n",
       "  'pred_soft': 4207,\n",
       "  'soft_val': 0.25977596640586853,\n",
       "  'sim_soft': 0.267538,\n",
       "  'pred_max_max': 242,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25888437032699585,\n",
       "  'sim_max_max': 0.21029814},\n",
       " 26081: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5216,\n",
       "  'pred_knn': 1172,\n",
       "  'sim_real_cl': 0.39883524,\n",
       "  'sim_best_knn': 0.40169263,\n",
       "  'pred_soft': 5216,\n",
       "  'soft_val': 0.32355645298957825,\n",
       "  'sim_soft': array(0.32355645, dtype=float32),\n",
       "  'pred_max_max': 5216,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.34539762139320374,\n",
       "  'sim_max_max': 0.40169263},\n",
       " 26084: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5216,\n",
       "  'pred_knn': 9371,\n",
       "  'sim_real_cl': 0.21317482,\n",
       "  'sim_best_knn': 0.347895,\n",
       "  'pred_soft': 9371,\n",
       "  'soft_val': 0.2605538070201874,\n",
       "  'sim_soft': 0.347895,\n",
       "  'pred_max_max': 9371,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.33428168296813965,\n",
       "  'sim_max_max': 0.347895},\n",
       " 26176: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5235,\n",
       "  'pred_knn': 8045,\n",
       "  'sim_real_cl': 0.2617553,\n",
       "  'sim_best_knn': 0.2864788,\n",
       "  'pred_soft': 5235,\n",
       "  'soft_val': 0.34222880005836487,\n",
       "  'sim_soft': array(0.3422288, dtype=float32),\n",
       "  'pred_max_max': 5235,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.36072322726249695,\n",
       "  'sim_max_max': 0.2864788},\n",
       " 26178: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5235,\n",
       "  'pred_knn': 1296,\n",
       "  'sim_real_cl': 0.25671756,\n",
       "  'sim_best_knn': 0.2923531,\n",
       "  'pred_soft': 5235,\n",
       "  'soft_val': 0.3092292249202728,\n",
       "  'sim_soft': array(0.30922922, dtype=float32),\n",
       "  'pred_max_max': 5235,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.34696972370147705,\n",
       "  'sim_max_max': 0.2923531},\n",
       " 26253: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5250,\n",
       "  'pred_knn': 4711,\n",
       "  'sim_real_cl': 0.2062632,\n",
       "  'sim_best_knn': 0.3406183,\n",
       "  'pred_soft': 4711,\n",
       "  'soft_val': 0.28989294171333313,\n",
       "  'sim_soft': 0.3406183,\n",
       "  'pred_max_max': 4711,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30606937408447266,\n",
       "  'sim_max_max': 0.3406183},\n",
       " 26254: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5250,\n",
       "  'pred_knn': 2754,\n",
       "  'sim_real_cl': 0.1130294,\n",
       "  'sim_best_knn': 0.2999219,\n",
       "  'pred_soft': 2754,\n",
       "  'soft_val': 0.2839438319206238,\n",
       "  'sim_soft': 0.2999219,\n",
       "  'pred_max_max': 2754,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2812096178531647,\n",
       "  'sim_max_max': 0.2999219},\n",
       " 26259: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5251,\n",
       "  'pred_knn': 3974,\n",
       "  'sim_real_cl': 0.35248482,\n",
       "  'sim_best_knn': 0.36820236,\n",
       "  'pred_soft': 999,\n",
       "  'soft_val': 0.3098500072956085,\n",
       "  'sim_soft': 0.35403934,\n",
       "  'pred_max_max': 999,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.29583311080932617,\n",
       "  'sim_max_max': 0.35403934},\n",
       " 26294: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5258,\n",
       "  'pred_knn': 9949,\n",
       "  'sim_real_cl': 0.37760788,\n",
       "  'sim_best_knn': 0.42691666,\n",
       "  'pred_soft': 9949,\n",
       "  'soft_val': 0.3956606686115265,\n",
       "  'sim_soft': 0.42691666,\n",
       "  'pred_max_max': 9949,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.42262184619903564,\n",
       "  'sim_max_max': 0.42691666},\n",
       " 26296: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5259,\n",
       "  'pred_knn': 5259,\n",
       "  'sim_real_cl': 0.43015224,\n",
       "  'sim_best_knn': 0.43015224,\n",
       "  'pred_soft': 7998,\n",
       "  'soft_val': 0.33797788619995117,\n",
       "  'sim_soft': 0.35436487},\n",
       " 26383: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5276,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.25267005,\n",
       "  'sim_best_knn': 0.32441175,\n",
       "  'pred_soft': 4977,\n",
       "  'soft_val': 0.21108712255954742,\n",
       "  'sim_soft': 0.26786065,\n",
       "  'pred_max_max': 7256,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.19558607041835785,\n",
       "  'sim_max_max': 0.20208511},\n",
       " 26394: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5278,\n",
       "  'pred_knn': 7687,\n",
       "  'sim_real_cl': 0.3293203,\n",
       "  'sim_best_knn': 0.35671377,\n",
       "  'pred_soft': 5278,\n",
       "  'soft_val': 0.30323857069015503,\n",
       "  'sim_soft': array(0.30323857, dtype=float32),\n",
       "  'pred_max_max': 5278,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.35730504989624023,\n",
       "  'sim_max_max': 0.35671377},\n",
       " 26449: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5289,\n",
       "  'pred_knn': 6275,\n",
       "  'sim_real_cl': 0.23145756,\n",
       "  'sim_best_knn': 0.32692248,\n",
       "  'pred_soft': 5289,\n",
       "  'soft_val': 0.2769213914871216,\n",
       "  'sim_soft': array(0.2769214, dtype=float32),\n",
       "  'pred_max_max': 6275,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.29831093549728394,\n",
       "  'sim_max_max': 0.32692248},\n",
       " 26502: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5300,\n",
       "  'pred_knn': 5300,\n",
       "  'sim_real_cl': 0.4023648,\n",
       "  'sim_best_knn': 0.4023648,\n",
       "  'pred_soft': 8726,\n",
       "  'soft_val': 0.28328120708465576,\n",
       "  'sim_soft': 0.31779212},\n",
       " 26504: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5300,\n",
       "  'pred_knn': 5300,\n",
       "  'sim_real_cl': 0.29004613,\n",
       "  'sim_best_knn': 0.29004613,\n",
       "  'pred_soft': 6768,\n",
       "  'soft_val': 0.2676810026168823,\n",
       "  'sim_soft': 0.24462289,\n",
       "  'pred_max_max': 6768,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2803303897380829,\n",
       "  'sim_max_max': 0.24462289},\n",
       " 26552: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5310,\n",
       "  'pred_knn': 1930,\n",
       "  'sim_real_cl': 0.31196934,\n",
       "  'sim_best_knn': 0.33539504,\n",
       "  'pred_soft': 4924,\n",
       "  'soft_val': 0.27641671895980835,\n",
       "  'sim_soft': 0.236493,\n",
       "  'pred_max_max': 2284,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2530716061592102,\n",
       "  'sim_max_max': 0.24699149},\n",
       " 26659: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5331,\n",
       "  'pred_knn': 8664,\n",
       "  'sim_real_cl': 0.31024337,\n",
       "  'sim_best_knn': 0.33843738,\n",
       "  'pred_soft': 8664,\n",
       "  'soft_val': 0.3072851002216339,\n",
       "  'sim_soft': 0.33843738,\n",
       "  'pred_max_max': 8664,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.32937881350517273,\n",
       "  'sim_max_max': 0.33843738},\n",
       " 26669: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5333,\n",
       "  'pred_knn': 639,\n",
       "  'sim_real_cl': 0.06440951,\n",
       "  'sim_best_knn': 0.34844196,\n",
       "  'pred_soft': 9949,\n",
       "  'soft_val': 0.2191523015499115,\n",
       "  'sim_soft': 0.2011362,\n",
       "  'pred_max_max': 7794,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.31114834547042847,\n",
       "  'sim_max_max': 0.20599727},\n",
       " 26811: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5362,\n",
       "  'pred_knn': 1172,\n",
       "  'sim_real_cl': 0.29152453,\n",
       "  'sim_best_knn': 0.38177907,\n",
       "  'pred_soft': 2091,\n",
       "  'soft_val': 0.25357916951179504,\n",
       "  'sim_soft': 0.3097539,\n",
       "  'pred_max_max': 2091,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2848424017429352,\n",
       "  'sim_max_max': 0.3097539},\n",
       " 26841: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5368,\n",
       "  'pred_knn': 3433,\n",
       "  'sim_real_cl': 0.36752003,\n",
       "  'sim_best_knn': 0.4019287,\n",
       "  'pred_soft': 5368,\n",
       "  'soft_val': 0.3733774721622467,\n",
       "  'sim_soft': array(0.37337747, dtype=float32),\n",
       "  'pred_max_max': 5368,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3755975365638733,\n",
       "  'sim_max_max': 0.4019287},\n",
       " 26902: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5380,\n",
       "  'pred_knn': 4456,\n",
       "  'sim_real_cl': 0.20613277,\n",
       "  'sim_best_knn': 0.3435943,\n",
       "  'pred_soft': 420,\n",
       "  'soft_val': 0.2299392819404602,\n",
       "  'sim_soft': 0.3108744,\n",
       "  'pred_max_max': 420,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.23211826384067535,\n",
       "  'sim_max_max': 0.3108744},\n",
       " 26903: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5380,\n",
       "  'pred_knn': 2091,\n",
       "  'sim_real_cl': 0.22243263,\n",
       "  'sim_best_knn': 0.42555058,\n",
       "  'pred_soft': 2091,\n",
       "  'soft_val': 0.3939376771450043,\n",
       "  'sim_soft': 0.42555058,\n",
       "  'pred_max_max': 2091,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.42778903245925903,\n",
       "  'sim_max_max': 0.42555058},\n",
       " 26904: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5380,\n",
       "  'pred_knn': 8372,\n",
       "  'sim_real_cl': 0.3002621,\n",
       "  'sim_best_knn': 0.32896316,\n",
       "  'pred_soft': 2091,\n",
       "  'soft_val': 0.2903704345226288,\n",
       "  'sim_soft': 0.3196643,\n",
       "  'pred_max_max': 2091,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.31606751680374146,\n",
       "  'sim_max_max': 0.3196643},\n",
       " 26981: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5396,\n",
       "  'pred_knn': 1867,\n",
       "  'sim_real_cl': 0.33055452,\n",
       "  'sim_best_knn': 0.35794693,\n",
       "  'pred_soft': 5396,\n",
       "  'soft_val': 0.2989410161972046,\n",
       "  'sim_soft': array(0.29894102, dtype=float32),\n",
       "  'pred_max_max': 5396,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.2758500874042511,\n",
       "  'sim_max_max': 0.35794693},\n",
       " 27064: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5412,\n",
       "  'pred_knn': 9105,\n",
       "  'sim_real_cl': 0.2908416,\n",
       "  'sim_best_knn': 0.3245256,\n",
       "  'pred_soft': 9105,\n",
       "  'soft_val': 0.27928638458251953,\n",
       "  'sim_soft': 0.3245256,\n",
       "  'pred_max_max': 9105,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2761494517326355,\n",
       "  'sim_max_max': 0.3245256},\n",
       " 27067: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5413,\n",
       "  'pred_knn': 1141,\n",
       "  'sim_real_cl': 0.32179394,\n",
       "  'sim_best_knn': 0.36668938,\n",
       "  'pred_soft': 1141,\n",
       "  'soft_val': 0.301224946975708,\n",
       "  'sim_soft': 0.36668938,\n",
       "  'pred_max_max': 1141,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3320021331310272,\n",
       "  'sim_max_max': 0.36668938},\n",
       " 27070: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5414,\n",
       "  'pred_knn': 2664,\n",
       "  'sim_real_cl': 0.31190127,\n",
       "  'sim_best_knn': 0.35368443,\n",
       "  'pred_soft': 2664,\n",
       "  'soft_val': 0.2859690487384796,\n",
       "  'sim_soft': 0.35368443,\n",
       "  'pred_max_max': 5414,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3594212532043457,\n",
       "  'sim_max_max': 0.35368443},\n",
       " 27171: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5434,\n",
       "  'pred_knn': 1933,\n",
       "  'sim_real_cl': 0.40007854,\n",
       "  'sim_best_knn': 0.4435967,\n",
       "  'pred_soft': 5434,\n",
       "  'soft_val': 0.45130524039268494,\n",
       "  'sim_soft': array(0.45130524, dtype=float32),\n",
       "  'pred_max_max': 5434,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.39115583896636963,\n",
       "  'sim_max_max': 0.4435967},\n",
       " 27192: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5438,\n",
       "  'pred_knn': 6467,\n",
       "  'sim_real_cl': 0.33129048,\n",
       "  'sim_best_knn': 0.35976166,\n",
       "  'pred_soft': 5438,\n",
       "  'soft_val': 0.3216744065284729,\n",
       "  'sim_soft': array(0.3216744, dtype=float32),\n",
       "  'pred_max_max': 5438,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.36144018173217773,\n",
       "  'sim_max_max': 0.35976166},\n",
       " 27194: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5438,\n",
       "  'pred_knn': 8855,\n",
       "  'sim_real_cl': 0.22326405,\n",
       "  'sim_best_knn': 0.3363558,\n",
       "  'pred_soft': 8855,\n",
       "  'soft_val': 0.24441201984882355,\n",
       "  'sim_soft': 0.3363558,\n",
       "  'pred_max_max': 5438,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.2975727617740631,\n",
       "  'sim_max_max': 0.3363558},\n",
       " 27221: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5444,\n",
       "  'pred_knn': 5444,\n",
       "  'sim_real_cl': 0.33372682,\n",
       "  'sim_best_knn': 0.33372682,\n",
       "  'pred_soft': 7792,\n",
       "  'soft_val': 0.2953276038169861,\n",
       "  'sim_soft': 0.2435893,\n",
       "  'pred_max_max': 375,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28606143593788147,\n",
       "  'sim_max_max': 0.3236797},\n",
       " 27224: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5444,\n",
       "  'pred_knn': 2289,\n",
       "  'sim_real_cl': 0.28664592,\n",
       "  'sim_best_knn': 0.3662659,\n",
       "  'pred_soft': 2289,\n",
       "  'soft_val': 0.3234543204307556,\n",
       "  'sim_soft': 0.3662659,\n",
       "  'pred_max_max': 2180,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2611736059188843,\n",
       "  'sim_max_max': 0.30893597},\n",
       " 27228: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5445,\n",
       "  'pred_knn': 6428,\n",
       "  'sim_real_cl': 0.21489221,\n",
       "  'sim_best_knn': 0.360331,\n",
       "  'pred_soft': 6428,\n",
       "  'soft_val': 0.36076104640960693,\n",
       "  'sim_soft': 0.360331,\n",
       "  'pred_max_max': 6428,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.35555851459503174,\n",
       "  'sim_max_max': 0.360331},\n",
       " 27268: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5453,\n",
       "  'pred_knn': 5360,\n",
       "  'sim_real_cl': 0.37158632,\n",
       "  'sim_best_knn': 0.3742556,\n",
       "  'pred_soft': 5360,\n",
       "  'soft_val': 0.3280513286590576,\n",
       "  'sim_soft': 0.3742556,\n",
       "  'pred_max_max': 5453,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3257094919681549,\n",
       "  'sim_max_max': 0.3742556},\n",
       " 27269: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5453,\n",
       "  'pred_knn': 5453,\n",
       "  'sim_real_cl': 0.37341335,\n",
       "  'sim_best_knn': 0.37341335,\n",
       "  'pred_soft': 5031,\n",
       "  'soft_val': 0.28077390789985657,\n",
       "  'sim_soft': 0.29785624,\n",
       "  'pred_max_max': 5031,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2531108260154724,\n",
       "  'sim_max_max': 0.29785624},\n",
       " 27301: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5460,\n",
       "  'pred_knn': 2532,\n",
       "  'sim_real_cl': 0.38572073,\n",
       "  'sim_best_knn': 0.43818387,\n",
       "  'pred_soft': 2532,\n",
       "  'soft_val': 0.31319165229797363,\n",
       "  'sim_soft': 0.43818387,\n",
       "  'pred_max_max': 5460,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.30812203884124756,\n",
       "  'sim_max_max': 0.43818387},\n",
       " 27338: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5467,\n",
       "  'pred_knn': 5467,\n",
       "  'sim_real_cl': 0.51787686,\n",
       "  'sim_best_knn': 0.51787686,\n",
       "  'pred_soft': 5262,\n",
       "  'soft_val': 0.43476781249046326,\n",
       "  'sim_soft': 0.4290873},\n",
       " 27354: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5470,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.24621615,\n",
       "  'sim_best_knn': 0.47861943,\n",
       "  'pred_soft': 5795,\n",
       "  'soft_val': 0.25089800357818604,\n",
       "  'sim_soft': 0.31999147,\n",
       "  'pred_max_max': 690,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.262786328792572,\n",
       "  'sim_max_max': 0.47861943},\n",
       " 27374: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5474,\n",
       "  'pred_knn': 5474,\n",
       "  'sim_real_cl': 0.44924924,\n",
       "  'sim_best_knn': 0.44924924,\n",
       "  'pred_soft': 2916,\n",
       "  'soft_val': 0.40231481194496155,\n",
       "  'sim_soft': 0.3795607,\n",
       "  'pred_max_max': 2916,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3905293941497803,\n",
       "  'sim_max_max': 0.3795607},\n",
       " 27435: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5487,\n",
       "  'pred_knn': 5309,\n",
       "  'sim_real_cl': 0.31150764,\n",
       "  'sim_best_knn': 0.32483652,\n",
       "  'pred_soft': 5699,\n",
       "  'soft_val': 0.2622348964214325,\n",
       "  'sim_soft': 0.30811173,\n",
       "  'pred_max_max': 9493,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27963170409202576,\n",
       "  'sim_max_max': 0.28581572},\n",
       " 27459: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5491,\n",
       "  'pred_knn': 3855,\n",
       "  'sim_real_cl': 0.25044286,\n",
       "  'sim_best_knn': 0.30558136,\n",
       "  'pred_soft': 2697,\n",
       "  'soft_val': 0.24588029086589813,\n",
       "  'sim_soft': 0.25685224,\n",
       "  'pred_max_max': 5491,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.26481756567955017,\n",
       "  'sim_max_max': 0.30558136},\n",
       " 27490: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5498,\n",
       "  'pred_knn': 4589,\n",
       "  'sim_real_cl': 0.28962433,\n",
       "  'sim_best_knn': 0.34821945,\n",
       "  'pred_soft': 4589,\n",
       "  'soft_val': 0.25163429975509644,\n",
       "  'sim_soft': 0.34821945,\n",
       "  'pred_max_max': 4589,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27804994583129883,\n",
       "  'sim_max_max': 0.34821945},\n",
       " 27537: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5507,\n",
       "  'pred_knn': 1172,\n",
       "  'sim_real_cl': 0.22521412,\n",
       "  'sim_best_knn': 0.3404442,\n",
       "  'pred_soft': 2232,\n",
       "  'soft_val': 0.27327752113342285,\n",
       "  'sim_soft': 0.26017708,\n",
       "  'pred_max_max': 2698,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2768803536891937,\n",
       "  'sim_max_max': 0.31397328},\n",
       " 27566: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5513,\n",
       "  'pred_knn': 3683,\n",
       "  'sim_real_cl': 0.28741568,\n",
       "  'sim_best_knn': 0.30934292,\n",
       "  'pred_soft': 3683,\n",
       "  'soft_val': 0.26181715726852417,\n",
       "  'sim_soft': 0.30934292,\n",
       "  'pred_max_max': 3683,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25078755617141724,\n",
       "  'sim_max_max': 0.30934292},\n",
       " 27578: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5515,\n",
       "  'pred_knn': 1172,\n",
       "  'sim_real_cl': 0.34108573,\n",
       "  'sim_best_knn': 0.36985478,\n",
       "  'pred_soft': 3931,\n",
       "  'soft_val': 0.29049190878868103,\n",
       "  'sim_soft': 0.32317492,\n",
       "  'pred_max_max': 3931,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3012655973434448,\n",
       "  'sim_max_max': 0.32317492},\n",
       " 27688: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5537,\n",
       "  'pred_knn': 8572,\n",
       "  'sim_real_cl': 0.26285344,\n",
       "  'sim_best_knn': 0.2978996,\n",
       "  'pred_soft': 8572,\n",
       "  'soft_val': 0.2812962234020233,\n",
       "  'sim_soft': 0.2978996,\n",
       "  'pred_max_max': 8572,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2937473654747009,\n",
       "  'sim_max_max': 0.2978996},\n",
       " 27722: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5544,\n",
       "  'pred_knn': 9794,\n",
       "  'sim_real_cl': 0.10915394,\n",
       "  'sim_best_knn': 0.3649831,\n",
       "  'pred_soft': 7880,\n",
       "  'soft_val': 0.24330522119998932,\n",
       "  'sim_soft': 0.2879215,\n",
       "  'pred_max_max': 2091,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.22264394164085388,\n",
       "  'sim_max_max': 0.22314075},\n",
       " 27793: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5558,\n",
       "  'pred_knn': 79,\n",
       "  'sim_real_cl': 0.39010167,\n",
       "  'sim_best_knn': 0.40289837,\n",
       "  'pred_soft': 5999,\n",
       "  'soft_val': 0.33145737648010254,\n",
       "  'sim_soft': 0.3811497,\n",
       "  'pred_max_max': 2804,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3024938404560089,\n",
       "  'sim_max_max': 0.34921807},\n",
       " 27836: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5567,\n",
       "  'pred_knn': 1432,\n",
       "  'sim_real_cl': 0.2890413,\n",
       "  'sim_best_knn': 0.32461026,\n",
       "  'pred_soft': 5567,\n",
       "  'soft_val': 0.29552632570266724,\n",
       "  'sim_soft': array(0.29552633, dtype=float32),\n",
       "  'pred_max_max': 2623,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2643783390522003,\n",
       "  'sim_max_max': 0.29474166},\n",
       " 27837: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5567,\n",
       "  'pred_knn': 2593,\n",
       "  'sim_real_cl': 0.3009094,\n",
       "  'sim_best_knn': 0.33435118,\n",
       "  'pred_soft': 5567,\n",
       "  'soft_val': 0.28983381390571594,\n",
       "  'sim_soft': array(0.2898338, dtype=float32),\n",
       "  'pred_max_max': 3360,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2606373429298401,\n",
       "  'sim_max_max': 0.25782862},\n",
       " 27982: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5596,\n",
       "  'pred_knn': 2106,\n",
       "  'sim_real_cl': 0.20865032,\n",
       "  'sim_best_knn': 0.30652684,\n",
       "  'pred_soft': 2106,\n",
       "  'soft_val': 0.2575400769710541,\n",
       "  'sim_soft': 0.30652684,\n",
       "  'pred_max_max': 5596,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.23908504843711853,\n",
       "  'sim_max_max': 0.30652684},\n",
       " 27988: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5597,\n",
       "  'pred_knn': 5597,\n",
       "  'sim_real_cl': 0.35207683,\n",
       "  'sim_best_knn': 0.35207683,\n",
       "  'pred_soft': 5601,\n",
       "  'soft_val': 0.28928807377815247,\n",
       "  'sim_soft': 0.2736744,\n",
       "  'pred_max_max': 3650,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.33099403977394104,\n",
       "  'sim_max_max': 0.3444552},\n",
       " 27989: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5597,\n",
       "  'pred_knn': 4629,\n",
       "  'sim_real_cl': 0.35554218,\n",
       "  'sim_best_knn': 0.35901195,\n",
       "  'pred_soft': 5597,\n",
       "  'soft_val': 0.2810375988483429,\n",
       "  'sim_soft': array(0.2810376, dtype=float32),\n",
       "  'pred_max_max': 5597,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3058856427669525,\n",
       "  'sim_max_max': 0.35901195},\n",
       " 28020: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5604,\n",
       "  'pred_knn': 8892,\n",
       "  'sim_real_cl': 0.32947326,\n",
       "  'sim_best_knn': 0.3607096,\n",
       "  'pred_soft': 5604,\n",
       "  'soft_val': 0.3462880253791809,\n",
       "  'sim_soft': array(0.34628803, dtype=float32),\n",
       "  'pred_max_max': 5604,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.29433172941207886,\n",
       "  'sim_max_max': 0.3607096},\n",
       " 28094: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5618,\n",
       "  'pred_knn': 7870,\n",
       "  'sim_real_cl': 0.1659408,\n",
       "  'sim_best_knn': 0.32822964,\n",
       "  'pred_soft': 3313,\n",
       "  'soft_val': 0.23984725773334503,\n",
       "  'sim_soft': 0.22622886,\n",
       "  'pred_max_max': 2725,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.23859384655952454,\n",
       "  'sim_max_max': 0.32291335},\n",
       " 28146: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5629,\n",
       "  'pred_knn': 4912,\n",
       "  'sim_real_cl': 0.32464027,\n",
       "  'sim_best_knn': 0.34913033,\n",
       "  'pred_soft': 4912,\n",
       "  'soft_val': 0.313865065574646,\n",
       "  'sim_soft': 0.34913033,\n",
       "  'pred_max_max': 5629,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3518543839454651,\n",
       "  'sim_max_max': 0.34913033},\n",
       " 28235: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5647,\n",
       "  'pred_knn': 3928,\n",
       "  'sim_real_cl': 0.241647,\n",
       "  'sim_best_knn': 0.25849247,\n",
       "  'pred_soft': 5647,\n",
       "  'soft_val': 0.29399144649505615,\n",
       "  'sim_soft': array(0.29399145, dtype=float32),\n",
       "  'pred_max_max': 5647,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.2855622172355652,\n",
       "  'sim_max_max': 0.25849247},\n",
       " 28238: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5647,\n",
       "  'pred_knn': 5647,\n",
       "  'sim_real_cl': 0.34791452,\n",
       "  'sim_best_knn': 0.34791452,\n",
       "  'pred_soft': 2091,\n",
       "  'soft_val': 0.3038741946220398,\n",
       "  'sim_soft': 0.31006867},\n",
       " 28292: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5658,\n",
       "  'pred_knn': 4174,\n",
       "  'sim_real_cl': 0.3554474,\n",
       "  'sim_best_knn': 0.40799525,\n",
       "  'pred_soft': 5658,\n",
       "  'soft_val': 0.3193371891975403,\n",
       "  'sim_soft': array(0.3193372, dtype=float32),\n",
       "  'pred_max_max': 4174,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.34326112270355225,\n",
       "  'sim_max_max': 0.40799525},\n",
       " 28308: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5661,\n",
       "  'pred_knn': 4533,\n",
       "  'sim_real_cl': 0.26431417,\n",
       "  'sim_best_knn': 0.29610404,\n",
       "  'pred_soft': 4533,\n",
       "  'soft_val': 0.2230415940284729,\n",
       "  'sim_soft': 0.29610404,\n",
       "  'pred_max_max': 1188,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26363465189933777,\n",
       "  'sim_max_max': 0.24572189},\n",
       " 28313: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5662,\n",
       "  'pred_knn': 6367,\n",
       "  'sim_real_cl': 0.3547568,\n",
       "  'sim_best_knn': 0.36227587,\n",
       "  'pred_soft': 5585,\n",
       "  'soft_val': 0.27536293864250183,\n",
       "  'sim_soft': 0.3049789,\n",
       "  'pred_max_max': 5585,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27082401514053345,\n",
       "  'sim_max_max': 0.3049789},\n",
       " 28315: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5663,\n",
       "  'pred_knn': 77,\n",
       "  'sim_real_cl': 0.23908038,\n",
       "  'sim_best_knn': 0.31212878,\n",
       "  'pred_soft': 993,\n",
       "  'soft_val': 0.26254087686538696,\n",
       "  'sim_soft': 0.29558438,\n",
       "  'pred_max_max': 4342,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28083088994026184,\n",
       "  'sim_max_max': 0.27664703},\n",
       " 28343: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5668,\n",
       "  'pred_knn': 8103,\n",
       "  'sim_real_cl': 0.10448645,\n",
       "  'sim_best_knn': 0.25446022,\n",
       "  'pred_soft': 8234,\n",
       "  'soft_val': 0.2716886103153229,\n",
       "  'sim_soft': 0.23480006,\n",
       "  'pred_max_max': 8234,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2785992920398712,\n",
       "  'sim_max_max': 0.23480006},\n",
       " 28389: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5677,\n",
       "  'pred_knn': 5677,\n",
       "  'sim_real_cl': 0.39414784,\n",
       "  'sim_best_knn': 0.39414784,\n",
       "  'pred_soft': 5448,\n",
       "  'soft_val': 0.31773877143859863,\n",
       "  'sim_soft': 0.33623123,\n",
       "  'pred_max_max': 5497,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3156447410583496,\n",
       "  'sim_max_max': 0.33628124},\n",
       " 28422: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5684,\n",
       "  'pred_knn': 7493,\n",
       "  'sim_real_cl': 0.31662813,\n",
       "  'sim_best_knn': 0.3382891,\n",
       "  'pred_soft': 5684,\n",
       "  'soft_val': 0.30451127886772156,\n",
       "  'sim_soft': array(0.30451128, dtype=float32),\n",
       "  'pred_max_max': 7493,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3395777642726898,\n",
       "  'sim_max_max': 0.3382891},\n",
       " 28423: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5684,\n",
       "  'pred_knn': 863,\n",
       "  'sim_real_cl': 0.2955552,\n",
       "  'sim_best_knn': 0.30046278,\n",
       "  'pred_soft': 5684,\n",
       "  'soft_val': 0.2609819769859314,\n",
       "  'sim_soft': array(0.26098198, dtype=float32),\n",
       "  'pred_max_max': 5684,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.2598380446434021,\n",
       "  'sim_max_max': 0.30046278},\n",
       " 28424: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5684,\n",
       "  'pred_knn': 639,\n",
       "  'sim_real_cl': 0.11143908,\n",
       "  'sim_best_knn': 0.31419563,\n",
       "  'pred_soft': 9949,\n",
       "  'soft_val': 0.1966663897037506,\n",
       "  'sim_soft': 0.20212394,\n",
       "  'pred_max_max': 6018,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.20811669528484344,\n",
       "  'sim_max_max': 0.043803472},\n",
       " 28443: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5688,\n",
       "  'pred_knn': 9487,\n",
       "  'sim_real_cl': 0.25712693,\n",
       "  'sim_best_knn': 0.3131551,\n",
       "  'pred_soft': 9487,\n",
       "  'soft_val': 0.2471996545791626,\n",
       "  'sim_soft': 0.3131551,\n",
       "  'pred_max_max': 9487,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27669090032577515,\n",
       "  'sim_max_max': 0.3131551},\n",
       " 28518: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5703,\n",
       "  'pred_knn': 8811,\n",
       "  'sim_real_cl': 0.18022124,\n",
       "  'sim_best_knn': 0.22376348,\n",
       "  'pred_soft': 8034,\n",
       "  'soft_val': 0.22301509976387024,\n",
       "  'sim_soft': 0.20788863,\n",
       "  'pred_max_max': 8034,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.20793217420578003,\n",
       "  'sim_max_max': 0.20788863},\n",
       " 28534: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5706,\n",
       "  'pred_knn': 5706,\n",
       "  'sim_real_cl': 0.3502481,\n",
       "  'sim_best_knn': 0.3502481,\n",
       "  'pred_soft': 4944,\n",
       "  'soft_val': 0.2868747413158417,\n",
       "  'sim_soft': 0.3137556,\n",
       "  'pred_max_max': 4944,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.30432480573654175,\n",
       "  'sim_max_max': 0.3137556},\n",
       " 28558: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5711,\n",
       "  'pred_knn': 5711,\n",
       "  'sim_real_cl': 0.3439225,\n",
       "  'sim_best_knn': 0.3439225,\n",
       "  'pred_soft': 2597,\n",
       "  'soft_val': 0.2954736053943634,\n",
       "  'sim_soft': 0.3347939},\n",
       " 28619: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5723,\n",
       "  'pred_knn': 5723,\n",
       "  'sim_real_cl': 0.46648043,\n",
       "  'sim_best_knn': 0.46648043,\n",
       "  'pred_soft': 5136,\n",
       "  'soft_val': 0.35049036145210266,\n",
       "  'sim_soft': 0.32490507},\n",
       " 28629: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5725,\n",
       "  'pred_knn': 2532,\n",
       "  'sim_real_cl': 0.24791734,\n",
       "  'sim_best_knn': 0.3409995,\n",
       "  'pred_soft': 4938,\n",
       "  'soft_val': 0.2745944857597351,\n",
       "  'sim_soft': 0.28458226,\n",
       "  'pred_max_max': 4042,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2753106355667114,\n",
       "  'sim_max_max': 0.34085816},\n",
       " 28696: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5739,\n",
       "  'pred_knn': 4525,\n",
       "  'sim_real_cl': 0.059896484,\n",
       "  'sim_best_knn': 0.29472753,\n",
       "  'pred_soft': 5483,\n",
       "  'soft_val': 0.21357041597366333,\n",
       "  'sim_soft': 0.22325867,\n",
       "  'pred_max_max': 8731,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2070622444152832,\n",
       "  'sim_max_max': 0.1265185},\n",
       " 28717: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5743,\n",
       "  'pred_knn': 4104,\n",
       "  'sim_real_cl': 0.2556638,\n",
       "  'sim_best_knn': 0.4118693,\n",
       "  'pred_soft': 4726,\n",
       "  'soft_val': 0.251455694437027,\n",
       "  'sim_soft': 0.25694606,\n",
       "  'pred_max_max': 4104,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3045373260974884,\n",
       "  'sim_max_max': 0.4118693},\n",
       " 28723: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5744,\n",
       "  'pred_knn': 7528,\n",
       "  'sim_real_cl': 0.11680098,\n",
       "  'sim_best_knn': 0.34630513,\n",
       "  'pred_soft': 9624,\n",
       "  'soft_val': 0.19571930170059204,\n",
       "  'sim_soft': 0.20164573,\n",
       "  'pred_max_max': 7528,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.226423978805542,\n",
       "  'sim_max_max': 0.34630513},\n",
       " 28828: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5765,\n",
       "  'pred_knn': 5765,\n",
       "  'sim_real_cl': 0.42053536,\n",
       "  'sim_best_knn': 0.42053536,\n",
       "  'pred_soft': 5368,\n",
       "  'soft_val': 0.2839718759059906,\n",
       "  'sim_soft': 0.3136716,\n",
       "  'pred_max_max': 2091,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3339316248893738,\n",
       "  'sim_max_max': 0.38718012},\n",
       " 28861: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5772,\n",
       "  'pred_knn': 352,\n",
       "  'sim_real_cl': 0.17629695,\n",
       "  'sim_best_knn': 0.2856402,\n",
       "  'pred_soft': 352,\n",
       "  'soft_val': 0.25205162167549133,\n",
       "  'sim_soft': 0.2856402,\n",
       "  'pred_max_max': 352,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27507758140563965,\n",
       "  'sim_max_max': 0.2856402},\n",
       " 28863: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5772,\n",
       "  'pred_knn': 2725,\n",
       "  'sim_real_cl': 0.18265533,\n",
       "  'sim_best_knn': 0.31495175,\n",
       "  'pred_soft': 352,\n",
       "  'soft_val': 0.25819146633148193,\n",
       "  'sim_soft': 0.29843402,\n",
       "  'pred_max_max': 352,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2876594066619873,\n",
       "  'sim_max_max': 0.29843402},\n",
       " 28867: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5773,\n",
       "  'pred_knn': 5773,\n",
       "  'sim_real_cl': 0.354806,\n",
       "  'sim_best_knn': 0.354806,\n",
       "  'pred_soft': 8369,\n",
       "  'soft_val': 0.25310373306274414,\n",
       "  'sim_soft': 0.26716962,\n",
       "  'pred_max_max': 7120,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24980539083480835,\n",
       "  'sim_max_max': 0.2939943},\n",
       " 28868: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5773,\n",
       "  'pred_knn': 5773,\n",
       "  'sim_real_cl': 0.43027744,\n",
       "  'sim_best_knn': 0.43027744,\n",
       "  'pred_soft': 7863,\n",
       "  'soft_val': 0.27107158303260803,\n",
       "  'sim_soft': 0.27055782},\n",
       " 28969: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5793,\n",
       "  'pred_knn': 3902,\n",
       "  'sim_real_cl': 0.30189458,\n",
       "  'sim_best_knn': 0.43031237,\n",
       "  'pred_soft': 5811,\n",
       "  'soft_val': 0.24170136451721191,\n",
       "  'sim_soft': 0.34948963,\n",
       "  'pred_max_max': 5811,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26797619462013245,\n",
       "  'sim_max_max': 0.34948963},\n",
       " 29070: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5814,\n",
       "  'pred_knn': 2930,\n",
       "  'sim_real_cl': 0.15929079,\n",
       "  'sim_best_knn': 0.27597928,\n",
       "  'pred_soft': 2930,\n",
       "  'soft_val': 0.25674349069595337,\n",
       "  'sim_soft': 0.27597928,\n",
       "  'pred_max_max': 2930,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27012884616851807,\n",
       "  'sim_max_max': 0.27597928},\n",
       " 29071: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5814,\n",
       "  'pred_knn': 4227,\n",
       "  'sim_real_cl': 0.18087065,\n",
       "  'sim_best_knn': 0.2640723,\n",
       "  'pred_soft': 8818,\n",
       "  'soft_val': 0.26746657490730286,\n",
       "  'sim_soft': 0.229858,\n",
       "  'pred_max_max': 4227,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2577275335788727,\n",
       "  'sim_max_max': 0.2640723},\n",
       " 29209: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5841,\n",
       "  'pred_knn': 2478,\n",
       "  'sim_real_cl': 0.324534,\n",
       "  'sim_best_knn': 0.3298033,\n",
       "  'pred_soft': 2597,\n",
       "  'soft_val': 0.26974427700042725,\n",
       "  'sim_soft': 0.29448035,\n",
       "  'pred_max_max': 5841,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.2711290419101715,\n",
       "  'sim_max_max': 0.3298033},\n",
       " 29282: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5856,\n",
       "  'pred_knn': 639,\n",
       "  'sim_real_cl': -0.03729351,\n",
       "  'sim_best_knn': 0.24127492,\n",
       "  'pred_soft': 1752,\n",
       "  'soft_val': 0.20692870020866394,\n",
       "  'sim_soft': 0.18332608,\n",
       "  'pred_max_max': 7794,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2655278146266937,\n",
       "  'sim_max_max': 0.14475948},\n",
       " 29291: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5858,\n",
       "  'pred_knn': 5284,\n",
       "  'sim_real_cl': 0.07985321,\n",
       "  'sim_best_knn': 0.2811549,\n",
       "  'pred_soft': 7863,\n",
       "  'soft_val': 0.23605912923812866,\n",
       "  'sim_soft': 0.24787523,\n",
       "  'pred_max_max': 7863,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2636248469352722,\n",
       "  'sim_max_max': 0.24787523},\n",
       " 29299: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5859,\n",
       "  'pred_knn': 5859,\n",
       "  'sim_real_cl': 0.35218206,\n",
       "  'sim_best_knn': 0.35218206,\n",
       "  'pred_soft': 8666,\n",
       "  'soft_val': 0.23989735543727875,\n",
       "  'sim_soft': 0.23270002},\n",
       " 29357: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5871,\n",
       "  'pred_knn': 3433,\n",
       "  'sim_real_cl': 0.3019581,\n",
       "  'sim_best_knn': 0.4202007,\n",
       "  'pred_soft': 5871,\n",
       "  'soft_val': 0.29315027594566345,\n",
       "  'sim_soft': array(0.29315028, dtype=float32),\n",
       "  'pred_max_max': 394,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3002873361110687,\n",
       "  'sim_max_max': 0.29594243},\n",
       " 29359: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5871,\n",
       "  'pred_knn': 8457,\n",
       "  'sim_real_cl': 0.29593134,\n",
       "  'sim_best_knn': 0.34733546,\n",
       "  'pred_soft': 8457,\n",
       "  'soft_val': 0.287802129983902,\n",
       "  'sim_soft': 0.34733546,\n",
       "  'pred_max_max': 5871,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.33323463797569275,\n",
       "  'sim_max_max': 0.34733546},\n",
       " 29398: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5879,\n",
       "  'pred_knn': 5879,\n",
       "  'sim_real_cl': 0.38017404,\n",
       "  'sim_best_knn': 0.38017404,\n",
       "  'pred_soft': 242,\n",
       "  'soft_val': 0.25793391466140747,\n",
       "  'sim_soft': 0.29012585,\n",
       "  'pred_max_max': 242,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3067617118358612,\n",
       "  'sim_max_max': 0.29012585},\n",
       " 29473: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5894,\n",
       "  'pred_knn': 690,\n",
       "  'sim_real_cl': 0.2456581,\n",
       "  'sim_best_knn': 0.31960177,\n",
       "  'pred_soft': 3871,\n",
       "  'soft_val': 0.2625856399536133,\n",
       "  'sim_soft': 0.2867856,\n",
       "  'pred_max_max': 3871,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2708760201931,\n",
       "  'sim_max_max': 0.2867856},\n",
       " 29535: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5907,\n",
       "  'pred_knn': 5561,\n",
       "  'sim_real_cl': 0.2223362,\n",
       "  'sim_best_knn': 0.3694883,\n",
       "  'pred_soft': 8484,\n",
       "  'soft_val': 0.27559345960617065,\n",
       "  'sim_soft': 0.24406102,\n",
       "  'pred_max_max': 3712,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2831142246723175,\n",
       "  'sim_max_max': 0.29371095},\n",
       " 29592: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5918,\n",
       "  'pred_knn': 5918,\n",
       "  'sim_real_cl': 0.38327783,\n",
       "  'sim_best_knn': 0.38327783,\n",
       "  'pred_soft': 2829,\n",
       "  'soft_val': 0.3010343313217163,\n",
       "  'sim_soft': 0.35911065,\n",
       "  'pred_max_max': 2829,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.32200247049331665,\n",
       "  'sim_max_max': 0.35911065},\n",
       " 29593: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5918,\n",
       "  'pred_knn': 3123,\n",
       "  'sim_real_cl': 0.2742049,\n",
       "  'sim_best_knn': 0.3056481,\n",
       "  'pred_soft': 999,\n",
       "  'soft_val': 0.22441479563713074,\n",
       "  'sim_soft': 0.28408945,\n",
       "  'pred_max_max': 951,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2527918517589569,\n",
       "  'sim_max_max': 0.25374234},\n",
       " 29594: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5918,\n",
       "  'pred_knn': 6334,\n",
       "  'sim_real_cl': 0.28829354,\n",
       "  'sim_best_knn': 0.3105453,\n",
       "  'pred_soft': 3697,\n",
       "  'soft_val': 0.25150737166404724,\n",
       "  'sim_soft': 0.29513115,\n",
       "  'pred_max_max': 5918,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.31677281856536865,\n",
       "  'sim_max_max': 0.3105453},\n",
       " 29639: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5927,\n",
       "  'pred_knn': 4280,\n",
       "  'sim_real_cl': 0.18264255,\n",
       "  'sim_best_knn': 0.2877031,\n",
       "  'pred_soft': 4976,\n",
       "  'soft_val': 0.22371000051498413,\n",
       "  'sim_soft': 0.27146608,\n",
       "  'pred_max_max': 4280,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.250698983669281,\n",
       "  'sim_max_max': 0.2877031},\n",
       " 29657: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 5931,\n",
       "  'pred_knn': 5931,\n",
       "  'sim_real_cl': 0.32881102,\n",
       "  'sim_best_knn': 0.32881102,\n",
       "  'pred_soft': 4729,\n",
       "  'soft_val': 0.3051682114601135,\n",
       "  'sim_soft': 0.32502776},\n",
       " 29787: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5957,\n",
       "  'pred_knn': 3197,\n",
       "  'sim_real_cl': 0.08379334,\n",
       "  'sim_best_knn': 0.32746822,\n",
       "  'pred_soft': 8768,\n",
       "  'soft_val': 0.26957717537879944,\n",
       "  'sim_soft': 0.2925427,\n",
       "  'pred_max_max': 8312,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25595173239707947,\n",
       "  'sim_max_max': 0.269449},\n",
       " 29832: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5966,\n",
       "  'pred_knn': 3197,\n",
       "  'sim_real_cl': 0.118300006,\n",
       "  'sim_best_knn': 0.3010286,\n",
       "  'pred_soft': 7833,\n",
       "  'soft_val': 0.20201227068901062,\n",
       "  'sim_soft': 0.23096532,\n",
       "  'pred_max_max': 6018,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2440330982208252,\n",
       "  'sim_max_max': 0.06583424},\n",
       " 29848: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5969,\n",
       "  'pred_knn': 4224,\n",
       "  'sim_real_cl': 0.2603593,\n",
       "  'sim_best_knn': 0.36325026,\n",
       "  'pred_soft': 4224,\n",
       "  'soft_val': 0.2827788293361664,\n",
       "  'sim_soft': 0.36325026,\n",
       "  'pred_max_max': 4224,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3258481025695801,\n",
       "  'sim_max_max': 0.36325026},\n",
       " 29849: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5969,\n",
       "  'pred_knn': 3854,\n",
       "  'sim_real_cl': 0.08633037,\n",
       "  'sim_best_knn': 0.28591657,\n",
       "  'pred_soft': 3854,\n",
       "  'soft_val': 0.26802489161491394,\n",
       "  'sim_soft': 0.28591657,\n",
       "  'pred_max_max': 3854,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2631896138191223,\n",
       "  'sim_max_max': 0.28591657},\n",
       " 29909: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5981,\n",
       "  'pred_knn': 7962,\n",
       "  'sim_real_cl': 0.41091377,\n",
       "  'sim_best_knn': 0.51664203,\n",
       "  'pred_soft': 7962,\n",
       "  'soft_val': 0.3396678566932678,\n",
       "  'sim_soft': 0.51664203,\n",
       "  'pred_max_max': 7962,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.40274590253829956,\n",
       "  'sim_max_max': 0.51664203},\n",
       " 29932: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5986,\n",
       "  'pred_knn': 8688,\n",
       "  'sim_real_cl': 0.3435217,\n",
       "  'sim_best_knn': 0.35529563,\n",
       "  'pred_soft': 5986,\n",
       "  'soft_val': 0.2713465988636017,\n",
       "  'sim_soft': array(0.2713466, dtype=float32),\n",
       "  'pred_max_max': 7031,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.29810941219329834,\n",
       "  'sim_max_max': 0.3384047},\n",
       " 29940: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5988,\n",
       "  'pred_knn': 6587,\n",
       "  'sim_real_cl': 0.20690116,\n",
       "  'sim_best_knn': 0.31795788,\n",
       "  'pred_soft': 4670,\n",
       "  'soft_val': 0.24857546389102936,\n",
       "  'sim_soft': 0.29194158,\n",
       "  'pred_max_max': 6587,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27638816833496094,\n",
       "  'sim_max_max': 0.31795788},\n",
       " 29997: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 5999,\n",
       "  'pred_knn': 1836,\n",
       "  'sim_real_cl': 0.49166754,\n",
       "  'sim_best_knn': 0.4974862,\n",
       "  'pred_soft': 5999,\n",
       "  'soft_val': 0.46161478757858276,\n",
       "  'sim_soft': array(0.4616148, dtype=float32),\n",
       "  'pred_max_max': 1836,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.44729769229888916,\n",
       "  'sim_max_max': 0.4974862},\n",
       " 30020: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6004,\n",
       "  'pred_knn': 6004,\n",
       "  'sim_real_cl': 0.4946598,\n",
       "  'sim_best_knn': 0.4946598,\n",
       "  'pred_soft': 6043,\n",
       "  'soft_val': 0.28381702303886414,\n",
       "  'sim_soft': 0.3061484},\n",
       " 30022: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6004,\n",
       "  'pred_knn': 6004,\n",
       "  'sim_real_cl': 0.6069566,\n",
       "  'sim_best_knn': 0.6069566,\n",
       "  'pred_soft': 3495,\n",
       "  'soft_val': 0.26520150899887085,\n",
       "  'sim_soft': 0.2861686},\n",
       " 30023: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6004,\n",
       "  'pred_knn': 6004,\n",
       "  'sim_real_cl': 0.4778593,\n",
       "  'sim_best_knn': 0.4778593,\n",
       "  'pred_soft': 6043,\n",
       "  'soft_val': 0.29414859414100647,\n",
       "  'sim_soft': 0.29871213},\n",
       " 30071: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6014,\n",
       "  'pred_knn': 6014,\n",
       "  'sim_real_cl': 0.41877502,\n",
       "  'sim_best_knn': 0.41877502,\n",
       "  'pred_soft': 2770,\n",
       "  'soft_val': 0.3675205707550049,\n",
       "  'sim_soft': 0.38173804,\n",
       "  'pred_max_max': 2770,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.35084253549575806,\n",
       "  'sim_max_max': 0.38173804},\n",
       " 30086: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6017,\n",
       "  'pred_knn': 7670,\n",
       "  'sim_real_cl': 0.3876703,\n",
       "  'sim_best_knn': 0.38955012,\n",
       "  'pred_soft': 7670,\n",
       "  'soft_val': 0.2827090322971344,\n",
       "  'sim_soft': 0.38955012,\n",
       "  'pred_max_max': 6017,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3181929290294647,\n",
       "  'sim_max_max': 0.38955012},\n",
       " 30122: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6024,\n",
       "  'pred_knn': 6024,\n",
       "  'sim_real_cl': 0.3634157,\n",
       "  'sim_best_knn': 0.3634157,\n",
       "  'pred_soft': 2429,\n",
       "  'soft_val': 0.29131028056144714,\n",
       "  'sim_soft': 0.22706924},\n",
       " 30123: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6024,\n",
       "  'pred_knn': 6024,\n",
       "  'sim_real_cl': 0.48002037,\n",
       "  'sim_best_knn': 0.48002037,\n",
       "  'pred_soft': 896,\n",
       "  'soft_val': 0.32248377799987793,\n",
       "  'sim_soft': 0.35819626,\n",
       "  'pred_max_max': 896,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.35907796025276184,\n",
       "  'sim_max_max': 0.35819626},\n",
       " 30211: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6042,\n",
       "  'pred_knn': 6042,\n",
       "  'sim_real_cl': 0.33545485,\n",
       "  'sim_best_knn': 0.33545485,\n",
       "  'pred_soft': 2380,\n",
       "  'soft_val': 0.2878583073616028,\n",
       "  'sim_soft': 0.31826305,\n",
       "  'pred_max_max': 2084,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3087857961654663,\n",
       "  'sim_max_max': 0.28982812},\n",
       " 30299: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6059,\n",
       "  'pred_knn': 6059,\n",
       "  'sim_real_cl': 0.34180528,\n",
       "  'sim_best_knn': 0.34180528,\n",
       "  'pred_soft': 6765,\n",
       "  'soft_val': 0.284244179725647,\n",
       "  'sim_soft': 0.26733685},\n",
       " 30372: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6074,\n",
       "  'pred_knn': 7264,\n",
       "  'sim_real_cl': 0.312692,\n",
       "  'sim_best_knn': 0.3315196,\n",
       "  'pred_soft': 4992,\n",
       "  'soft_val': 0.2501988410949707,\n",
       "  'sim_soft': 0.26134592,\n",
       "  'pred_max_max': 7264,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28639912605285645,\n",
       "  'sim_max_max': 0.3315196},\n",
       " 30400: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6080,\n",
       "  'pred_knn': 2223,\n",
       "  'sim_real_cl': 0.31644708,\n",
       "  'sim_best_knn': 0.34526652,\n",
       "  'pred_soft': 6080,\n",
       "  'soft_val': 0.32168594002723694,\n",
       "  'sim_soft': array(0.32168594, dtype=float32),\n",
       "  'pred_max_max': 6080,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.36709752678871155,\n",
       "  'sim_max_max': 0.34526652},\n",
       " 30419: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6083,\n",
       "  'pred_knn': 6083,\n",
       "  'sim_real_cl': 0.41883284,\n",
       "  'sim_best_knn': 0.41883284,\n",
       "  'pred_soft': 890,\n",
       "  'soft_val': 0.3130059540271759,\n",
       "  'sim_soft': 0.32522297},\n",
       " 30448: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6089,\n",
       "  'pred_knn': 6089,\n",
       "  'sim_real_cl': 0.38128066,\n",
       "  'sim_best_knn': 0.38128066,\n",
       "  'pred_soft': 5418,\n",
       "  'soft_val': 0.3535133898258209,\n",
       "  'sim_soft': 0.35051435},\n",
       " 30483: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6096,\n",
       "  'pred_knn': 9824,\n",
       "  'sim_real_cl': 0.1549372,\n",
       "  'sim_best_knn': 0.29506606,\n",
       "  'pred_soft': 5358,\n",
       "  'soft_val': 0.21995709836483002,\n",
       "  'sim_soft': 0.21447347,\n",
       "  'pred_max_max': 9824,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.23325584828853607,\n",
       "  'sim_max_max': 0.29506606},\n",
       " 30488: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6097,\n",
       "  'pred_knn': 886,\n",
       "  'sim_real_cl': 0.33813292,\n",
       "  'sim_best_knn': 0.37060326,\n",
       "  'pred_soft': 9571,\n",
       "  'soft_val': 0.26212719082832336,\n",
       "  'sim_soft': 0.25143838,\n",
       "  'pred_max_max': 886,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3021697998046875,\n",
       "  'sim_max_max': 0.37060326},\n",
       " 30491: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6098,\n",
       "  'pred_knn': 6098,\n",
       "  'sim_real_cl': 0.41073328,\n",
       "  'sim_best_knn': 0.41073328,\n",
       "  'pred_soft': 9791,\n",
       "  'soft_val': 0.2770296335220337,\n",
       "  'sim_soft': 0.28005415},\n",
       " 30518: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6103,\n",
       "  'pred_knn': 6420,\n",
       "  'sim_real_cl': 0.31927615,\n",
       "  'sim_best_knn': 0.41940522,\n",
       "  'pred_soft': 980,\n",
       "  'soft_val': 0.26342782378196716,\n",
       "  'sim_soft': 0.25318548,\n",
       "  'pred_max_max': 6420,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3382003605365753,\n",
       "  'sim_max_max': 0.41940522},\n",
       " 30572: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6114,\n",
       "  'pred_knn': 4179,\n",
       "  'sim_real_cl': 0.2776748,\n",
       "  'sim_best_knn': 0.41131014,\n",
       "  'pred_soft': 9013,\n",
       "  'soft_val': 0.2791382670402527,\n",
       "  'sim_soft': 0.29277885,\n",
       "  'pred_max_max': 4179,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3565421402454376,\n",
       "  'sim_max_max': 0.41131014},\n",
       " 30573: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6114,\n",
       "  'pred_knn': 3040,\n",
       "  'sim_real_cl': 0.18143666,\n",
       "  'sim_best_knn': 0.35151237,\n",
       "  'pred_soft': 5709,\n",
       "  'soft_val': 0.3070929944515228,\n",
       "  'sim_soft': 0.3187726,\n",
       "  'pred_max_max': 5709,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3270149827003479,\n",
       "  'sim_max_max': 0.3187726},\n",
       " 30574: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6114,\n",
       "  'pred_knn': 6114,\n",
       "  'sim_real_cl': 0.5331725,\n",
       "  'sim_best_knn': 0.5331725,\n",
       "  'pred_soft': 5338,\n",
       "  'soft_val': 0.33205217123031616,\n",
       "  'sim_soft': 0.36113757},\n",
       " 30645: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6129,\n",
       "  'pred_knn': 9136,\n",
       "  'sim_real_cl': 0.25643575,\n",
       "  'sim_best_knn': 0.35635126,\n",
       "  'pred_soft': 2890,\n",
       "  'soft_val': 0.2556171715259552,\n",
       "  'sim_soft': 0.30518717,\n",
       "  'pred_max_max': 9136,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.263519287109375,\n",
       "  'sim_max_max': 0.35635126},\n",
       " 30647: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6129,\n",
       "  'pred_knn': 9136,\n",
       "  'sim_real_cl': 0.24006173,\n",
       "  'sim_best_knn': 0.298338,\n",
       "  'pred_soft': 7693,\n",
       "  'soft_val': 0.2130037099123001,\n",
       "  'sim_soft': 0.2492125,\n",
       "  'pred_max_max': 9136,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.22461888194084167,\n",
       "  'sim_max_max': 0.298338},\n",
       " 30655: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6131,\n",
       "  'pred_knn': 7477,\n",
       "  'sim_real_cl': 0.38646325,\n",
       "  'sim_best_knn': 0.4005124,\n",
       "  'pred_soft': 6131,\n",
       "  'soft_val': 0.33339715003967285,\n",
       "  'sim_soft': array(0.33339715, dtype=float32),\n",
       "  'pred_max_max': 6131,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3666601777076721,\n",
       "  'sim_max_max': 0.4005124},\n",
       " 30736: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6147,\n",
       "  'pred_knn': 7570,\n",
       "  'sim_real_cl': 0.20580238,\n",
       "  'sim_best_knn': 0.2469007,\n",
       "  'pred_soft': 2360,\n",
       "  'soft_val': 0.22623102366924286,\n",
       "  'sim_soft': 0.19263151,\n",
       "  'pred_max_max': 9673,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2151242345571518,\n",
       "  'sim_max_max': 0.16928595},\n",
       " 30737: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6147,\n",
       "  'pred_knn': 6147,\n",
       "  'sim_real_cl': 0.56660736,\n",
       "  'sim_best_knn': 0.56660736,\n",
       "  'pred_soft': 6983,\n",
       "  'soft_val': 0.33651426434516907,\n",
       "  'sim_soft': 0.36194998},\n",
       " 30754: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6150,\n",
       "  'pred_knn': 6150,\n",
       "  'sim_real_cl': 0.41856343,\n",
       "  'sim_best_knn': 0.41856343,\n",
       "  'pred_soft': 5098,\n",
       "  'soft_val': 0.30715224146842957,\n",
       "  'sim_soft': 0.31434575,\n",
       "  'pred_max_max': 747,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.32061225175857544,\n",
       "  'sim_max_max': 0.32778668},\n",
       " 30777: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6155,\n",
       "  'pred_knn': 1336,\n",
       "  'sim_real_cl': 0.14888164,\n",
       "  'sim_best_knn': 0.2507339,\n",
       "  'pred_soft': 1336,\n",
       "  'soft_val': 0.25025075674057007,\n",
       "  'sim_soft': 0.2507339,\n",
       "  'pred_max_max': 1336,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.23264461755752563,\n",
       "  'sim_max_max': 0.2507339},\n",
       " 30790: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6158,\n",
       "  'pred_knn': 6158,\n",
       "  'sim_real_cl': 0.454072,\n",
       "  'sim_best_knn': 0.454072,\n",
       "  'pred_soft': 1904,\n",
       "  'soft_val': 0.24923057854175568,\n",
       "  'sim_soft': 0.26695168},\n",
       " 30883: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6176,\n",
       "  'pred_knn': 4456,\n",
       "  'sim_real_cl': 0.28153035,\n",
       "  'sim_best_knn': 0.29279086,\n",
       "  'pred_soft': 9196,\n",
       "  'soft_val': 0.2280227094888687,\n",
       "  'sim_soft': 0.27050382,\n",
       "  'pred_max_max': 1247,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.24946177005767822,\n",
       "  'sim_max_max': 0.23559862},\n",
       " 30892: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6178,\n",
       "  'pred_knn': 6178,\n",
       "  'sim_real_cl': 0.31883967,\n",
       "  'sim_best_knn': 0.31883967,\n",
       "  'pred_soft': 6731,\n",
       "  'soft_val': 0.3023049235343933,\n",
       "  'sim_soft': 0.31407842},\n",
       " 30893: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6178,\n",
       "  'pred_knn': 6178,\n",
       "  'sim_real_cl': 0.52310395,\n",
       "  'sim_best_knn': 0.52310395,\n",
       "  'pred_soft': 5833,\n",
       "  'soft_val': 0.348662793636322,\n",
       "  'sim_soft': 0.38419205},\n",
       " 30912: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6182,\n",
       "  'pred_knn': 6182,\n",
       "  'sim_real_cl': 0.2836937,\n",
       "  'sim_best_knn': 0.2836937,\n",
       "  'pred_soft': 2626,\n",
       "  'soft_val': 0.2538406252861023,\n",
       "  'sim_soft': 0.23919678,\n",
       "  'pred_max_max': 8953,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2504074275493622,\n",
       "  'sim_max_max': 0.245823},\n",
       " 30926: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6185,\n",
       "  'pred_knn': 6185,\n",
       "  'sim_real_cl': 0.5319642,\n",
       "  'sim_best_knn': 0.5319642,\n",
       "  'pred_soft': 926,\n",
       "  'soft_val': 0.28467315435409546,\n",
       "  'sim_soft': 0.3549154},\n",
       " 30981: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6196,\n",
       "  'pred_knn': 9805,\n",
       "  'sim_real_cl': 0.33439615,\n",
       "  'sim_best_knn': 0.35442695,\n",
       "  'pred_soft': 9805,\n",
       "  'soft_val': 0.29723334312438965,\n",
       "  'sim_soft': 0.35442695,\n",
       "  'pred_max_max': 6196,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.33556365966796875,\n",
       "  'sim_max_max': 0.35442695},\n",
       " 30982: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6196,\n",
       "  'pred_knn': 3653,\n",
       "  'sim_real_cl': 0.14822152,\n",
       "  'sim_best_knn': 0.3847239,\n",
       "  'pred_soft': 9497,\n",
       "  'soft_val': 0.300944060087204,\n",
       "  'sim_soft': 0.34074968,\n",
       "  'pred_max_max': 5368,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.29695773124694824,\n",
       "  'sim_max_max': 0.2820776},\n",
       " 31026: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6205,\n",
       "  'pred_knn': 6205,\n",
       "  'sim_real_cl': 0.35777617,\n",
       "  'sim_best_knn': 0.35777617,\n",
       "  'pred_soft': 6864,\n",
       "  'soft_val': 0.27011507749557495,\n",
       "  'sim_soft': 0.32398093,\n",
       "  'pred_max_max': 2881,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2874802350997925,\n",
       "  'sim_max_max': 0.3457296},\n",
       " 31027: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6205,\n",
       "  'pred_knn': 4957,\n",
       "  'sim_real_cl': 0.28751752,\n",
       "  'sim_best_knn': 0.31891152,\n",
       "  'pred_soft': 4957,\n",
       "  'soft_val': 0.2402288317680359,\n",
       "  'sim_soft': 0.31891152,\n",
       "  'pred_max_max': 489,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28565722703933716,\n",
       "  'sim_max_max': 0.3014718},\n",
       " 31073: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6214,\n",
       "  'pred_knn': 2061,\n",
       "  'sim_real_cl': 0.37287575,\n",
       "  'sim_best_knn': 0.37721902,\n",
       "  'pred_soft': 2061,\n",
       "  'soft_val': 0.34034764766693115,\n",
       "  'sim_soft': 0.37721902,\n",
       "  'pred_max_max': 6214,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.4044515788555145,\n",
       "  'sim_max_max': 0.37721902},\n",
       " 31097: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6219,\n",
       "  'pred_knn': 6219,\n",
       "  'sim_real_cl': 0.35894674,\n",
       "  'sim_best_knn': 0.35894674,\n",
       "  'pred_soft': 5820,\n",
       "  'soft_val': 0.2711320221424103,\n",
       "  'sim_soft': 0.26164815,\n",
       "  'pred_max_max': 8032,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27995654940605164,\n",
       "  'sim_max_max': 0.356511},\n",
       " 31099: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6219,\n",
       "  'pred_knn': 6219,\n",
       "  'sim_real_cl': 0.31730834,\n",
       "  'sim_best_knn': 0.31730834,\n",
       "  'pred_soft': 1895,\n",
       "  'soft_val': 0.2304055243730545,\n",
       "  'sim_soft': 0.22637478,\n",
       "  'pred_max_max': 9167,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25584179162979126,\n",
       "  'sim_max_max': 0.22142196},\n",
       " 31128: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6225,\n",
       "  'pred_knn': 3566,\n",
       "  'sim_real_cl': 0.2653129,\n",
       "  'sim_best_knn': 0.36495537,\n",
       "  'pred_soft': 2998,\n",
       "  'soft_val': 0.28428253531455994,\n",
       "  'sim_soft': 0.2948779,\n",
       "  'pred_max_max': 3566,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3069903552532196,\n",
       "  'sim_max_max': 0.36495537},\n",
       " 31164: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6232,\n",
       "  'pred_knn': 2540,\n",
       "  'sim_real_cl': 0.27256507,\n",
       "  'sim_best_knn': 0.34674978,\n",
       "  'pred_soft': 7513,\n",
       "  'soft_val': 0.26293087005615234,\n",
       "  'sim_soft': 0.24404661,\n",
       "  'pred_max_max': 1120,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.25309324264526367,\n",
       "  'sim_max_max': 0.29986578},\n",
       " 31184: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6236,\n",
       "  'pred_knn': 79,\n",
       "  'sim_real_cl': 0.29495293,\n",
       "  'sim_best_knn': 0.35512722,\n",
       "  'pred_soft': 79,\n",
       "  'soft_val': 0.28255605697631836,\n",
       "  'sim_soft': 0.35512722,\n",
       "  'pred_max_max': 484,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.26350143551826477,\n",
       "  'sim_max_max': 0.27676105},\n",
       " 31216: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6243,\n",
       "  'pred_knn': 6243,\n",
       "  'sim_real_cl': 0.3507497,\n",
       "  'sim_best_knn': 0.3507497,\n",
       "  'pred_soft': 3922,\n",
       "  'soft_val': 0.27639657258987427,\n",
       "  'sim_soft': 0.31188297},\n",
       " 31227: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6245,\n",
       "  'pred_knn': 6245,\n",
       "  'sim_real_cl': 0.38424915,\n",
       "  'sim_best_knn': 0.38424915,\n",
       "  'pred_soft': 5820,\n",
       "  'soft_val': 0.2856449782848358,\n",
       "  'sim_soft': 0.27499664,\n",
       "  'pred_max_max': 996,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.27053964138031006,\n",
       "  'sim_max_max': 0.29691312},\n",
       " 31244: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6248,\n",
       "  'pred_knn': 6248,\n",
       "  'sim_real_cl': 0.43138143,\n",
       "  'sim_best_knn': 0.43138143,\n",
       "  'pred_soft': 8904,\n",
       "  'soft_val': 0.3153194487094879,\n",
       "  'sim_soft': 0.4039907,\n",
       "  'pred_max_max': 7364,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.31212934851646423,\n",
       "  'sim_max_max': 0.37923795},\n",
       " 31281: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6256,\n",
       "  'pred_knn': 6256,\n",
       "  'sim_real_cl': 0.32312903,\n",
       "  'sim_best_knn': 0.32312903,\n",
       "  'pred_soft': 8898,\n",
       "  'soft_val': 0.2551068663597107,\n",
       "  'sim_soft': 0.2467089},\n",
       " 31331: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6266,\n",
       "  'pred_knn': 6266,\n",
       "  'sim_real_cl': 0.524423,\n",
       "  'sim_best_knn': 0.524423,\n",
       "  'pred_soft': 940,\n",
       "  'soft_val': 0.2831076979637146,\n",
       "  'sim_soft': 0.30612183},\n",
       " 31341: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6268,\n",
       "  'pred_knn': 6268,\n",
       "  'sim_real_cl': 0.43110347,\n",
       "  'sim_best_knn': 0.43110347,\n",
       "  'pred_soft': 5477,\n",
       "  'soft_val': 0.2864810526371002,\n",
       "  'sim_soft': 0.3310845,\n",
       "  'pred_max_max': 8007,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.29024824500083923,\n",
       "  'sim_max_max': 0.35079646},\n",
       " 31367: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6273,\n",
       "  'pred_knn': 1730,\n",
       "  'sim_real_cl': 0.22454819,\n",
       "  'sim_best_knn': 0.30288357,\n",
       "  'pred_soft': 2829,\n",
       "  'soft_val': 0.2825224697589874,\n",
       "  'sim_soft': 0.2928507,\n",
       "  'pred_max_max': 2829,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2960096597671509,\n",
       "  'sim_max_max': 0.2928507},\n",
       " 31378: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6275,\n",
       "  'pred_knn': 7949,\n",
       "  'sim_real_cl': 0.35415113,\n",
       "  'sim_best_knn': 0.38096228,\n",
       "  'pred_soft': 6275,\n",
       "  'soft_val': 0.27007612586021423,\n",
       "  'sim_soft': array(0.27007613, dtype=float32),\n",
       "  'pred_max_max': 1765,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3346787393093109,\n",
       "  'sim_max_max': 0.32542503},\n",
       " 31391: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6278,\n",
       "  'pred_knn': 963,\n",
       "  'sim_real_cl': 0.31656379,\n",
       "  'sim_best_knn': 0.3448186,\n",
       "  'pred_soft': 7545,\n",
       "  'soft_val': 0.2857573330402374,\n",
       "  'sim_soft': 0.270868,\n",
       "  'pred_max_max': 6278,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3016630709171295,\n",
       "  'sim_max_max': 0.3448186},\n",
       " 31392: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6278,\n",
       "  'pred_knn': 9403,\n",
       "  'sim_real_cl': 0.31986874,\n",
       "  'sim_best_knn': 0.37272662,\n",
       "  'pred_soft': 9403,\n",
       "  'soft_val': 0.2907448709011078,\n",
       "  'sim_soft': 0.37272662,\n",
       "  'pred_max_max': 9403,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.3178483843803406,\n",
       "  'sim_max_max': 0.37272662},\n",
       " 31394: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6278,\n",
       "  'pred_knn': 6278,\n",
       "  'sim_real_cl': 0.36301565,\n",
       "  'sim_best_knn': 0.36301565,\n",
       "  'pred_soft': 9763,\n",
       "  'soft_val': 0.2564791738986969,\n",
       "  'sim_soft': 0.24435031},\n",
       " 31399: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6279,\n",
       "  'pred_knn': 6279,\n",
       "  'sim_real_cl': 0.41688818,\n",
       "  'sim_best_knn': 0.41688818,\n",
       "  'pred_soft': 6555,\n",
       "  'soft_val': 0.2674732804298401,\n",
       "  'sim_soft': 0.31065625},\n",
       " 31415: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6283,\n",
       "  'pred_knn': 6283,\n",
       "  'sim_real_cl': 0.46767843,\n",
       "  'sim_best_knn': 0.46767843,\n",
       "  'pred_soft': 1498,\n",
       "  'soft_val': 0.27905404567718506,\n",
       "  'sim_soft': 0.34004772},\n",
       " 31428: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6285,\n",
       "  'pred_knn': 506,\n",
       "  'sim_real_cl': 0.3216679,\n",
       "  'sim_best_knn': 0.44559938,\n",
       "  'pred_soft': 2997,\n",
       "  'soft_val': 0.28738218545913696,\n",
       "  'sim_soft': 0.28405195,\n",
       "  'pred_max_max': 506,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.36970171332359314,\n",
       "  'sim_max_max': 0.44559938},\n",
       " 31512: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6302,\n",
       "  'pred_knn': 9921,\n",
       "  'sim_real_cl': 0.23968731,\n",
       "  'sim_best_knn': 0.34823883,\n",
       "  'pred_soft': 5734,\n",
       "  'soft_val': 0.27218905091285706,\n",
       "  'sim_soft': 0.2502097,\n",
       "  'pred_max_max': 5734,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.2730577886104584,\n",
       "  'sim_max_max': 0.2502097},\n",
       " 31529: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6305,\n",
       "  'pred_knn': 4654,\n",
       "  'sim_real_cl': 0.38956168,\n",
       "  'sim_best_knn': 0.4134968,\n",
       "  'pred_soft': 4654,\n",
       "  'soft_val': 0.25483667850494385,\n",
       "  'sim_soft': 0.4134968,\n",
       "  'pred_max_max': 6305,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3163450062274933,\n",
       "  'sim_max_max': 0.4134968},\n",
       " 31548: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6309,\n",
       "  'pred_knn': 6309,\n",
       "  'sim_real_cl': 0.3929658,\n",
       "  'sim_best_knn': 0.3929658,\n",
       "  'pred_soft': 8026,\n",
       "  'soft_val': 0.26571136713027954,\n",
       "  'sim_soft': 0.29003465},\n",
       " 31622: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6324,\n",
       "  'pred_knn': 6324,\n",
       "  'sim_real_cl': 0.3467214,\n",
       "  'sim_best_knn': 0.3467214,\n",
       "  'pred_soft': 5456,\n",
       "  'soft_val': 0.2477533221244812,\n",
       "  'sim_soft': 0.26826775},\n",
       " 31642: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6328,\n",
       "  'pred_knn': 6328,\n",
       "  'sim_real_cl': 0.39973179,\n",
       "  'sim_best_knn': 0.39973179,\n",
       "  'pred_soft': 5541,\n",
       "  'soft_val': 0.3017323911190033,\n",
       "  'sim_soft': 0.29774046},\n",
       " 31644: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6328,\n",
       "  'pred_knn': 6328,\n",
       "  'sim_real_cl': 0.37113106,\n",
       "  'sim_best_knn': 0.37113106,\n",
       "  'pred_soft': 8214,\n",
       "  'soft_val': 0.2882789373397827,\n",
       "  'sim_soft': 0.23981315},\n",
       " 31655: {'bad_soft': True,\n",
       "  'bad_knn': False,\n",
       "  'real_class': 6331,\n",
       "  'pred_knn': 6331,\n",
       "  'sim_real_cl': 0.38532588,\n",
       "  'sim_best_knn': 0.38532588,\n",
       "  'pred_soft': 8087,\n",
       "  'soft_val': 0.277159184217453,\n",
       "  'sim_soft': 0.26050803},\n",
       " 31694: {'bad_soft': False,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6338,\n",
       "  'pred_knn': 8227,\n",
       "  'sim_real_cl': 0.2235266,\n",
       "  'sim_best_knn': 0.30827615,\n",
       "  'pred_soft': 6338,\n",
       "  'soft_val': 0.2606671452522278,\n",
       "  'sim_soft': array(0.26066715, dtype=float32),\n",
       "  'pred_max_max': 6338,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.2811586558818817,\n",
       "  'sim_max_max': 0.30827615},\n",
       " 31731: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6346,\n",
       "  'pred_knn': 8892,\n",
       "  'sim_real_cl': 0.3759057,\n",
       "  'sim_best_knn': 0.40088138,\n",
       "  'pred_soft': 5297,\n",
       "  'soft_val': 0.2593744099140167,\n",
       "  'sim_soft': 0.3586624,\n",
       "  'pred_max_max': 1593,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.28124862909317017,\n",
       "  'sim_max_max': 0.31966043},\n",
       " 31758: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6351,\n",
       "  'pred_knn': 1357,\n",
       "  'sim_real_cl': 0.37708634,\n",
       "  'sim_best_knn': 0.41577244,\n",
       "  'pred_soft': 9229,\n",
       "  'soft_val': 0.32105594873428345,\n",
       "  'sim_soft': 0.3973869,\n",
       "  'pred_max_max': 9229,\n",
       "  'bad_max_max': True,\n",
       "  'max_max_val': 0.34548574686050415,\n",
       "  'sim_max_max': 0.3973869},\n",
       " 31867: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6373,\n",
       "  'pred_knn': 2868,\n",
       "  'sim_real_cl': 0.37391815,\n",
       "  'sim_best_knn': 0.4038099,\n",
       "  'pred_soft': 2868,\n",
       "  'soft_val': 0.36389783024787903,\n",
       "  'sim_soft': 0.4038099,\n",
       "  'pred_max_max': 6373,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.35485517978668213,\n",
       "  'sim_max_max': 0.4038099},\n",
       " 31869: {'bad_soft': True,\n",
       "  'bad_knn': True,\n",
       "  'real_class': 6373,\n",
       "  'pred_knn': 2868,\n",
       "  'sim_real_cl': 0.37768105,\n",
       "  'sim_best_knn': 0.3851684,\n",
       "  'pred_soft': 2868,\n",
       "  'soft_val': 0.3440539538860321,\n",
       "  'sim_soft': 0.3851684,\n",
       "  'pred_max_max': 6373,\n",
       "  'bad_max_max': False,\n",
       "  'max_max_val': 0.3746485412120819,\n",
       "  'sim_max_max': 0.3851684},\n",
       " ...}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods = [\"full_random\", \"half_random\", \"kmeans\", \"sample_kmeans\"]\n",
    "keys = ['idx', 'bad_knn', 'real_class', 'pred_knn', 'sim_real_cl', 'sim_best_knn', 'pred_soft', 'bad_soft', 'soft_val', 'sim_soft', 'pred_max_max', 'bad_max_max', 'max_max_val', 'sim_max_max']\n",
    "result_dic = dict()\n",
    "for k in keys:\n",
    "    result_dic[k] = []\n",
    "\n",
    "for d in dic:\n",
    "    for k in keys:\n",
    "        if k == 'idx':\n",
    "            result_dic[k] += [d]\n",
    "        elif k in dic[d]:\n",
    "            result_dic[k] += [dic[d][k]]\n",
    "        else:\n",
    "            result_dic[k] += ['NaN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': [46,\n",
       "  47,\n",
       "  48,\n",
       "  77,\n",
       "  88,\n",
       "  96,\n",
       "  98,\n",
       "  116,\n",
       "  220,\n",
       "  228,\n",
       "  229,\n",
       "  235,\n",
       "  237,\n",
       "  246,\n",
       "  256,\n",
       "  317,\n",
       "  349,\n",
       "  411,\n",
       "  434,\n",
       "  444,\n",
       "  500,\n",
       "  563,\n",
       "  642,\n",
       "  648,\n",
       "  691,\n",
       "  715,\n",
       "  728,\n",
       "  740,\n",
       "  745,\n",
       "  768,\n",
       "  771,\n",
       "  829,\n",
       "  864,\n",
       "  892,\n",
       "  899,\n",
       "  934,\n",
       "  957,\n",
       "  1029,\n",
       "  1138,\n",
       "  1149,\n",
       "  1154,\n",
       "  1160,\n",
       "  1169,\n",
       "  1183,\n",
       "  1190,\n",
       "  1218,\n",
       "  1273,\n",
       "  1298,\n",
       "  1299,\n",
       "  1359,\n",
       "  1361,\n",
       "  1391,\n",
       "  1486,\n",
       "  1489,\n",
       "  1578,\n",
       "  1621,\n",
       "  1676,\n",
       "  1712,\n",
       "  1779,\n",
       "  1800,\n",
       "  1853,\n",
       "  1900,\n",
       "  1901,\n",
       "  1929,\n",
       "  1956,\n",
       "  1959,\n",
       "  1993,\n",
       "  1999,\n",
       "  2010,\n",
       "  2011,\n",
       "  2014,\n",
       "  2077,\n",
       "  2084,\n",
       "  2099,\n",
       "  2104,\n",
       "  2117,\n",
       "  2125,\n",
       "  2141,\n",
       "  2142,\n",
       "  2143,\n",
       "  2144,\n",
       "  2153,\n",
       "  2159,\n",
       "  2174,\n",
       "  2198,\n",
       "  2199,\n",
       "  2205,\n",
       "  2227,\n",
       "  2322,\n",
       "  2402,\n",
       "  2412,\n",
       "  2413,\n",
       "  2414,\n",
       "  2437,\n",
       "  2438,\n",
       "  2439,\n",
       "  2482,\n",
       "  2623,\n",
       "  2627,\n",
       "  2637,\n",
       "  2691,\n",
       "  2772,\n",
       "  2782,\n",
       "  2794,\n",
       "  2818,\n",
       "  2839,\n",
       "  2884,\n",
       "  2902,\n",
       "  2971,\n",
       "  2972,\n",
       "  2974,\n",
       "  3011,\n",
       "  3031,\n",
       "  3050,\n",
       "  3074,\n",
       "  3129,\n",
       "  3133,\n",
       "  3136,\n",
       "  3164,\n",
       "  3167,\n",
       "  3171,\n",
       "  3178,\n",
       "  3187,\n",
       "  3197,\n",
       "  3199,\n",
       "  3217,\n",
       "  3254,\n",
       "  3276,\n",
       "  3311,\n",
       "  3317,\n",
       "  3387,\n",
       "  3423,\n",
       "  3435,\n",
       "  3446,\n",
       "  3448,\n",
       "  3451,\n",
       "  3453,\n",
       "  3454,\n",
       "  3474,\n",
       "  3544,\n",
       "  3554,\n",
       "  3565,\n",
       "  3568,\n",
       "  3569,\n",
       "  3716,\n",
       "  3720,\n",
       "  3769,\n",
       "  3852,\n",
       "  3876,\n",
       "  3989,\n",
       "  4018,\n",
       "  4029,\n",
       "  4097,\n",
       "  4119,\n",
       "  4128,\n",
       "  4143,\n",
       "  4148,\n",
       "  4155,\n",
       "  4162,\n",
       "  4163,\n",
       "  4164,\n",
       "  4223,\n",
       "  4272,\n",
       "  4285,\n",
       "  4292,\n",
       "  4317,\n",
       "  4332,\n",
       "  4334,\n",
       "  4394,\n",
       "  4433,\n",
       "  4488,\n",
       "  4523,\n",
       "  4535,\n",
       "  4549,\n",
       "  4572,\n",
       "  4640,\n",
       "  4642,\n",
       "  4699,\n",
       "  4746,\n",
       "  4769,\n",
       "  4822,\n",
       "  4834,\n",
       "  4897,\n",
       "  4937,\n",
       "  5004,\n",
       "  5009,\n",
       "  5030,\n",
       "  5031,\n",
       "  5032,\n",
       "  5034,\n",
       "  5052,\n",
       "  5098,\n",
       "  5127,\n",
       "  5149,\n",
       "  5156,\n",
       "  5165,\n",
       "  5168,\n",
       "  5198,\n",
       "  5246,\n",
       "  5393,\n",
       "  5446,\n",
       "  5449,\n",
       "  5479,\n",
       "  5494,\n",
       "  5557,\n",
       "  5574,\n",
       "  5604,\n",
       "  5617,\n",
       "  5619,\n",
       "  5625,\n",
       "  5757,\n",
       "  5763,\n",
       "  5788,\n",
       "  5792,\n",
       "  5848,\n",
       "  5864,\n",
       "  5893,\n",
       "  5904,\n",
       "  5927,\n",
       "  5939,\n",
       "  5962,\n",
       "  5966,\n",
       "  6015,\n",
       "  6024,\n",
       "  6052,\n",
       "  6082,\n",
       "  6094,\n",
       "  6173,\n",
       "  6192,\n",
       "  6207,\n",
       "  6241,\n",
       "  6253,\n",
       "  6258,\n",
       "  6372,\n",
       "  6373,\n",
       "  6382,\n",
       "  6384,\n",
       "  6394,\n",
       "  6464,\n",
       "  6478,\n",
       "  6486,\n",
       "  6603,\n",
       "  6689,\n",
       "  6715,\n",
       "  6717,\n",
       "  6725,\n",
       "  6727,\n",
       "  6729,\n",
       "  6749,\n",
       "  6773,\n",
       "  6777,\n",
       "  6779,\n",
       "  6797,\n",
       "  6804,\n",
       "  6819,\n",
       "  6852,\n",
       "  6906,\n",
       "  6926,\n",
       "  6953,\n",
       "  6971,\n",
       "  6993,\n",
       "  6994,\n",
       "  6998,\n",
       "  7009,\n",
       "  7039,\n",
       "  7040,\n",
       "  7056,\n",
       "  7075,\n",
       "  7079,\n",
       "  7080,\n",
       "  7093,\n",
       "  7094,\n",
       "  7104,\n",
       "  7109,\n",
       "  7163,\n",
       "  7188,\n",
       "  7248,\n",
       "  7260,\n",
       "  7367,\n",
       "  7368,\n",
       "  7397,\n",
       "  7409,\n",
       "  7417,\n",
       "  7522,\n",
       "  7532,\n",
       "  7547,\n",
       "  7551,\n",
       "  7558,\n",
       "  7578,\n",
       "  7583,\n",
       "  7611,\n",
       "  7660,\n",
       "  7661,\n",
       "  7664,\n",
       "  7679,\n",
       "  7770,\n",
       "  7849,\n",
       "  7912,\n",
       "  7914,\n",
       "  7948,\n",
       "  7977,\n",
       "  7986,\n",
       "  7987,\n",
       "  7989,\n",
       "  8072,\n",
       "  8073,\n",
       "  8206,\n",
       "  8251,\n",
       "  8278,\n",
       "  8299,\n",
       "  8308,\n",
       "  8311,\n",
       "  8313,\n",
       "  8314,\n",
       "  8365,\n",
       "  8367,\n",
       "  8369,\n",
       "  8380,\n",
       "  8406,\n",
       "  8531,\n",
       "  8587,\n",
       "  8604,\n",
       "  8607,\n",
       "  8628,\n",
       "  8651,\n",
       "  8654,\n",
       "  8784,\n",
       "  8803,\n",
       "  8844,\n",
       "  8872,\n",
       "  8909,\n",
       "  8933,\n",
       "  8974,\n",
       "  9042,\n",
       "  9051,\n",
       "  9058,\n",
       "  9149,\n",
       "  9220,\n",
       "  9320,\n",
       "  9353,\n",
       "  9358,\n",
       "  9383,\n",
       "  9436,\n",
       "  9479,\n",
       "  9535,\n",
       "  9569,\n",
       "  9577,\n",
       "  9593,\n",
       "  9660,\n",
       "  9705,\n",
       "  9717,\n",
       "  9807,\n",
       "  9879,\n",
       "  9962,\n",
       "  9971,\n",
       "  9972,\n",
       "  9995,\n",
       "  10136,\n",
       "  10162,\n",
       "  10235,\n",
       "  10342,\n",
       "  10350,\n",
       "  10370,\n",
       "  10371,\n",
       "  10374,\n",
       "  10519,\n",
       "  10527,\n",
       "  10649,\n",
       "  10693,\n",
       "  10834,\n",
       "  10880,\n",
       "  10934,\n",
       "  10943,\n",
       "  10996,\n",
       "  11011,\n",
       "  11027,\n",
       "  11086,\n",
       "  11135,\n",
       "  11137,\n",
       "  11186,\n",
       "  11204,\n",
       "  11258,\n",
       "  11292,\n",
       "  11361,\n",
       "  11379,\n",
       "  11386,\n",
       "  11423,\n",
       "  11467,\n",
       "  11532,\n",
       "  11572,\n",
       "  11575,\n",
       "  11579,\n",
       "  11593,\n",
       "  11602,\n",
       "  11624,\n",
       "  11639,\n",
       "  11656,\n",
       "  11715,\n",
       "  11737,\n",
       "  11764,\n",
       "  11790,\n",
       "  11874,\n",
       "  11878,\n",
       "  11987,\n",
       "  11994,\n",
       "  12014,\n",
       "  12154,\n",
       "  12155,\n",
       "  12215,\n",
       "  12222,\n",
       "  12430,\n",
       "  12536,\n",
       "  12587,\n",
       "  12646,\n",
       "  12701,\n",
       "  12736,\n",
       "  12750,\n",
       "  12789,\n",
       "  12825,\n",
       "  12828,\n",
       "  12849,\n",
       "  12860,\n",
       "  12885,\n",
       "  12913,\n",
       "  12927,\n",
       "  12928,\n",
       "  13011,\n",
       "  13037,\n",
       "  13044,\n",
       "  13118,\n",
       "  13133,\n",
       "  13149,\n",
       "  13151,\n",
       "  13199,\n",
       "  13214,\n",
       "  13268,\n",
       "  13386,\n",
       "  13472,\n",
       "  13482,\n",
       "  13546,\n",
       "  13594,\n",
       "  13640,\n",
       "  13644,\n",
       "  13744,\n",
       "  13794,\n",
       "  13868,\n",
       "  13892,\n",
       "  13904,\n",
       "  13927,\n",
       "  13944,\n",
       "  13974,\n",
       "  14031,\n",
       "  14072,\n",
       "  14097,\n",
       "  14121,\n",
       "  14157,\n",
       "  14264,\n",
       "  14370,\n",
       "  14393,\n",
       "  14476,\n",
       "  14565,\n",
       "  14569,\n",
       "  14583,\n",
       "  14693,\n",
       "  14705,\n",
       "  14768,\n",
       "  14791,\n",
       "  14811,\n",
       "  14856,\n",
       "  14864,\n",
       "  14888,\n",
       "  14981,\n",
       "  15001,\n",
       "  15062,\n",
       "  15120,\n",
       "  15121,\n",
       "  15124,\n",
       "  15132,\n",
       "  15142,\n",
       "  15157,\n",
       "  15178,\n",
       "  15180,\n",
       "  15182,\n",
       "  15183,\n",
       "  15206,\n",
       "  15249,\n",
       "  15303,\n",
       "  15309,\n",
       "  15332,\n",
       "  15395,\n",
       "  15417,\n",
       "  15425,\n",
       "  15473,\n",
       "  15514,\n",
       "  15548,\n",
       "  15557,\n",
       "  15558,\n",
       "  15561,\n",
       "  15567,\n",
       "  15615,\n",
       "  15616,\n",
       "  15657,\n",
       "  15690,\n",
       "  15724,\n",
       "  15764,\n",
       "  15789,\n",
       "  15822,\n",
       "  15824,\n",
       "  15989,\n",
       "  16027,\n",
       "  16044,\n",
       "  16083,\n",
       "  16102,\n",
       "  16112,\n",
       "  16168,\n",
       "  16180,\n",
       "  16212,\n",
       "  16244,\n",
       "  16323,\n",
       "  16324,\n",
       "  16338,\n",
       "  16434,\n",
       "  16452,\n",
       "  16517,\n",
       "  16520,\n",
       "  16553,\n",
       "  16578,\n",
       "  16623,\n",
       "  16635,\n",
       "  16638,\n",
       "  16655,\n",
       "  16661,\n",
       "  16664,\n",
       "  16701,\n",
       "  16702,\n",
       "  16709,\n",
       "  16757,\n",
       "  16772,\n",
       "  16891,\n",
       "  16892,\n",
       "  16894,\n",
       "  16924,\n",
       "  16933,\n",
       "  16934,\n",
       "  17040,\n",
       "  17046,\n",
       "  17052,\n",
       "  17136,\n",
       "  17139,\n",
       "  17165,\n",
       "  17188,\n",
       "  17198,\n",
       "  17224,\n",
       "  17248,\n",
       "  17267,\n",
       "  17274,\n",
       "  17342,\n",
       "  17347,\n",
       "  17425,\n",
       "  17429,\n",
       "  17467,\n",
       "  17468,\n",
       "  17470,\n",
       "  17481,\n",
       "  17541,\n",
       "  17587,\n",
       "  17589,\n",
       "  17619,\n",
       "  17631,\n",
       "  17677,\n",
       "  17703,\n",
       "  17778,\n",
       "  17789,\n",
       "  17877,\n",
       "  17927,\n",
       "  17944,\n",
       "  17959,\n",
       "  18044,\n",
       "  18062,\n",
       "  18064,\n",
       "  18101,\n",
       "  18102,\n",
       "  18152,\n",
       "  18157,\n",
       "  18168,\n",
       "  18198,\n",
       "  18266,\n",
       "  18297,\n",
       "  18301,\n",
       "  18313,\n",
       "  18356,\n",
       "  18455,\n",
       "  18474,\n",
       "  18495,\n",
       "  18502,\n",
       "  18526,\n",
       "  18528,\n",
       "  18530,\n",
       "  18534,\n",
       "  18539,\n",
       "  18649,\n",
       "  18758,\n",
       "  18775,\n",
       "  18776,\n",
       "  18788,\n",
       "  18826,\n",
       "  18827,\n",
       "  18887,\n",
       "  18914,\n",
       "  18926,\n",
       "  18932,\n",
       "  18972,\n",
       "  19012,\n",
       "  19013,\n",
       "  19076,\n",
       "  19102,\n",
       "  19118,\n",
       "  19144,\n",
       "  19154,\n",
       "  19228,\n",
       "  19341,\n",
       "  19352,\n",
       "  19373,\n",
       "  19400,\n",
       "  19563,\n",
       "  19723,\n",
       "  19821,\n",
       "  19823,\n",
       "  19842,\n",
       "  19878,\n",
       "  19884,\n",
       "  19888,\n",
       "  19896,\n",
       "  19918,\n",
       "  19943,\n",
       "  20045,\n",
       "  20078,\n",
       "  20090,\n",
       "  20185,\n",
       "  20189,\n",
       "  20227,\n",
       "  20239,\n",
       "  20289,\n",
       "  20302,\n",
       "  20316,\n",
       "  20340,\n",
       "  20367,\n",
       "  20394,\n",
       "  20421,\n",
       "  20463,\n",
       "  20467,\n",
       "  20524,\n",
       "  20543,\n",
       "  20563,\n",
       "  20585,\n",
       "  20591,\n",
       "  20592,\n",
       "  20599,\n",
       "  20614,\n",
       "  20679,\n",
       "  20692,\n",
       "  20693,\n",
       "  20698,\n",
       "  20699,\n",
       "  20700,\n",
       "  20704,\n",
       "  20733,\n",
       "  20778,\n",
       "  20804,\n",
       "  20808,\n",
       "  20814,\n",
       "  20899,\n",
       "  20922,\n",
       "  20924,\n",
       "  20937,\n",
       "  20951,\n",
       "  20956,\n",
       "  20974,\n",
       "  21028,\n",
       "  21037,\n",
       "  21067,\n",
       "  21138,\n",
       "  21158,\n",
       "  21243,\n",
       "  21316,\n",
       "  21317,\n",
       "  21369,\n",
       "  21389,\n",
       "  21420,\n",
       "  21422,\n",
       "  21424,\n",
       "  21459,\n",
       "  21478,\n",
       "  21481,\n",
       "  21535,\n",
       "  21536,\n",
       "  21537,\n",
       "  21646,\n",
       "  21654,\n",
       "  21722,\n",
       "  21724,\n",
       "  21753,\n",
       "  21883,\n",
       "  21934,\n",
       "  21970,\n",
       "  21973,\n",
       "  21974,\n",
       "  21996,\n",
       "  21997,\n",
       "  22048,\n",
       "  22174,\n",
       "  22211,\n",
       "  22286,\n",
       "  22288,\n",
       "  22294,\n",
       "  22299,\n",
       "  22328,\n",
       "  22337,\n",
       "  22347,\n",
       "  22354,\n",
       "  22446,\n",
       "  22489,\n",
       "  22506,\n",
       "  22513,\n",
       "  22516,\n",
       "  22527,\n",
       "  22539,\n",
       "  22611,\n",
       "  22647,\n",
       "  22678,\n",
       "  22679,\n",
       "  22688,\n",
       "  22734,\n",
       "  22777,\n",
       "  22864,\n",
       "  22883,\n",
       "  22884,\n",
       "  23102,\n",
       "  23105,\n",
       "  23108,\n",
       "  23127,\n",
       "  23128,\n",
       "  23147,\n",
       "  23159,\n",
       "  23292,\n",
       "  23320,\n",
       "  23328,\n",
       "  23391,\n",
       "  23419,\n",
       "  23465,\n",
       "  23467,\n",
       "  23469,\n",
       "  23497,\n",
       "  23506,\n",
       "  23554,\n",
       "  23679,\n",
       "  23741,\n",
       "  23764,\n",
       "  23769,\n",
       "  23852,\n",
       "  23855,\n",
       "  23913,\n",
       "  23923,\n",
       "  23949,\n",
       "  23958,\n",
       "  23982,\n",
       "  24090,\n",
       "  24162,\n",
       "  24198,\n",
       "  24230,\n",
       "  24263,\n",
       "  24280,\n",
       "  24298,\n",
       "  24307,\n",
       "  24308,\n",
       "  24357,\n",
       "  24376,\n",
       "  24456,\n",
       "  24485,\n",
       "  24547,\n",
       "  24549,\n",
       "  24595,\n",
       "  24622,\n",
       "  24628,\n",
       "  24674,\n",
       "  24769,\n",
       "  24866,\n",
       "  24901,\n",
       "  24909,\n",
       "  24941,\n",
       "  25067,\n",
       "  25077,\n",
       "  25094,\n",
       "  25129,\n",
       "  25137,\n",
       "  25139,\n",
       "  25244,\n",
       "  25278,\n",
       "  25284,\n",
       "  25302,\n",
       "  25338,\n",
       "  25449,\n",
       "  25456,\n",
       "  25566,\n",
       "  25581,\n",
       "  25609,\n",
       "  25637,\n",
       "  25657,\n",
       "  25659,\n",
       "  25674,\n",
       "  25720,\n",
       "  25721,\n",
       "  25786,\n",
       "  25815,\n",
       "  25818,\n",
       "  25827,\n",
       "  25828,\n",
       "  25890,\n",
       "  25919,\n",
       "  25985,\n",
       "  25987,\n",
       "  26036,\n",
       "  26038,\n",
       "  26045,\n",
       "  26058,\n",
       "  26059,\n",
       "  26081,\n",
       "  26084,\n",
       "  26176,\n",
       "  26178,\n",
       "  26253,\n",
       "  26254,\n",
       "  26259,\n",
       "  26294,\n",
       "  26296,\n",
       "  26383,\n",
       "  26394,\n",
       "  26449,\n",
       "  26502,\n",
       "  26504,\n",
       "  26552,\n",
       "  26659,\n",
       "  26669,\n",
       "  26811,\n",
       "  26841,\n",
       "  26902,\n",
       "  26903,\n",
       "  26904,\n",
       "  26981,\n",
       "  27064,\n",
       "  27067,\n",
       "  27070,\n",
       "  27171,\n",
       "  27192,\n",
       "  27194,\n",
       "  27221,\n",
       "  27224,\n",
       "  27228,\n",
       "  27268,\n",
       "  27269,\n",
       "  27301,\n",
       "  27338,\n",
       "  27354,\n",
       "  27374,\n",
       "  27435,\n",
       "  27459,\n",
       "  27490,\n",
       "  27537,\n",
       "  27566,\n",
       "  27578,\n",
       "  27688,\n",
       "  27722,\n",
       "  27793,\n",
       "  27836,\n",
       "  27837,\n",
       "  27982,\n",
       "  27988,\n",
       "  27989,\n",
       "  28020,\n",
       "  28094,\n",
       "  28146,\n",
       "  28235,\n",
       "  28238,\n",
       "  28292,\n",
       "  28308,\n",
       "  28313,\n",
       "  28315,\n",
       "  28343,\n",
       "  28389,\n",
       "  28422,\n",
       "  28423,\n",
       "  28424,\n",
       "  28443,\n",
       "  28518,\n",
       "  28534,\n",
       "  28558,\n",
       "  28619,\n",
       "  28629,\n",
       "  28696,\n",
       "  28717,\n",
       "  28723,\n",
       "  28828,\n",
       "  28861,\n",
       "  28863,\n",
       "  28867,\n",
       "  28868,\n",
       "  28969,\n",
       "  29070,\n",
       "  29071,\n",
       "  29209,\n",
       "  29282,\n",
       "  29291,\n",
       "  29299,\n",
       "  29357,\n",
       "  29359,\n",
       "  29398,\n",
       "  29473,\n",
       "  29535,\n",
       "  29592,\n",
       "  29593,\n",
       "  29594,\n",
       "  29639,\n",
       "  29657,\n",
       "  29787,\n",
       "  29832,\n",
       "  29848,\n",
       "  29849,\n",
       "  29909,\n",
       "  29932,\n",
       "  29940,\n",
       "  29997,\n",
       "  30020,\n",
       "  30022,\n",
       "  30023,\n",
       "  30071,\n",
       "  30086,\n",
       "  30122,\n",
       "  30123,\n",
       "  30211,\n",
       "  30299,\n",
       "  30372,\n",
       "  30400,\n",
       "  30419,\n",
       "  30448,\n",
       "  30483,\n",
       "  30488,\n",
       "  30491,\n",
       "  30518,\n",
       "  30572,\n",
       "  30573,\n",
       "  30574,\n",
       "  30645,\n",
       "  30647,\n",
       "  30655,\n",
       "  30736,\n",
       "  30737,\n",
       "  30754,\n",
       "  30777,\n",
       "  30790,\n",
       "  30883,\n",
       "  30892,\n",
       "  30893,\n",
       "  30912,\n",
       "  30926,\n",
       "  30981,\n",
       "  30982,\n",
       "  31026,\n",
       "  31027,\n",
       "  31073,\n",
       "  31097,\n",
       "  31099,\n",
       "  31128,\n",
       "  31164,\n",
       "  31184,\n",
       "  31216,\n",
       "  31227,\n",
       "  31244,\n",
       "  31281,\n",
       "  31331,\n",
       "  31341,\n",
       "  31367,\n",
       "  31378,\n",
       "  31391,\n",
       "  31392,\n",
       "  31394,\n",
       "  31399,\n",
       "  31415,\n",
       "  31428,\n",
       "  31512,\n",
       "  31529,\n",
       "  31548,\n",
       "  31622,\n",
       "  31642,\n",
       "  31644,\n",
       "  31655,\n",
       "  31694,\n",
       "  31731,\n",
       "  31758,\n",
       "  31867,\n",
       "  31869,\n",
       "  ...],\n",
       " 'bad_knn': [False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  ...],\n",
       " 'real_class': [9,\n",
       "  9,\n",
       "  9,\n",
       "  15,\n",
       "  17,\n",
       "  19,\n",
       "  19,\n",
       "  23,\n",
       "  44,\n",
       "  45,\n",
       "  45,\n",
       "  47,\n",
       "  47,\n",
       "  49,\n",
       "  51,\n",
       "  63,\n",
       "  69,\n",
       "  82,\n",
       "  86,\n",
       "  88,\n",
       "  100,\n",
       "  112,\n",
       "  128,\n",
       "  129,\n",
       "  138,\n",
       "  143,\n",
       "  145,\n",
       "  148,\n",
       "  149,\n",
       "  153,\n",
       "  154,\n",
       "  165,\n",
       "  172,\n",
       "  178,\n",
       "  179,\n",
       "  186,\n",
       "  191,\n",
       "  205,\n",
       "  227,\n",
       "  229,\n",
       "  230,\n",
       "  232,\n",
       "  233,\n",
       "  236,\n",
       "  238,\n",
       "  243,\n",
       "  254,\n",
       "  259,\n",
       "  259,\n",
       "  271,\n",
       "  272,\n",
       "  278,\n",
       "  297,\n",
       "  297,\n",
       "  315,\n",
       "  324,\n",
       "  335,\n",
       "  342,\n",
       "  355,\n",
       "  360,\n",
       "  370,\n",
       "  380,\n",
       "  380,\n",
       "  385,\n",
       "  391,\n",
       "  391,\n",
       "  398,\n",
       "  399,\n",
       "  402,\n",
       "  402,\n",
       "  402,\n",
       "  415,\n",
       "  416,\n",
       "  419,\n",
       "  420,\n",
       "  423,\n",
       "  425,\n",
       "  428,\n",
       "  428,\n",
       "  428,\n",
       "  428,\n",
       "  430,\n",
       "  431,\n",
       "  434,\n",
       "  439,\n",
       "  439,\n",
       "  441,\n",
       "  445,\n",
       "  464,\n",
       "  480,\n",
       "  482,\n",
       "  482,\n",
       "  482,\n",
       "  487,\n",
       "  487,\n",
       "  487,\n",
       "  496,\n",
       "  524,\n",
       "  525,\n",
       "  527,\n",
       "  538,\n",
       "  554,\n",
       "  556,\n",
       "  558,\n",
       "  563,\n",
       "  567,\n",
       "  576,\n",
       "  580,\n",
       "  594,\n",
       "  594,\n",
       "  594,\n",
       "  602,\n",
       "  606,\n",
       "  610,\n",
       "  614,\n",
       "  625,\n",
       "  626,\n",
       "  627,\n",
       "  632,\n",
       "  633,\n",
       "  634,\n",
       "  635,\n",
       "  637,\n",
       "  639,\n",
       "  639,\n",
       "  643,\n",
       "  650,\n",
       "  655,\n",
       "  662,\n",
       "  663,\n",
       "  677,\n",
       "  684,\n",
       "  687,\n",
       "  689,\n",
       "  689,\n",
       "  690,\n",
       "  690,\n",
       "  690,\n",
       "  694,\n",
       "  708,\n",
       "  710,\n",
       "  713,\n",
       "  713,\n",
       "  713,\n",
       "  743,\n",
       "  744,\n",
       "  753,\n",
       "  770,\n",
       "  775,\n",
       "  797,\n",
       "  803,\n",
       "  805,\n",
       "  819,\n",
       "  823,\n",
       "  825,\n",
       "  828,\n",
       "  829,\n",
       "  831,\n",
       "  832,\n",
       "  832,\n",
       "  832,\n",
       "  844,\n",
       "  854,\n",
       "  857,\n",
       "  858,\n",
       "  863,\n",
       "  866,\n",
       "  866,\n",
       "  878,\n",
       "  886,\n",
       "  897,\n",
       "  904,\n",
       "  907,\n",
       "  909,\n",
       "  914,\n",
       "  928,\n",
       "  928,\n",
       "  939,\n",
       "  949,\n",
       "  953,\n",
       "  964,\n",
       "  966,\n",
       "  979,\n",
       "  987,\n",
       "  1000,\n",
       "  1001,\n",
       "  1006,\n",
       "  1006,\n",
       "  1006,\n",
       "  1006,\n",
       "  1010,\n",
       "  1019,\n",
       "  1025,\n",
       "  1029,\n",
       "  1031,\n",
       "  1033,\n",
       "  1033,\n",
       "  1039,\n",
       "  1049,\n",
       "  1078,\n",
       "  1089,\n",
       "  1089,\n",
       "  1095,\n",
       "  1098,\n",
       "  1111,\n",
       "  1114,\n",
       "  1120,\n",
       "  1123,\n",
       "  1123,\n",
       "  1125,\n",
       "  1151,\n",
       "  1152,\n",
       "  1157,\n",
       "  1158,\n",
       "  1169,\n",
       "  1172,\n",
       "  1178,\n",
       "  1180,\n",
       "  1185,\n",
       "  1187,\n",
       "  1192,\n",
       "  1193,\n",
       "  1203,\n",
       "  1204,\n",
       "  1210,\n",
       "  1216,\n",
       "  1218,\n",
       "  1234,\n",
       "  1238,\n",
       "  1241,\n",
       "  1248,\n",
       "  1250,\n",
       "  1251,\n",
       "  1274,\n",
       "  1274,\n",
       "  1276,\n",
       "  1276,\n",
       "  1278,\n",
       "  1292,\n",
       "  1295,\n",
       "  1297,\n",
       "  1320,\n",
       "  1337,\n",
       "  1343,\n",
       "  1343,\n",
       "  1345,\n",
       "  1345,\n",
       "  1345,\n",
       "  1349,\n",
       "  1354,\n",
       "  1355,\n",
       "  1355,\n",
       "  1359,\n",
       "  1360,\n",
       "  1363,\n",
       "  1370,\n",
       "  1381,\n",
       "  1385,\n",
       "  1390,\n",
       "  1394,\n",
       "  1398,\n",
       "  1398,\n",
       "  1399,\n",
       "  1401,\n",
       "  1407,\n",
       "  1408,\n",
       "  1411,\n",
       "  1415,\n",
       "  1415,\n",
       "  1416,\n",
       "  1418,\n",
       "  1418,\n",
       "  1420,\n",
       "  1421,\n",
       "  1432,\n",
       "  1437,\n",
       "  1449,\n",
       "  1452,\n",
       "  1473,\n",
       "  1473,\n",
       "  1479,\n",
       "  1481,\n",
       "  1483,\n",
       "  1504,\n",
       "  1506,\n",
       "  1509,\n",
       "  1510,\n",
       "  1511,\n",
       "  1515,\n",
       "  1516,\n",
       "  1522,\n",
       "  1532,\n",
       "  1532,\n",
       "  1532,\n",
       "  1535,\n",
       "  1554,\n",
       "  1569,\n",
       "  1582,\n",
       "  1582,\n",
       "  1589,\n",
       "  1595,\n",
       "  1597,\n",
       "  1597,\n",
       "  1597,\n",
       "  1614,\n",
       "  1614,\n",
       "  1641,\n",
       "  1650,\n",
       "  1655,\n",
       "  1659,\n",
       "  1661,\n",
       "  1662,\n",
       "  1662,\n",
       "  1662,\n",
       "  1673,\n",
       "  1673,\n",
       "  1673,\n",
       "  1676,\n",
       "  1681,\n",
       "  1706,\n",
       "  1717,\n",
       "  1720,\n",
       "  1721,\n",
       "  1725,\n",
       "  1730,\n",
       "  1730,\n",
       "  1756,\n",
       "  1760,\n",
       "  1768,\n",
       "  1774,\n",
       "  1781,\n",
       "  1786,\n",
       "  1794,\n",
       "  1808,\n",
       "  1810,\n",
       "  1811,\n",
       "  1829,\n",
       "  1844,\n",
       "  1864,\n",
       "  1870,\n",
       "  1871,\n",
       "  1876,\n",
       "  1887,\n",
       "  1895,\n",
       "  1907,\n",
       "  1913,\n",
       "  1915,\n",
       "  1918,\n",
       "  1932,\n",
       "  1941,\n",
       "  1943,\n",
       "  1961,\n",
       "  1975,\n",
       "  1992,\n",
       "  1994,\n",
       "  1994,\n",
       "  1999,\n",
       "  2027,\n",
       "  2032,\n",
       "  2047,\n",
       "  2068,\n",
       "  2070,\n",
       "  2074,\n",
       "  2074,\n",
       "  2074,\n",
       "  2103,\n",
       "  2105,\n",
       "  2129,\n",
       "  2138,\n",
       "  2166,\n",
       "  2176,\n",
       "  2186,\n",
       "  2188,\n",
       "  2199,\n",
       "  2202,\n",
       "  2205,\n",
       "  2217,\n",
       "  2227,\n",
       "  2227,\n",
       "  2237,\n",
       "  2240,\n",
       "  2251,\n",
       "  2258,\n",
       "  2272,\n",
       "  2275,\n",
       "  2277,\n",
       "  2284,\n",
       "  2293,\n",
       "  2306,\n",
       "  2314,\n",
       "  2315,\n",
       "  2315,\n",
       "  2318,\n",
       "  2320,\n",
       "  2324,\n",
       "  2327,\n",
       "  2331,\n",
       "  2343,\n",
       "  2347,\n",
       "  2352,\n",
       "  2358,\n",
       "  2374,\n",
       "  2375,\n",
       "  2397,\n",
       "  2398,\n",
       "  2402,\n",
       "  2430,\n",
       "  2431,\n",
       "  2443,\n",
       "  2444,\n",
       "  2486,\n",
       "  2507,\n",
       "  2517,\n",
       "  2529,\n",
       "  2540,\n",
       "  2547,\n",
       "  2550,\n",
       "  2557,\n",
       "  2565,\n",
       "  2565,\n",
       "  2569,\n",
       "  2572,\n",
       "  2577,\n",
       "  2582,\n",
       "  2585,\n",
       "  2585,\n",
       "  2602,\n",
       "  2607,\n",
       "  2608,\n",
       "  2623,\n",
       "  2626,\n",
       "  2629,\n",
       "  2630,\n",
       "  2639,\n",
       "  2642,\n",
       "  2653,\n",
       "  2677,\n",
       "  2694,\n",
       "  2696,\n",
       "  2709,\n",
       "  2718,\n",
       "  2728,\n",
       "  2728,\n",
       "  2748,\n",
       "  2758,\n",
       "  2773,\n",
       "  2778,\n",
       "  2780,\n",
       "  2785,\n",
       "  2788,\n",
       "  2794,\n",
       "  2806,\n",
       "  2814,\n",
       "  2819,\n",
       "  2824,\n",
       "  2831,\n",
       "  2852,\n",
       "  2874,\n",
       "  2878,\n",
       "  2895,\n",
       "  2913,\n",
       "  2913,\n",
       "  2916,\n",
       "  2938,\n",
       "  2941,\n",
       "  2953,\n",
       "  2958,\n",
       "  2962,\n",
       "  2971,\n",
       "  2972,\n",
       "  2977,\n",
       "  2996,\n",
       "  3000,\n",
       "  3012,\n",
       "  3024,\n",
       "  3024,\n",
       "  3024,\n",
       "  3026,\n",
       "  3028,\n",
       "  3031,\n",
       "  3035,\n",
       "  3036,\n",
       "  3036,\n",
       "  3036,\n",
       "  3041,\n",
       "  3049,\n",
       "  3060,\n",
       "  3061,\n",
       "  3066,\n",
       "  3079,\n",
       "  3083,\n",
       "  3085,\n",
       "  3094,\n",
       "  3102,\n",
       "  3109,\n",
       "  3111,\n",
       "  3111,\n",
       "  3112,\n",
       "  3113,\n",
       "  3123,\n",
       "  3123,\n",
       "  3131,\n",
       "  3138,\n",
       "  3144,\n",
       "  3152,\n",
       "  3157,\n",
       "  3164,\n",
       "  3164,\n",
       "  3197,\n",
       "  3205,\n",
       "  3208,\n",
       "  3216,\n",
       "  3220,\n",
       "  3222,\n",
       "  3233,\n",
       "  3236,\n",
       "  3242,\n",
       "  3248,\n",
       "  3264,\n",
       "  3264,\n",
       "  3267,\n",
       "  3286,\n",
       "  3290,\n",
       "  3303,\n",
       "  3304,\n",
       "  3310,\n",
       "  3315,\n",
       "  3324,\n",
       "  3327,\n",
       "  3327,\n",
       "  3331,\n",
       "  3332,\n",
       "  3332,\n",
       "  3340,\n",
       "  3340,\n",
       "  3341,\n",
       "  3351,\n",
       "  3354,\n",
       "  3378,\n",
       "  3378,\n",
       "  3378,\n",
       "  3384,\n",
       "  3386,\n",
       "  3386,\n",
       "  3408,\n",
       "  3409,\n",
       "  3410,\n",
       "  3427,\n",
       "  3427,\n",
       "  3433,\n",
       "  3437,\n",
       "  3439,\n",
       "  3444,\n",
       "  3449,\n",
       "  3453,\n",
       "  3454,\n",
       "  3468,\n",
       "  3469,\n",
       "  3485,\n",
       "  3485,\n",
       "  3493,\n",
       "  3493,\n",
       "  3494,\n",
       "  3496,\n",
       "  3508,\n",
       "  3517,\n",
       "  3517,\n",
       "  3523,\n",
       "  3526,\n",
       "  3535,\n",
       "  3540,\n",
       "  3555,\n",
       "  3557,\n",
       "  3575,\n",
       "  3585,\n",
       "  3588,\n",
       "  3591,\n",
       "  3608,\n",
       "  3612,\n",
       "  3612,\n",
       "  3620,\n",
       "  3620,\n",
       "  3630,\n",
       "  3631,\n",
       "  3633,\n",
       "  3639,\n",
       "  3653,\n",
       "  3659,\n",
       "  3660,\n",
       "  3662,\n",
       "  3671,\n",
       "  3691,\n",
       "  3694,\n",
       "  3699,\n",
       "  3700,\n",
       "  3705,\n",
       "  3705,\n",
       "  3706,\n",
       "  3706,\n",
       "  3707,\n",
       "  3729,\n",
       "  3751,\n",
       "  3755,\n",
       "  3755,\n",
       "  3757,\n",
       "  3765,\n",
       "  3765,\n",
       "  3777,\n",
       "  3782,\n",
       "  3785,\n",
       "  3786,\n",
       "  3794,\n",
       "  3802,\n",
       "  3802,\n",
       "  3815,\n",
       "  3820,\n",
       "  3823,\n",
       "  3828,\n",
       "  3830,\n",
       "  3845,\n",
       "  3868,\n",
       "  3870,\n",
       "  3874,\n",
       "  3880,\n",
       "  3912,\n",
       "  3944,\n",
       "  3964,\n",
       "  3964,\n",
       "  3968,\n",
       "  3975,\n",
       "  3976,\n",
       "  3977,\n",
       "  3979,\n",
       "  3983,\n",
       "  3988,\n",
       "  4009,\n",
       "  4015,\n",
       "  4018,\n",
       "  4037,\n",
       "  4037,\n",
       "  4045,\n",
       "  4047,\n",
       "  4057,\n",
       "  4060,\n",
       "  4063,\n",
       "  4068,\n",
       "  4073,\n",
       "  4078,\n",
       "  4084,\n",
       "  4092,\n",
       "  4093,\n",
       "  4104,\n",
       "  4108,\n",
       "  4112,\n",
       "  4117,\n",
       "  4118,\n",
       "  4118,\n",
       "  4119,\n",
       "  4122,\n",
       "  4135,\n",
       "  4138,\n",
       "  4138,\n",
       "  4139,\n",
       "  4139,\n",
       "  4140,\n",
       "  4140,\n",
       "  4146,\n",
       "  4155,\n",
       "  4160,\n",
       "  4161,\n",
       "  4162,\n",
       "  4179,\n",
       "  4184,\n",
       "  4184,\n",
       "  4187,\n",
       "  4190,\n",
       "  4191,\n",
       "  4194,\n",
       "  4205,\n",
       "  4207,\n",
       "  4213,\n",
       "  4227,\n",
       "  4231,\n",
       "  4248,\n",
       "  4263,\n",
       "  4263,\n",
       "  4273,\n",
       "  4277,\n",
       "  4284,\n",
       "  4284,\n",
       "  4284,\n",
       "  4291,\n",
       "  4295,\n",
       "  4296,\n",
       "  4307,\n",
       "  4307,\n",
       "  4307,\n",
       "  4329,\n",
       "  4330,\n",
       "  4344,\n",
       "  4344,\n",
       "  4350,\n",
       "  4376,\n",
       "  4386,\n",
       "  4394,\n",
       "  4394,\n",
       "  4394,\n",
       "  4399,\n",
       "  4399,\n",
       "  4409,\n",
       "  4434,\n",
       "  4442,\n",
       "  4457,\n",
       "  4457,\n",
       "  4458,\n",
       "  4459,\n",
       "  4465,\n",
       "  4467,\n",
       "  4469,\n",
       "  4470,\n",
       "  4489,\n",
       "  4497,\n",
       "  4501,\n",
       "  4502,\n",
       "  4503,\n",
       "  4505,\n",
       "  4507,\n",
       "  4522,\n",
       "  4529,\n",
       "  4535,\n",
       "  4535,\n",
       "  4537,\n",
       "  4546,\n",
       "  4555,\n",
       "  4572,\n",
       "  4576,\n",
       "  4576,\n",
       "  4620,\n",
       "  4621,\n",
       "  4621,\n",
       "  4625,\n",
       "  4625,\n",
       "  4629,\n",
       "  4631,\n",
       "  4658,\n",
       "  4664,\n",
       "  4665,\n",
       "  4678,\n",
       "  4683,\n",
       "  4693,\n",
       "  4693,\n",
       "  4693,\n",
       "  4699,\n",
       "  4701,\n",
       "  4710,\n",
       "  4735,\n",
       "  4748,\n",
       "  4752,\n",
       "  4753,\n",
       "  4770,\n",
       "  4771,\n",
       "  4782,\n",
       "  4784,\n",
       "  4789,\n",
       "  4791,\n",
       "  4796,\n",
       "  4818,\n",
       "  4832,\n",
       "  4839,\n",
       "  4846,\n",
       "  4852,\n",
       "  4856,\n",
       "  4859,\n",
       "  4861,\n",
       "  4861,\n",
       "  4871,\n",
       "  4875,\n",
       "  4891,\n",
       "  4897,\n",
       "  4909,\n",
       "  4909,\n",
       "  4919,\n",
       "  4924,\n",
       "  4925,\n",
       "  4934,\n",
       "  4953,\n",
       "  4973,\n",
       "  4980,\n",
       "  4981,\n",
       "  4988,\n",
       "  5013,\n",
       "  5015,\n",
       "  5018,\n",
       "  5025,\n",
       "  5027,\n",
       "  5027,\n",
       "  5048,\n",
       "  5055,\n",
       "  5056,\n",
       "  5060,\n",
       "  5067,\n",
       "  5089,\n",
       "  5091,\n",
       "  5113,\n",
       "  5116,\n",
       "  5121,\n",
       "  5127,\n",
       "  5131,\n",
       "  5131,\n",
       "  5134,\n",
       "  5144,\n",
       "  5144,\n",
       "  5157,\n",
       "  5163,\n",
       "  5163,\n",
       "  5165,\n",
       "  5165,\n",
       "  5178,\n",
       "  5183,\n",
       "  5197,\n",
       "  5197,\n",
       "  5207,\n",
       "  5207,\n",
       "  5209,\n",
       "  5211,\n",
       "  5211,\n",
       "  5216,\n",
       "  5216,\n",
       "  5235,\n",
       "  5235,\n",
       "  5250,\n",
       "  5250,\n",
       "  5251,\n",
       "  5258,\n",
       "  5259,\n",
       "  5276,\n",
       "  5278,\n",
       "  5289,\n",
       "  5300,\n",
       "  5300,\n",
       "  5310,\n",
       "  5331,\n",
       "  5333,\n",
       "  5362,\n",
       "  5368,\n",
       "  5380,\n",
       "  5380,\n",
       "  5380,\n",
       "  5396,\n",
       "  5412,\n",
       "  5413,\n",
       "  5414,\n",
       "  5434,\n",
       "  5438,\n",
       "  5438,\n",
       "  5444,\n",
       "  5444,\n",
       "  5445,\n",
       "  5453,\n",
       "  5453,\n",
       "  5460,\n",
       "  5467,\n",
       "  5470,\n",
       "  5474,\n",
       "  5487,\n",
       "  5491,\n",
       "  5498,\n",
       "  5507,\n",
       "  5513,\n",
       "  5515,\n",
       "  5537,\n",
       "  5544,\n",
       "  5558,\n",
       "  5567,\n",
       "  5567,\n",
       "  5596,\n",
       "  5597,\n",
       "  5597,\n",
       "  5604,\n",
       "  5618,\n",
       "  5629,\n",
       "  5647,\n",
       "  5647,\n",
       "  5658,\n",
       "  5661,\n",
       "  5662,\n",
       "  5663,\n",
       "  5668,\n",
       "  5677,\n",
       "  5684,\n",
       "  5684,\n",
       "  5684,\n",
       "  5688,\n",
       "  5703,\n",
       "  5706,\n",
       "  5711,\n",
       "  5723,\n",
       "  5725,\n",
       "  5739,\n",
       "  5743,\n",
       "  5744,\n",
       "  5765,\n",
       "  5772,\n",
       "  5772,\n",
       "  5773,\n",
       "  5773,\n",
       "  5793,\n",
       "  5814,\n",
       "  5814,\n",
       "  5841,\n",
       "  5856,\n",
       "  5858,\n",
       "  5859,\n",
       "  5871,\n",
       "  5871,\n",
       "  5879,\n",
       "  5894,\n",
       "  5907,\n",
       "  5918,\n",
       "  5918,\n",
       "  5918,\n",
       "  5927,\n",
       "  5931,\n",
       "  5957,\n",
       "  5966,\n",
       "  5969,\n",
       "  5969,\n",
       "  5981,\n",
       "  5986,\n",
       "  5988,\n",
       "  5999,\n",
       "  6004,\n",
       "  6004,\n",
       "  6004,\n",
       "  6014,\n",
       "  6017,\n",
       "  6024,\n",
       "  6024,\n",
       "  6042,\n",
       "  6059,\n",
       "  6074,\n",
       "  6080,\n",
       "  6083,\n",
       "  6089,\n",
       "  6096,\n",
       "  6097,\n",
       "  6098,\n",
       "  6103,\n",
       "  6114,\n",
       "  6114,\n",
       "  6114,\n",
       "  6129,\n",
       "  6129,\n",
       "  6131,\n",
       "  6147,\n",
       "  6147,\n",
       "  6150,\n",
       "  6155,\n",
       "  6158,\n",
       "  6176,\n",
       "  6178,\n",
       "  6178,\n",
       "  6182,\n",
       "  6185,\n",
       "  6196,\n",
       "  6196,\n",
       "  6205,\n",
       "  6205,\n",
       "  6214,\n",
       "  6219,\n",
       "  6219,\n",
       "  6225,\n",
       "  6232,\n",
       "  6236,\n",
       "  6243,\n",
       "  6245,\n",
       "  6248,\n",
       "  6256,\n",
       "  6266,\n",
       "  6268,\n",
       "  6273,\n",
       "  6275,\n",
       "  6278,\n",
       "  6278,\n",
       "  6278,\n",
       "  6279,\n",
       "  6283,\n",
       "  6285,\n",
       "  6302,\n",
       "  6305,\n",
       "  6309,\n",
       "  6324,\n",
       "  6328,\n",
       "  6328,\n",
       "  6331,\n",
       "  6338,\n",
       "  6346,\n",
       "  6351,\n",
       "  6373,\n",
       "  6373,\n",
       "  ...],\n",
       " 'pred_knn': [9,\n",
       "  1422,\n",
       "  9,\n",
       "  15,\n",
       "  4226,\n",
       "  19,\n",
       "  3576,\n",
       "  2471,\n",
       "  7272,\n",
       "  1981,\n",
       "  4368,\n",
       "  47,\n",
       "  47,\n",
       "  49,\n",
       "  51,\n",
       "  63,\n",
       "  5790,\n",
       "  2691,\n",
       "  5632,\n",
       "  5705,\n",
       "  9252,\n",
       "  8561,\n",
       "  3024,\n",
       "  2725,\n",
       "  4671,\n",
       "  7630,\n",
       "  1184,\n",
       "  4400,\n",
       "  3685,\n",
       "  153,\n",
       "  154,\n",
       "  5359,\n",
       "  172,\n",
       "  178,\n",
       "  754,\n",
       "  186,\n",
       "  191,\n",
       "  9739,\n",
       "  227,\n",
       "  1172,\n",
       "  230,\n",
       "  4980,\n",
       "  2924,\n",
       "  236,\n",
       "  238,\n",
       "  5159,\n",
       "  8595,\n",
       "  1534,\n",
       "  2593,\n",
       "  271,\n",
       "  2543,\n",
       "  2531,\n",
       "  1310,\n",
       "  297,\n",
       "  4060,\n",
       "  324,\n",
       "  335,\n",
       "  9796,\n",
       "  355,\n",
       "  7455,\n",
       "  370,\n",
       "  380,\n",
       "  2134,\n",
       "  1250,\n",
       "  391,\n",
       "  391,\n",
       "  2154,\n",
       "  3805,\n",
       "  402,\n",
       "  402,\n",
       "  402,\n",
       "  1820,\n",
       "  416,\n",
       "  4537,\n",
       "  690,\n",
       "  6984,\n",
       "  3274,\n",
       "  428,\n",
       "  428,\n",
       "  428,\n",
       "  428,\n",
       "  4506,\n",
       "  431,\n",
       "  434,\n",
       "  439,\n",
       "  2044,\n",
       "  2091,\n",
       "  7880,\n",
       "  7949,\n",
       "  480,\n",
       "  482,\n",
       "  482,\n",
       "  1172,\n",
       "  487,\n",
       "  2015,\n",
       "  5981,\n",
       "  2484,\n",
       "  524,\n",
       "  4638,\n",
       "  527,\n",
       "  1933,\n",
       "  829,\n",
       "  8856,\n",
       "  558,\n",
       "  563,\n",
       "  567,\n",
       "  256,\n",
       "  6943,\n",
       "  594,\n",
       "  594,\n",
       "  594,\n",
       "  602,\n",
       "  3493,\n",
       "  8523,\n",
       "  2028,\n",
       "  8001,\n",
       "  626,\n",
       "  627,\n",
       "  1483,\n",
       "  633,\n",
       "  1134,\n",
       "  635,\n",
       "  2059,\n",
       "  639,\n",
       "  639,\n",
       "  8093,\n",
       "  650,\n",
       "  6309,\n",
       "  662,\n",
       "  2532,\n",
       "  8273,\n",
       "  684,\n",
       "  9006,\n",
       "  6050,\n",
       "  689,\n",
       "  6043,\n",
       "  9528,\n",
       "  690,\n",
       "  694,\n",
       "  2589,\n",
       "  5447,\n",
       "  713,\n",
       "  713,\n",
       "  5631,\n",
       "  7983,\n",
       "  639,\n",
       "  753,\n",
       "  770,\n",
       "  3661,\n",
       "  6374,\n",
       "  3445,\n",
       "  805,\n",
       "  5047,\n",
       "  823,\n",
       "  825,\n",
       "  3845,\n",
       "  829,\n",
       "  7368,\n",
       "  5249,\n",
       "  591,\n",
       "  1277,\n",
       "  9669,\n",
       "  6068,\n",
       "  2070,\n",
       "  7198,\n",
       "  863,\n",
       "  866,\n",
       "  2075,\n",
       "  5547,\n",
       "  7460,\n",
       "  5934,\n",
       "  1432,\n",
       "  1308,\n",
       "  639,\n",
       "  3261,\n",
       "  890,\n",
       "  928,\n",
       "  939,\n",
       "  3653,\n",
       "  1134,\n",
       "  964,\n",
       "  9794,\n",
       "  979,\n",
       "  987,\n",
       "  6528,\n",
       "  1001,\n",
       "  1006,\n",
       "  1006,\n",
       "  1006,\n",
       "  1006,\n",
       "  1010,\n",
       "  8001,\n",
       "  1025,\n",
       "  1029,\n",
       "  3868,\n",
       "  5593,\n",
       "  6716,\n",
       "  4084,\n",
       "  8654,\n",
       "  1078,\n",
       "  1089,\n",
       "  1089,\n",
       "  2368,\n",
       "  5982,\n",
       "  1111,\n",
       "  1114,\n",
       "  1120,\n",
       "  1123,\n",
       "  1123,\n",
       "  2381,\n",
       "  1151,\n",
       "  1152,\n",
       "  639,\n",
       "  6933,\n",
       "  1169,\n",
       "  1172,\n",
       "  3354,\n",
       "  1180,\n",
       "  8587,\n",
       "  4525,\n",
       "  8943,\n",
       "  2252,\n",
       "  7544,\n",
       "  8170,\n",
       "  4117,\n",
       "  4728,\n",
       "  7602,\n",
       "  3995,\n",
       "  1238,\n",
       "  1241,\n",
       "  1248,\n",
       "  2091,\n",
       "  2018,\n",
       "  1632,\n",
       "  7533,\n",
       "  1276,\n",
       "  8588,\n",
       "  1278,\n",
       "  7222,\n",
       "  1295,\n",
       "  1297,\n",
       "  2279,\n",
       "  690,\n",
       "  1343,\n",
       "  1343,\n",
       "  974,\n",
       "  639,\n",
       "  1345,\n",
       "  1349,\n",
       "  690,\n",
       "  8541,\n",
       "  1355,\n",
       "  7540,\n",
       "  395,\n",
       "  9794,\n",
       "  1370,\n",
       "  8185,\n",
       "  2333,\n",
       "  6022,\n",
       "  1394,\n",
       "  1398,\n",
       "  7114,\n",
       "  3625,\n",
       "  5817,\n",
       "  8561,\n",
       "  6488,\n",
       "  1411,\n",
       "  1415,\n",
       "  4976,\n",
       "  1416,\n",
       "  1418,\n",
       "  1418,\n",
       "  1420,\n",
       "  5982,\n",
       "  1432,\n",
       "  1437,\n",
       "  1449,\n",
       "  1452,\n",
       "  6501,\n",
       "  2974,\n",
       "  6479,\n",
       "  1481,\n",
       "  1483,\n",
       "  5695,\n",
       "  1506,\n",
       "  1509,\n",
       "  1510,\n",
       "  1511,\n",
       "  713,\n",
       "  8306,\n",
       "  1522,\n",
       "  1532,\n",
       "  2856,\n",
       "  7899,\n",
       "  2540,\n",
       "  9248,\n",
       "  2333,\n",
       "  1582,\n",
       "  1582,\n",
       "  2507,\n",
       "  7036,\n",
       "  1597,\n",
       "  1597,\n",
       "  1597,\n",
       "  1614,\n",
       "  9035,\n",
       "  1641,\n",
       "  1650,\n",
       "  9731,\n",
       "  1659,\n",
       "  1234,\n",
       "  1662,\n",
       "  1662,\n",
       "  1573,\n",
       "  1673,\n",
       "  1673,\n",
       "  1673,\n",
       "  1676,\n",
       "  5026,\n",
       "  690,\n",
       "  5620,\n",
       "  1534,\n",
       "  3497,\n",
       "  4007,\n",
       "  8583,\n",
       "  1730,\n",
       "  3485,\n",
       "  1760,\n",
       "  7492,\n",
       "  1774,\n",
       "  156,\n",
       "  1786,\n",
       "  2593,\n",
       "  1808,\n",
       "  1810,\n",
       "  5536,\n",
       "  1829,\n",
       "  2396,\n",
       "  7528,\n",
       "  2827,\n",
       "  1871,\n",
       "  1876,\n",
       "  3283,\n",
       "  7061,\n",
       "  1907,\n",
       "  4622,\n",
       "  1915,\n",
       "  7685,\n",
       "  1932,\n",
       "  1291,\n",
       "  1864,\n",
       "  434,\n",
       "  3024,\n",
       "  3929,\n",
       "  7345,\n",
       "  7345,\n",
       "  1999,\n",
       "  2655,\n",
       "  2032,\n",
       "  4456,\n",
       "  4952,\n",
       "  2738,\n",
       "  2074,\n",
       "  2074,\n",
       "  2074,\n",
       "  2103,\n",
       "  4379,\n",
       "  690,\n",
       "  9006,\n",
       "  2166,\n",
       "  675,\n",
       "  2186,\n",
       "  2188,\n",
       "  690,\n",
       "  7485,\n",
       "  2205,\n",
       "  2479,\n",
       "  3653,\n",
       "  736,\n",
       "  2237,\n",
       "  690,\n",
       "  1647,\n",
       "  2698,\n",
       "  6340,\n",
       "  9495,\n",
       "  5836,\n",
       "  7009,\n",
       "  6116,\n",
       "  6049,\n",
       "  2314,\n",
       "  4589,\n",
       "  3916,\n",
       "  2318,\n",
       "  4530,\n",
       "  984,\n",
       "  312,\n",
       "  6895,\n",
       "  3790,\n",
       "  1379,\n",
       "  2352,\n",
       "  3796,\n",
       "  2374,\n",
       "  2625,\n",
       "  8239,\n",
       "  2398,\n",
       "  4473,\n",
       "  3880,\n",
       "  8938,\n",
       "  699,\n",
       "  3797,\n",
       "  2028,\n",
       "  5631,\n",
       "  2517,\n",
       "  2856,\n",
       "  2540,\n",
       "  8319,\n",
       "  5484,\n",
       "  2604,\n",
       "  9794,\n",
       "  9794,\n",
       "  4779,\n",
       "  2275,\n",
       "  2577,\n",
       "  2582,\n",
       "  3777,\n",
       "  9356,\n",
       "  7809,\n",
       "  2607,\n",
       "  2556,\n",
       "  3943,\n",
       "  3242,\n",
       "  4528,\n",
       "  8548,\n",
       "  6421,\n",
       "  4038,\n",
       "  4456,\n",
       "  1747,\n",
       "  2694,\n",
       "  1861,\n",
       "  2709,\n",
       "  1677,\n",
       "  399,\n",
       "  7441,\n",
       "  1491,\n",
       "  1371,\n",
       "  2773,\n",
       "  2778,\n",
       "  3931,\n",
       "  2785,\n",
       "  2788,\n",
       "  7703,\n",
       "  2806,\n",
       "  4401,\n",
       "  2819,\n",
       "  4483,\n",
       "  4775,\n",
       "  4532,\n",
       "  3713,\n",
       "  6584,\n",
       "  6420,\n",
       "  690,\n",
       "  9949,\n",
       "  4557,\n",
       "  7283,\n",
       "  786,\n",
       "  3197,\n",
       "  2958,\n",
       "  6229,\n",
       "  1452,\n",
       "  7273,\n",
       "  2114,\n",
       "  7222,\n",
       "  3000,\n",
       "  3012,\n",
       "  918,\n",
       "  918,\n",
       "  8493,\n",
       "  9574,\n",
       "  3028,\n",
       "  3031,\n",
       "  3035,\n",
       "  3036,\n",
       "  3036,\n",
       "  3036,\n",
       "  9969,\n",
       "  3049,\n",
       "  4489,\n",
       "  9955,\n",
       "  9139,\n",
       "  4422,\n",
       "  2983,\n",
       "  3085,\n",
       "  1277,\n",
       "  3102,\n",
       "  9794,\n",
       "  3111,\n",
       "  3039,\n",
       "  3613,\n",
       "  2593,\n",
       "  3123,\n",
       "  9136,\n",
       "  4007,\n",
       "  2680,\n",
       "  3144,\n",
       "  6466,\n",
       "  3157,\n",
       "  3164,\n",
       "  3164,\n",
       "  9794,\n",
       "  1141,\n",
       "  3208,\n",
       "  3216,\n",
       "  3220,\n",
       "  3222,\n",
       "  3233,\n",
       "  9200,\n",
       "  3242,\n",
       "  3248,\n",
       "  3264,\n",
       "  3264,\n",
       "  3267,\n",
       "  3286,\n",
       "  9360,\n",
       "  3303,\n",
       "  3304,\n",
       "  8735,\n",
       "  3315,\n",
       "  3324,\n",
       "  3327,\n",
       "  1250,\n",
       "  1835,\n",
       "  3332,\n",
       "  3332,\n",
       "  3340,\n",
       "  3340,\n",
       "  3341,\n",
       "  690,\n",
       "  3354,\n",
       "  9926,\n",
       "  4898,\n",
       "  5851,\n",
       "  3384,\n",
       "  7870,\n",
       "  2103,\n",
       "  4557,\n",
       "  3931,\n",
       "  3410,\n",
       "  3427,\n",
       "  3427,\n",
       "  3433,\n",
       "  3303,\n",
       "  186,\n",
       "  3444,\n",
       "  3449,\n",
       "  1445,\n",
       "  3454,\n",
       "  8561,\n",
       "  3469,\n",
       "  3048,\n",
       "  9835,\n",
       "  3493,\n",
       "  3493,\n",
       "  5408,\n",
       "  1981,\n",
       "  3508,\n",
       "  3517,\n",
       "  3517,\n",
       "  7193,\n",
       "  3526,\n",
       "  7787,\n",
       "  4743,\n",
       "  62,\n",
       "  3557,\n",
       "  3575,\n",
       "  9474,\n",
       "  3588,\n",
       "  3591,\n",
       "  3608,\n",
       "  690,\n",
       "  9837,\n",
       "  1933,\n",
       "  1933,\n",
       "  1963,\n",
       "  6683,\n",
       "  2231,\n",
       "  3835,\n",
       "  3653,\n",
       "  12,\n",
       "  1250,\n",
       "  4459,\n",
       "  8609,\n",
       "  3691,\n",
       "  3694,\n",
       "  3699,\n",
       "  5515,\n",
       "  3685,\n",
       "  3705,\n",
       "  3706,\n",
       "  3706,\n",
       "  3707,\n",
       "  9794,\n",
       "  3751,\n",
       "  7870,\n",
       "  2725,\n",
       "  3757,\n",
       "  2581,\n",
       "  5484,\n",
       "  6570,\n",
       "  3782,\n",
       "  3785,\n",
       "  5918,\n",
       "  742,\n",
       "  3802,\n",
       "  3802,\n",
       "  3815,\n",
       "  1201,\n",
       "  3823,\n",
       "  9574,\n",
       "  3294,\n",
       "  3845,\n",
       "  3868,\n",
       "  8263,\n",
       "  1743,\n",
       "  5480,\n",
       "  3912,\n",
       "  1270,\n",
       "  4057,\n",
       "  3964,\n",
       "  6205,\n",
       "  691,\n",
       "  9731,\n",
       "  639,\n",
       "  1014,\n",
       "  6723,\n",
       "  690,\n",
       "  4009,\n",
       "  894,\n",
       "  9537,\n",
       "  5342,\n",
       "  1016,\n",
       "  4045,\n",
       "  853,\n",
       "  4057,\n",
       "  4060,\n",
       "  4063,\n",
       "  6427,\n",
       "  4073,\n",
       "  5631,\n",
       "  4084,\n",
       "  4092,\n",
       "  7037,\n",
       "  4104,\n",
       "  8582,\n",
       "  4112,\n",
       "  4117,\n",
       "  4118,\n",
       "  4118,\n",
       "  3024,\n",
       "  2858,\n",
       "  4135,\n",
       "  4138,\n",
       "  7584,\n",
       "  4139,\n",
       "  4139,\n",
       "  2829,\n",
       "  5848,\n",
       "  4146,\n",
       "  8786,\n",
       "  4160,\n",
       "  4161,\n",
       "  4162,\n",
       "  4179,\n",
       "  4184,\n",
       "  3038,\n",
       "  6569,\n",
       "  4190,\n",
       "  4191,\n",
       "  4194,\n",
       "  4205,\n",
       "  4360,\n",
       "  7630,\n",
       "  183,\n",
       "  7455,\n",
       "  6127,\n",
       "  4263,\n",
       "  4263,\n",
       "  8414,\n",
       "  5525,\n",
       "  4284,\n",
       "  4284,\n",
       "  4284,\n",
       "  4291,\n",
       "  4295,\n",
       "  5092,\n",
       "  4307,\n",
       "  4307,\n",
       "  4307,\n",
       "  4329,\n",
       "  4330,\n",
       "  690,\n",
       "  4344,\n",
       "  9549,\n",
       "  1008,\n",
       "  9197,\n",
       "  4394,\n",
       "  9136,\n",
       "  2784,\n",
       "  2071,\n",
       "  8852,\n",
       "  2555,\n",
       "  3546,\n",
       "  4442,\n",
       "  9921,\n",
       "  3986,\n",
       "  4458,\n",
       "  4459,\n",
       "  4465,\n",
       "  4880,\n",
       "  4469,\n",
       "  7638,\n",
       "  1081,\n",
       "  962,\n",
       "  2381,\n",
       "  6470,\n",
       "  4503,\n",
       "  1836,\n",
       "  8358,\n",
       "  4522,\n",
       "  4529,\n",
       "  4535,\n",
       "  4535,\n",
       "  9583,\n",
       "  4546,\n",
       "  795,\n",
       "  4572,\n",
       "  4576,\n",
       "  4576,\n",
       "  690,\n",
       "  2410,\n",
       "  2410,\n",
       "  4625,\n",
       "  4625,\n",
       "  4621,\n",
       "  4141,\n",
       "  1300,\n",
       "  4664,\n",
       "  7485,\n",
       "  690,\n",
       "  4683,\n",
       "  4693,\n",
       "  4693,\n",
       "  4693,\n",
       "  9882,\n",
       "  696,\n",
       "  4710,\n",
       "  4735,\n",
       "  4748,\n",
       "  8620,\n",
       "  9110,\n",
       "  4770,\n",
       "  1593,\n",
       "  98,\n",
       "  2182,\n",
       "  5433,\n",
       "  4791,\n",
       "  6078,\n",
       "  6055,\n",
       "  7190,\n",
       "  4839,\n",
       "  5277,\n",
       "  4852,\n",
       "  1954,\n",
       "  4859,\n",
       "  4861,\n",
       "  4861,\n",
       "  2873,\n",
       "  4875,\n",
       "  6327,\n",
       "  7870,\n",
       "  4451,\n",
       "  8438,\n",
       "  4919,\n",
       "  1308,\n",
       "  4959,\n",
       "  2713,\n",
       "  5585,\n",
       "  6360,\n",
       "  4980,\n",
       "  8569,\n",
       "  1836,\n",
       "  6390,\n",
       "  5015,\n",
       "  1144,\n",
       "  5025,\n",
       "  9980,\n",
       "  9980,\n",
       "  639,\n",
       "  3038,\n",
       "  9658,\n",
       "  2381,\n",
       "  5067,\n",
       "  1922,\n",
       "  5772,\n",
       "  5113,\n",
       "  5116,\n",
       "  4103,\n",
       "  3995,\n",
       "  2834,\n",
       "  5117,\n",
       "  8821,\n",
       "  2860,\n",
       "  6819,\n",
       "  5157,\n",
       "  690,\n",
       "  806,\n",
       "  9102,\n",
       "  6655,\n",
       "  1944,\n",
       "  8583,\n",
       "  8959,\n",
       "  2091,\n",
       "  6083,\n",
       "  576,\n",
       "  5209,\n",
       "  4074,\n",
       "  2443,\n",
       "  1172,\n",
       "  9371,\n",
       "  8045,\n",
       "  1296,\n",
       "  4711,\n",
       "  2754,\n",
       "  3974,\n",
       "  9949,\n",
       "  5259,\n",
       "  690,\n",
       "  7687,\n",
       "  6275,\n",
       "  5300,\n",
       "  5300,\n",
       "  1930,\n",
       "  8664,\n",
       "  639,\n",
       "  1172,\n",
       "  3433,\n",
       "  4456,\n",
       "  2091,\n",
       "  8372,\n",
       "  1867,\n",
       "  9105,\n",
       "  1141,\n",
       "  2664,\n",
       "  1933,\n",
       "  6467,\n",
       "  8855,\n",
       "  5444,\n",
       "  2289,\n",
       "  6428,\n",
       "  5360,\n",
       "  5453,\n",
       "  2532,\n",
       "  5467,\n",
       "  690,\n",
       "  5474,\n",
       "  5309,\n",
       "  3855,\n",
       "  4589,\n",
       "  1172,\n",
       "  3683,\n",
       "  1172,\n",
       "  8572,\n",
       "  9794,\n",
       "  79,\n",
       "  1432,\n",
       "  2593,\n",
       "  2106,\n",
       "  5597,\n",
       "  4629,\n",
       "  8892,\n",
       "  7870,\n",
       "  4912,\n",
       "  3928,\n",
       "  5647,\n",
       "  4174,\n",
       "  4533,\n",
       "  6367,\n",
       "  77,\n",
       "  8103,\n",
       "  5677,\n",
       "  7493,\n",
       "  863,\n",
       "  639,\n",
       "  9487,\n",
       "  8811,\n",
       "  5706,\n",
       "  5711,\n",
       "  5723,\n",
       "  2532,\n",
       "  4525,\n",
       "  4104,\n",
       "  7528,\n",
       "  5765,\n",
       "  352,\n",
       "  2725,\n",
       "  5773,\n",
       "  5773,\n",
       "  3902,\n",
       "  2930,\n",
       "  4227,\n",
       "  2478,\n",
       "  639,\n",
       "  5284,\n",
       "  5859,\n",
       "  3433,\n",
       "  8457,\n",
       "  5879,\n",
       "  690,\n",
       "  5561,\n",
       "  5918,\n",
       "  3123,\n",
       "  6334,\n",
       "  4280,\n",
       "  5931,\n",
       "  3197,\n",
       "  3197,\n",
       "  4224,\n",
       "  3854,\n",
       "  7962,\n",
       "  8688,\n",
       "  6587,\n",
       "  1836,\n",
       "  6004,\n",
       "  6004,\n",
       "  6004,\n",
       "  6014,\n",
       "  7670,\n",
       "  6024,\n",
       "  6024,\n",
       "  6042,\n",
       "  6059,\n",
       "  7264,\n",
       "  2223,\n",
       "  6083,\n",
       "  6089,\n",
       "  9824,\n",
       "  886,\n",
       "  6098,\n",
       "  6420,\n",
       "  4179,\n",
       "  3040,\n",
       "  6114,\n",
       "  9136,\n",
       "  9136,\n",
       "  7477,\n",
       "  7570,\n",
       "  6147,\n",
       "  6150,\n",
       "  1336,\n",
       "  6158,\n",
       "  4456,\n",
       "  6178,\n",
       "  6178,\n",
       "  6182,\n",
       "  6185,\n",
       "  9805,\n",
       "  3653,\n",
       "  6205,\n",
       "  4957,\n",
       "  2061,\n",
       "  6219,\n",
       "  6219,\n",
       "  3566,\n",
       "  2540,\n",
       "  79,\n",
       "  6243,\n",
       "  6245,\n",
       "  6248,\n",
       "  6256,\n",
       "  6266,\n",
       "  6268,\n",
       "  1730,\n",
       "  7949,\n",
       "  963,\n",
       "  9403,\n",
       "  6278,\n",
       "  6279,\n",
       "  6283,\n",
       "  506,\n",
       "  9921,\n",
       "  4654,\n",
       "  6309,\n",
       "  6324,\n",
       "  6328,\n",
       "  6328,\n",
       "  6331,\n",
       "  8227,\n",
       "  8892,\n",
       "  1357,\n",
       "  2868,\n",
       "  2868,\n",
       "  ...],\n",
       " 'sim_real_cl': [0.3787729,\n",
       "  0.33357036,\n",
       "  0.34057662,\n",
       "  0.40127861,\n",
       "  0.26933986,\n",
       "  0.3935858,\n",
       "  0.18410777,\n",
       "  -0.02725693,\n",
       "  0.26364726,\n",
       "  0.32124493,\n",
       "  0.2091138,\n",
       "  0.42649508,\n",
       "  0.52084446,\n",
       "  0.43190652,\n",
       "  0.38331258,\n",
       "  0.37018222,\n",
       "  0.18947527,\n",
       "  0.3203189,\n",
       "  0.37928212,\n",
       "  0.25097913,\n",
       "  0.36523888,\n",
       "  0.35098022,\n",
       "  0.20751303,\n",
       "  0.14229026,\n",
       "  0.29570252,\n",
       "  0.24355383,\n",
       "  0.25747424,\n",
       "  0.2977659,\n",
       "  0.31844375,\n",
       "  0.45606214,\n",
       "  0.4928539,\n",
       "  0.213122,\n",
       "  0.36719498,\n",
       "  0.40999764,\n",
       "  0.3509307,\n",
       "  0.37673873,\n",
       "  0.42558947,\n",
       "  0.34885246,\n",
       "  0.3792551,\n",
       "  0.43224815,\n",
       "  0.42149827,\n",
       "  0.342443,\n",
       "  0.30489296,\n",
       "  0.31195438,\n",
       "  0.32134223,\n",
       "  0.2336345,\n",
       "  0.30275887,\n",
       "  0.21646827,\n",
       "  0.26664498,\n",
       "  0.34172577,\n",
       "  0.26329032,\n",
       "  0.19079506,\n",
       "  0.38968784,\n",
       "  0.46470517,\n",
       "  0.28184986,\n",
       "  0.47577477,\n",
       "  0.3236446,\n",
       "  0.17012051,\n",
       "  0.4184053,\n",
       "  0.29155838,\n",
       "  0.37198472,\n",
       "  0.3039737,\n",
       "  0.2955222,\n",
       "  0.32766107,\n",
       "  0.4565553,\n",
       "  0.4379038,\n",
       "  0.23703375,\n",
       "  0.2778447,\n",
       "  0.35937935,\n",
       "  0.41440386,\n",
       "  0.4636144,\n",
       "  0.2584417,\n",
       "  0.3699586,\n",
       "  0.1459429,\n",
       "  0.29883504,\n",
       "  0.25679862,\n",
       "  0.19214186,\n",
       "  0.37062454,\n",
       "  0.2946927,\n",
       "  0.5634748,\n",
       "  0.42650035,\n",
       "  0.09040867,\n",
       "  0.38539058,\n",
       "  0.44813305,\n",
       "  0.3657999,\n",
       "  0.22164902,\n",
       "  0.3193235,\n",
       "  0.23355615,\n",
       "  0.23814763,\n",
       "  0.26242074,\n",
       "  0.33779934,\n",
       "  0.38991258,\n",
       "  0.21593297,\n",
       "  0.38072777,\n",
       "  0.006670229,\n",
       "  0.355469,\n",
       "  0.21754567,\n",
       "  0.3703248,\n",
       "  0.37533367,\n",
       "  0.46206337,\n",
       "  0.26841798,\n",
       "  0.31381726,\n",
       "  0.3527118,\n",
       "  0.29139495,\n",
       "  0.4066427,\n",
       "  0.35451347,\n",
       "  0.3427993,\n",
       "  0.22809482,\n",
       "  0.60851157,\n",
       "  0.5783013,\n",
       "  0.45473248,\n",
       "  0.5249647,\n",
       "  0.2793603,\n",
       "  0.17586529,\n",
       "  0.12398876,\n",
       "  0.12776591,\n",
       "  0.4025011,\n",
       "  0.44717357,\n",
       "  0.37318462,\n",
       "  0.34161538,\n",
       "  0.3578186,\n",
       "  0.41126505,\n",
       "  0.1778372,\n",
       "  0.42082605,\n",
       "  0.3607023,\n",
       "  0.105744235,\n",
       "  0.36186773,\n",
       "  0.31577653,\n",
       "  0.65484613,\n",
       "  0.26831466,\n",
       "  0.17223302,\n",
       "  0.45462653,\n",
       "  -0.034710795,\n",
       "  0.4408933,\n",
       "  0.43140513,\n",
       "  0.2975729,\n",
       "  0.16361752,\n",
       "  0.38986683,\n",
       "  0.36841536,\n",
       "  0.16119911,\n",
       "  0.14757597,\n",
       "  0.3606065,\n",
       "  0.40010127,\n",
       "  0.28260383,\n",
       "  0.2704673,\n",
       "  0.037324503,\n",
       "  0.35611895,\n",
       "  0.4133738,\n",
       "  0.37434924,\n",
       "  0.18662629,\n",
       "  0.2873228,\n",
       "  0.30991238,\n",
       "  0.3217488,\n",
       "  0.345231,\n",
       "  0.29366237,\n",
       "  0.34268847,\n",
       "  0.43924046,\n",
       "  0.10582006,\n",
       "  0.28680503,\n",
       "  0.31947792,\n",
       "  0.23722412,\n",
       "  0.3196853,\n",
       "  0.2792542,\n",
       "  0.2996548,\n",
       "  0.36475638,\n",
       "  0.32673752,\n",
       "  0.395213,\n",
       "  0.23776942,\n",
       "  0.1208905,\n",
       "  0.2865997,\n",
       "  0.30278373,\n",
       "  0.074638456,\n",
       "  0.25418264,\n",
       "  0.103733875,\n",
       "  0.23697227,\n",
       "  0.22572044,\n",
       "  0.2928269,\n",
       "  0.4374851,\n",
       "  0.29363763,\n",
       "  0.3016799,\n",
       "  0.46274403,\n",
       "  0.10605295,\n",
       "  0.68819547,\n",
       "  0.34731105,\n",
       "  0.3061601,\n",
       "  0.37139404,\n",
       "  0.7032964,\n",
       "  0.6481538,\n",
       "  0.33436894,\n",
       "  0.55964446,\n",
       "  0.49240702,\n",
       "  0.21386087,\n",
       "  0.39079362,\n",
       "  0.468408,\n",
       "  0.26902044,\n",
       "  0.31848183,\n",
       "  0.2449289,\n",
       "  0.22549212,\n",
       "  0.2514611,\n",
       "  0.45975935,\n",
       "  0.5054563,\n",
       "  0.50845623,\n",
       "  0.2869135,\n",
       "  0.24704859,\n",
       "  0.41505522,\n",
       "  0.5617101,\n",
       "  0.38863164,\n",
       "  0.40108037,\n",
       "  0.40735948,\n",
       "  0.3777827,\n",
       "  0.35507533,\n",
       "  0.30054674,\n",
       "  0.12250869,\n",
       "  0.35738742,\n",
       "  0.38869327,\n",
       "  0.42099515,\n",
       "  0.28497714,\n",
       "  0.31473464,\n",
       "  0.31411296,\n",
       "  0.31332102,\n",
       "  0.27431104,\n",
       "  0.32962996,\n",
       "  0.306643,\n",
       "  0.2248585,\n",
       "  0.2416254,\n",
       "  0.21856067,\n",
       "  0.33391652,\n",
       "  0.15984324,\n",
       "  0.3279441,\n",
       "  0.39792246,\n",
       "  0.35142183,\n",
       "  0.31697655,\n",
       "  0.2496351,\n",
       "  0.27685654,\n",
       "  0.19822055,\n",
       "  0.39254576,\n",
       "  0.296673,\n",
       "  0.38232592,\n",
       "  0.17421165,\n",
       "  0.31916294,\n",
       "  0.4303757,\n",
       "  0.22975382,\n",
       "  0.13012901,\n",
       "  0.8371222,\n",
       "  0.5841209,\n",
       "  0.34337765,\n",
       "  0.14982411,\n",
       "  0.41528583,\n",
       "  0.40026367,\n",
       "  0.12647867,\n",
       "  0.3389252,\n",
       "  0.34386873,\n",
       "  0.32344273,\n",
       "  0.28978065,\n",
       "  0.2531302,\n",
       "  0.35579824,\n",
       "  0.16945094,\n",
       "  0.24290575,\n",
       "  0.3351859,\n",
       "  0.48900333,\n",
       "  0.4251233,\n",
       "  0.15740275,\n",
       "  0.3093766,\n",
       "  0.12710658,\n",
       "  0.2600562,\n",
       "  0.360188,\n",
       "  0.54932255,\n",
       "  0.33595356,\n",
       "  0.2484807,\n",
       "  0.42639172,\n",
       "  0.39075303,\n",
       "  0.31114167,\n",
       "  0.2544822,\n",
       "  0.30311984,\n",
       "  0.39364067,\n",
       "  0.47737247,\n",
       "  0.3858334,\n",
       "  0.5484719,\n",
       "  0.31904316,\n",
       "  0.27050987,\n",
       "  0.2717136,\n",
       "  0.48399597,\n",
       "  0.36543366,\n",
       "  0.20826057,\n",
       "  0.35023257,\n",
       "  0.48847902,\n",
       "  0.64263403,\n",
       "  0.36329693,\n",
       "  0.29128623,\n",
       "  0.37783653,\n",
       "  0.34876063,\n",
       "  0.45350924,\n",
       "  0.2498357,\n",
       "  0.086918615,\n",
       "  0.24470645,\n",
       "  0.20597276,\n",
       "  0.30824777,\n",
       "  0.41116992,\n",
       "  0.4107648,\n",
       "  0.21281692,\n",
       "  0.34457627,\n",
       "  0.50026643,\n",
       "  0.4481678,\n",
       "  0.45642638,\n",
       "  0.3519879,\n",
       "  0.20245342,\n",
       "  0.39019915,\n",
       "  0.41213298,\n",
       "  0.28176022,\n",
       "  0.40068975,\n",
       "  0.06690055,\n",
       "  0.5485502,\n",
       "  0.6046735,\n",
       "  0.30250555,\n",
       "  0.47111934,\n",
       "  0.32541448,\n",
       "  0.4067713,\n",
       "  0.34411576,\n",
       "  0.19461358,\n",
       "  0.36031014,\n",
       "  0.2621104,\n",
       "  0.09117855,\n",
       "  0.29102117,\n",
       "  0.3877665,\n",
       "  0.3078926,\n",
       "  0.38744003,\n",
       "  0.2651633,\n",
       "  0.5257171,\n",
       "  0.09937758,\n",
       "  0.38049698,\n",
       "  0.12511593,\n",
       "  0.30897215,\n",
       "  0.29133102,\n",
       "  0.45993495,\n",
       "  0.26224804,\n",
       "  0.31883454,\n",
       "  0.35666817,\n",
       "  0.3342549,\n",
       "  0.23466992,\n",
       "  0.27631807,\n",
       "  0.33035624,\n",
       "  0.3431661,\n",
       "  0.310273,\n",
       "  0.29893607,\n",
       "  0.36527196,\n",
       "  0.26404098,\n",
       "  0.29780394,\n",
       "  0.22040632,\n",
       "  0.56163186,\n",
       "  0.38686946,\n",
       "  0.24935366,\n",
       "  0.36828077,\n",
       "  0.3155081,\n",
       "  0.33832353,\n",
       "  0.31832498,\n",
       "  0.298163,\n",
       "  0.40971118,\n",
       "  0.26590568,\n",
       "  0.39318103,\n",
       "  0.2784722,\n",
       "  0.33632654,\n",
       "  0.2987997,\n",
       "  0.85913384,\n",
       "  0.6219476,\n",
       "  0.52909505,\n",
       "  0.29940027,\n",
       "  0.29743147,\n",
       "  0.27172178,\n",
       "  0.21058695,\n",
       "  0.37209147,\n",
       "  0.25241792,\n",
       "  0.3907627,\n",
       "  0.33451727,\n",
       "  0.3907978,\n",
       "  0.018858213,\n",
       "  0.31788245,\n",
       "  0.07659031,\n",
       "  0.35591936,\n",
       "  0.33391774,\n",
       "  0.32837468,\n",
       "  0.35498828,\n",
       "  0.32880473,\n",
       "  0.25766253,\n",
       "  0.27926317,\n",
       "  -0.037731007,\n",
       "  0.38088804,\n",
       "  0.07730405,\n",
       "  0.12495045,\n",
       "  0.21167485,\n",
       "  0.34399602,\n",
       "  0.2351905,\n",
       "  0.35900328,\n",
       "  0.41379505,\n",
       "  0.300414,\n",
       "  0.3081046,\n",
       "  0.3110438,\n",
       "  0.1776954,\n",
       "  0.22724402,\n",
       "  0.251141,\n",
       "  0.34240872,\n",
       "  0.26258257,\n",
       "  0.45258242,\n",
       "  0.3305023,\n",
       "  0.26317328,\n",
       "  0.35305604,\n",
       "  0.3211449,\n",
       "  0.31311214,\n",
       "  0.2843755,\n",
       "  0.2540654,\n",
       "  0.34086236,\n",
       "  0.19318251,\n",
       "  0.33547962,\n",
       "  0.33001775,\n",
       "  0.26323333,\n",
       "  0.35322958,\n",
       "  0.33175576,\n",
       "  0.24937934,\n",
       "  0.31263793,\n",
       "  0.20710859,\n",
       "  0.27360618,\n",
       "  0.22088802,\n",
       "  0.3250268,\n",
       "  0.3672686,\n",
       "  0.5658771,\n",
       "  0.16768593,\n",
       "  0.38746607,\n",
       "  0.47701627,\n",
       "  0.34967342,\n",
       "  0.43201017,\n",
       "  0.2776736,\n",
       "  0.2620046,\n",
       "  0.19637887,\n",
       "  0.35155278,\n",
       "  0.23694748,\n",
       "  0.29535854,\n",
       "  0.28496036,\n",
       "  0.50782734,\n",
       "  0.36014274,\n",
       "  0.28818858,\n",
       "  0.37738043,\n",
       "  0.23216097,\n",
       "  0.4208672,\n",
       "  0.27061006,\n",
       "  0.34764874,\n",
       "  0.26861227,\n",
       "  0.4200276,\n",
       "  0.50057214,\n",
       "  0.22463356,\n",
       "  0.3946734,\n",
       "  0.4588916,\n",
       "  0.19263043,\n",
       "  0.3327844,\n",
       "  0.2575863,\n",
       "  0.30859986,\n",
       "  0.18137182,\n",
       "  0.21463174,\n",
       "  0.31638718,\n",
       "  0.30822805,\n",
       "  0.2735458,\n",
       "  0.31417984,\n",
       "  0.20212522,\n",
       "  0.17965797,\n",
       "  0.2639815,\n",
       "  0.30567074,\n",
       "  0.19970432,\n",
       "  0.1752398,\n",
       "  0.29364696,\n",
       "  0.46086663,\n",
       "  0.2554891,\n",
       "  0.28505385,\n",
       "  0.30898708,\n",
       "  0.30228925,\n",
       "  0.3503969,\n",
       "  0.40498734,\n",
       "  0.36367074,\n",
       "  0.3542598,\n",
       "  0.20399086,\n",
       "  0.23229241,\n",
       "  0.43691993,\n",
       "  0.38208652,\n",
       "  0.4390872,\n",
       "  0.36408317,\n",
       "  0.50452626,\n",
       "  0.4759128,\n",
       "  0.24525718,\n",
       "  0.5035988,\n",
       "  0.26861507,\n",
       "  0.36420688,\n",
       "  0.26872122,\n",
       "  0.20759866,\n",
       "  0.20903113,\n",
       "  0.40276754,\n",
       "  0.014138211,\n",
       "  0.46220368,\n",
       "  0.1040348,\n",
       "  0.4507947,\n",
       "  0.31883544,\n",
       "  0.03492943,\n",
       "  0.22207937,\n",
       "  0.3576606,\n",
       "  0.14253347,\n",
       "  0.3178575,\n",
       "  0.3093272,\n",
       "  0.5609969,\n",
       "  0.29128262,\n",
       "  0.5007311,\n",
       "  0.52546597,\n",
       "  0.6455319,\n",
       "  0.2755384,\n",
       "  0.27806562,\n",
       "  0.5733762,\n",
       "  0.40368262,\n",
       "  0.35373336,\n",
       "  0.5045876,\n",
       "  0.5054751,\n",
       "  0.2510268,\n",
       "  0.36934453,\n",
       "  0.34045902,\n",
       "  0.44076133,\n",
       "  0.4047134,\n",
       "  0.42376736,\n",
       "  0.38677606,\n",
       "  0.26385367,\n",
       "  0.3620284,\n",
       "  0.3515054,\n",
       "  0.20769578,\n",
       "  0.3023721,\n",
       "  0.34825683,\n",
       "  0.43002743,\n",
       "  0.43094957,\n",
       "  0.26301962,\n",
       "  0.38328266,\n",
       "  0.39368445,\n",
       "  0.35399026,\n",
       "  0.43895912,\n",
       "  0.35329467,\n",
       "  0.1503548,\n",
       "  0.48426917,\n",
       "  0.24358757,\n",
       "  0.3318736,\n",
       "  0.28637624,\n",
       "  0.39294523,\n",
       "  0.221265,\n",
       "  0.26434776,\n",
       "  0.19372824,\n",
       "  0.3909698,\n",
       "  0.43709815,\n",
       "  0.35918623,\n",
       "  0.35903555,\n",
       "  0.5919688,\n",
       "  0.30927318,\n",
       "  0.29227525,\n",
       "  0.607484,\n",
       "  0.3989334,\n",
       "  0.29259866,\n",
       "  0.40020463,\n",
       "  0.3388989,\n",
       "  0.34255692,\n",
       "  0.17583512,\n",
       "  0.18198888,\n",
       "  0.3569228,\n",
       "  0.5556954,\n",
       "  0.10955407,\n",
       "  -0.083287135,\n",
       "  0.46717733,\n",
       "  0.4483294,\n",
       "  0.50417846,\n",
       "  0.2478517,\n",
       "  0.38684493,\n",
       "  0.2772646,\n",
       "  0.30776143,\n",
       "  0.29168507,\n",
       "  0.31555462,\n",
       "  0.40656096,\n",
       "  0.30255264,\n",
       "  0.35951728,\n",
       "  0.39702672,\n",
       "  0.3542971,\n",
       "  0.20873903,\n",
       "  0.36801755,\n",
       "  0.27824926,\n",
       "  0.2729613,\n",
       "  0.31942445,\n",
       "  0.23943552,\n",
       "  0.143925,\n",
       "  0.3465281,\n",
       "  0.57105017,\n",
       "  0.30305374,\n",
       "  0.30483192,\n",
       "  0.2682068,\n",
       "  0.30790207,\n",
       "  0.58474237,\n",
       "  0.36668333,\n",
       "  0.4216623,\n",
       "  0.46258754,\n",
       "  0.2996756,\n",
       "  0.445485,\n",
       "  0.4129691,\n",
       "  0.4701079,\n",
       "  0.37502164,\n",
       "  0.25255063,\n",
       "  0.42051476,\n",
       "  0.34124517,\n",
       "  0.2544526,\n",
       "  0.57893705,\n",
       "  0.26892054,\n",
       "  0.17368717,\n",
       "  0.18969789,\n",
       "  0.4776625,\n",
       "  0.30555642,\n",
       "  0.23251578,\n",
       "  0.31693873,\n",
       "  0.48589325,\n",
       "  0.4124924,\n",
       "  0.4255623,\n",
       "  0.34527707,\n",
       "  0.3740618,\n",
       "  0.38475692,\n",
       "  0.050441936,\n",
       "  0.47894844,\n",
       "  0.46619046,\n",
       "  0.3326062,\n",
       "  0.22998808,\n",
       "  0.38467014,\n",
       "  0.33182117,\n",
       "  0.16162074,\n",
       "  0.33847672,\n",
       "  0.33831227,\n",
       "  0.27790096,\n",
       "  0.23394987,\n",
       "  0.40830404,\n",
       "  0.20052683,\n",
       "  0.31242052,\n",
       "  0.10640319,\n",
       "  0.22183459,\n",
       "  0.41312853,\n",
       "  0.12669623,\n",
       "  0.2638144,\n",
       "  0.2530144,\n",
       "  0.2641933,\n",
       "  0.35075793,\n",
       "  0.13867056,\n",
       "  0.425418,\n",
       "  0.3937989,\n",
       "  0.4623094,\n",
       "  0.25051823,\n",
       "  0.35920462,\n",
       "  0.29537714,\n",
       "  0.49086618,\n",
       "  0.5058651,\n",
       "  0.13186948,\n",
       "  0.32675815,\n",
       "  0.229711,\n",
       "  0.40831876,\n",
       "  0.46677893,\n",
       "  0.5360196,\n",
       "  0.5611099,\n",
       "  0.30672485,\n",
       "  0.08968119,\n",
       "  0.49333206,\n",
       "  0.5121912,\n",
       "  0.012214635,\n",
       "  0.2940391,\n",
       "  0.3058501,\n",
       "  0.15164529,\n",
       "  0.03782279,\n",
       "  0.35710463,\n",
       "  0.3762977,\n",
       "  0.49049586,\n",
       "  0.49351323,\n",
       "  0.34533373,\n",
       "  0.5181758,\n",
       "  0.3534354,\n",
       "  0.37431806,\n",
       "  0.13630939,\n",
       "  0.34209433,\n",
       "  0.46137506,\n",
       "  0.48478943,\n",
       "  0.46277463,\n",
       "  0.29351857,\n",
       "  0.277862,\n",
       "  0.25625587,\n",
       "  0.31055272,\n",
       "  0.40775007,\n",
       "  0.576291,\n",
       "  0.4823063,\n",
       "  0.17998719,\n",
       "  0.32688254,\n",
       "  0.528005,\n",
       "  0.33624274,\n",
       "  0.31016338,\n",
       "  0.37568858,\n",
       "  0.33395475,\n",
       "  0.27425534,\n",
       "  0.40920722,\n",
       "  0.42883688,\n",
       "  0.4147225,\n",
       "  0.3632689,\n",
       "  0.3388161,\n",
       "  0.33653438,\n",
       "  0.46170345,\n",
       "  0.29199544,\n",
       "  0.21362525,\n",
       "  0.33269235,\n",
       "  0.3835597,\n",
       "  0.23406474,\n",
       "  0.25396365,\n",
       "  0.044510156,\n",
       "  0.22715002,\n",
       "  0.21681646,\n",
       "  0.1300875,\n",
       "  0.4401312,\n",
       "  0.28331542,\n",
       "  0.36595494,\n",
       "  0.48302186,\n",
       "  0.47525042,\n",
       "  0.4370502,\n",
       "  0.21135202,\n",
       "  0.36822262,\n",
       "  0.26989278,\n",
       "  0.10370466,\n",
       "  0.19593132,\n",
       "  0.32343853,\n",
       "  0.3559902,\n",
       "  0.36472014,\n",
       "  0.3156528,\n",
       "  0.18239668,\n",
       "  0.40670896,\n",
       "  0.38357514,\n",
       "  0.44723225,\n",
       "  0.39595604,\n",
       "  0.3724956,\n",
       "  0.5053837,\n",
       "  0.44080073,\n",
       "  0.35618454,\n",
       "  0.4094695,\n",
       "  0.4803031,\n",
       "  0.20211877,\n",
       "  0.21668813,\n",
       "  0.22832322,\n",
       "  0.6743851,\n",
       "  0.8417259,\n",
       "  0.113193855,\n",
       "  0.27954435,\n",
       "  0.31120828,\n",
       "  0.3000186,\n",
       "  0.28110325,\n",
       "  0.27152365,\n",
       "  0.43941242,\n",
       "  0.40260744,\n",
       "  0.4267552,\n",
       "  0.40579647,\n",
       "  0.36777,\n",
       "  0.36787182,\n",
       "  0.4254895,\n",
       "  0.375032,\n",
       "  0.3429888,\n",
       "  0.32863852,\n",
       "  0.25263572,\n",
       "  0.28432575,\n",
       "  0.2744648,\n",
       "  0.24657276,\n",
       "  0.3157993,\n",
       "  0.26845813,\n",
       "  0.44088584,\n",
       "  0.18242064,\n",
       "  0.30667323,\n",
       "  0.21336624,\n",
       "  0.51364,\n",
       "  0.3042211,\n",
       "  0.5097574,\n",
       "  0.12476061,\n",
       "  0.5983263,\n",
       "  0.583158,\n",
       "  0.60265005,\n",
       "  0.23902854,\n",
       "  0.4288702,\n",
       "  0.33656153,\n",
       "  0.3470079,\n",
       "  0.37439418,\n",
       "  0.34888953,\n",
       "  0.3518002,\n",
       "  0.2845639,\n",
       "  0.314525,\n",
       "  0.3719561,\n",
       "  0.2702698,\n",
       "  0.24740776,\n",
       "  0.26947224,\n",
       "  0.3475972,\n",
       "  0.34052324,\n",
       "  0.3383906,\n",
       "  0.3722751,\n",
       "  0.33147395,\n",
       "  0.3438071,\n",
       "  0.30738375,\n",
       "  0.31709284,\n",
       "  0.22879618,\n",
       "  0.34968328,\n",
       "  0.10051687,\n",
       "  0.41779476,\n",
       "  0.42465603,\n",
       "  0.35246217,\n",
       "  0.32148212,\n",
       "  0.32366943,\n",
       "  0.26060832,\n",
       "  0.28928584,\n",
       "  0.16783546,\n",
       "  0.19703695,\n",
       "  0.22805966,\n",
       "  0.22074206,\n",
       "  0.22724335,\n",
       "  0.24630472,\n",
       "  0.47767895,\n",
       "  0.28792801,\n",
       "  0.31353596,\n",
       "  0.29779792,\n",
       "  0.15027204,\n",
       "  0.3075143,\n",
       "  0.34412968,\n",
       "  0.25663733,\n",
       "  0.058435917,\n",
       "  0.20660804,\n",
       "  0.23922849,\n",
       "  0.331474,\n",
       "  0.33797723,\n",
       "  0.27929047,\n",
       "  0.39883524,\n",
       "  0.21317482,\n",
       "  0.2617553,\n",
       "  0.25671756,\n",
       "  0.2062632,\n",
       "  0.1130294,\n",
       "  0.35248482,\n",
       "  0.37760788,\n",
       "  0.43015224,\n",
       "  0.25267005,\n",
       "  0.3293203,\n",
       "  0.23145756,\n",
       "  0.4023648,\n",
       "  0.29004613,\n",
       "  0.31196934,\n",
       "  0.31024337,\n",
       "  0.06440951,\n",
       "  0.29152453,\n",
       "  0.36752003,\n",
       "  0.20613277,\n",
       "  0.22243263,\n",
       "  0.3002621,\n",
       "  0.33055452,\n",
       "  0.2908416,\n",
       "  0.32179394,\n",
       "  0.31190127,\n",
       "  0.40007854,\n",
       "  0.33129048,\n",
       "  0.22326405,\n",
       "  0.33372682,\n",
       "  0.28664592,\n",
       "  0.21489221,\n",
       "  0.37158632,\n",
       "  0.37341335,\n",
       "  0.38572073,\n",
       "  0.51787686,\n",
       "  0.24621615,\n",
       "  0.44924924,\n",
       "  0.31150764,\n",
       "  0.25044286,\n",
       "  0.28962433,\n",
       "  0.22521412,\n",
       "  0.28741568,\n",
       "  0.34108573,\n",
       "  0.26285344,\n",
       "  0.10915394,\n",
       "  0.39010167,\n",
       "  0.2890413,\n",
       "  0.3009094,\n",
       "  0.20865032,\n",
       "  0.35207683,\n",
       "  0.35554218,\n",
       "  0.32947326,\n",
       "  0.1659408,\n",
       "  0.32464027,\n",
       "  0.241647,\n",
       "  0.34791452,\n",
       "  0.3554474,\n",
       "  0.26431417,\n",
       "  0.3547568,\n",
       "  0.23908038,\n",
       "  0.10448645,\n",
       "  0.39414784,\n",
       "  0.31662813,\n",
       "  0.2955552,\n",
       "  0.11143908,\n",
       "  0.25712693,\n",
       "  0.18022124,\n",
       "  0.3502481,\n",
       "  0.3439225,\n",
       "  0.46648043,\n",
       "  0.24791734,\n",
       "  0.059896484,\n",
       "  0.2556638,\n",
       "  0.11680098,\n",
       "  0.42053536,\n",
       "  0.17629695,\n",
       "  0.18265533,\n",
       "  0.354806,\n",
       "  0.43027744,\n",
       "  0.30189458,\n",
       "  0.15929079,\n",
       "  0.18087065,\n",
       "  0.324534,\n",
       "  -0.03729351,\n",
       "  0.07985321,\n",
       "  0.35218206,\n",
       "  0.3019581,\n",
       "  0.29593134,\n",
       "  0.38017404,\n",
       "  0.2456581,\n",
       "  0.2223362,\n",
       "  0.38327783,\n",
       "  0.2742049,\n",
       "  0.28829354,\n",
       "  0.18264255,\n",
       "  0.32881102,\n",
       "  0.08379334,\n",
       "  0.118300006,\n",
       "  0.2603593,\n",
       "  0.08633037,\n",
       "  0.41091377,\n",
       "  0.3435217,\n",
       "  0.20690116,\n",
       "  0.49166754,\n",
       "  0.4946598,\n",
       "  0.6069566,\n",
       "  0.4778593,\n",
       "  0.41877502,\n",
       "  0.3876703,\n",
       "  0.3634157,\n",
       "  0.48002037,\n",
       "  0.33545485,\n",
       "  0.34180528,\n",
       "  0.312692,\n",
       "  0.31644708,\n",
       "  0.41883284,\n",
       "  0.38128066,\n",
       "  0.1549372,\n",
       "  0.33813292,\n",
       "  0.41073328,\n",
       "  0.31927615,\n",
       "  0.2776748,\n",
       "  0.18143666,\n",
       "  0.5331725,\n",
       "  0.25643575,\n",
       "  0.24006173,\n",
       "  0.38646325,\n",
       "  0.20580238,\n",
       "  0.56660736,\n",
       "  0.41856343,\n",
       "  0.14888164,\n",
       "  0.454072,\n",
       "  0.28153035,\n",
       "  0.31883967,\n",
       "  0.52310395,\n",
       "  0.2836937,\n",
       "  0.5319642,\n",
       "  0.33439615,\n",
       "  0.14822152,\n",
       "  0.35777617,\n",
       "  0.28751752,\n",
       "  0.37287575,\n",
       "  0.35894674,\n",
       "  0.31730834,\n",
       "  0.2653129,\n",
       "  0.27256507,\n",
       "  0.29495293,\n",
       "  0.3507497,\n",
       "  0.38424915,\n",
       "  0.43138143,\n",
       "  0.32312903,\n",
       "  0.524423,\n",
       "  0.43110347,\n",
       "  0.22454819,\n",
       "  0.35415113,\n",
       "  0.31656379,\n",
       "  0.31986874,\n",
       "  0.36301565,\n",
       "  0.41688818,\n",
       "  0.46767843,\n",
       "  0.3216679,\n",
       "  0.23968731,\n",
       "  0.38956168,\n",
       "  0.3929658,\n",
       "  0.3467214,\n",
       "  0.39973179,\n",
       "  0.37113106,\n",
       "  0.38532588,\n",
       "  0.2235266,\n",
       "  0.3759057,\n",
       "  0.37708634,\n",
       "  0.37391815,\n",
       "  0.37768105,\n",
       "  ...],\n",
       " 'sim_best_knn': [0.3787729,\n",
       "  0.42919788,\n",
       "  0.34057662,\n",
       "  0.40127861,\n",
       "  0.2897963,\n",
       "  0.3935858,\n",
       "  0.3012818,\n",
       "  0.28370714,\n",
       "  0.2730968,\n",
       "  0.3799205,\n",
       "  0.35196215,\n",
       "  0.42649508,\n",
       "  0.52084446,\n",
       "  0.43190652,\n",
       "  0.38331258,\n",
       "  0.37018222,\n",
       "  0.28940135,\n",
       "  0.32563576,\n",
       "  0.3890313,\n",
       "  0.27012256,\n",
       "  0.38114762,\n",
       "  0.38469598,\n",
       "  0.4198647,\n",
       "  0.29701722,\n",
       "  0.30783913,\n",
       "  0.3177352,\n",
       "  0.32597142,\n",
       "  0.31122804,\n",
       "  0.33221495,\n",
       "  0.45606214,\n",
       "  0.4928539,\n",
       "  0.2986697,\n",
       "  0.36719498,\n",
       "  0.40999764,\n",
       "  0.36337793,\n",
       "  0.37673873,\n",
       "  0.42558947,\n",
       "  0.34958708,\n",
       "  0.3792551,\n",
       "  0.44238305,\n",
       "  0.42149827,\n",
       "  0.37488732,\n",
       "  0.327809,\n",
       "  0.31195438,\n",
       "  0.32134223,\n",
       "  0.43066204,\n",
       "  0.32640645,\n",
       "  0.32864124,\n",
       "  0.31735444,\n",
       "  0.34172577,\n",
       "  0.31939608,\n",
       "  0.41817093,\n",
       "  0.39443552,\n",
       "  0.46470517,\n",
       "  0.31228778,\n",
       "  0.47577477,\n",
       "  0.3236446,\n",
       "  0.2754858,\n",
       "  0.4184053,\n",
       "  0.39031088,\n",
       "  0.37198472,\n",
       "  0.3039737,\n",
       "  0.31903297,\n",
       "  0.3396576,\n",
       "  0.4565553,\n",
       "  0.4379038,\n",
       "  0.29853916,\n",
       "  0.38006932,\n",
       "  0.35937935,\n",
       "  0.41440386,\n",
       "  0.4636144,\n",
       "  0.30890834,\n",
       "  0.3699586,\n",
       "  0.3187923,\n",
       "  0.33489037,\n",
       "  0.32644165,\n",
       "  0.2437264,\n",
       "  0.37062454,\n",
       "  0.2946927,\n",
       "  0.5634748,\n",
       "  0.42650035,\n",
       "  0.21934769,\n",
       "  0.38539058,\n",
       "  0.44813305,\n",
       "  0.3657999,\n",
       "  0.3102197,\n",
       "  0.37307107,\n",
       "  0.2812924,\n",
       "  0.36471772,\n",
       "  0.26242074,\n",
       "  0.33779934,\n",
       "  0.38991258,\n",
       "  0.3587632,\n",
       "  0.38072777,\n",
       "  0.28617555,\n",
       "  0.39314497,\n",
       "  0.35239452,\n",
       "  0.3703248,\n",
       "  0.39057454,\n",
       "  0.46206337,\n",
       "  0.3696918,\n",
       "  0.37468138,\n",
       "  0.38148302,\n",
       "  0.29139495,\n",
       "  0.4066427,\n",
       "  0.35451347,\n",
       "  0.35047406,\n",
       "  0.2678833,\n",
       "  0.60851157,\n",
       "  0.5783013,\n",
       "  0.45473248,\n",
       "  0.5249647,\n",
       "  0.3097785,\n",
       "  0.27983335,\n",
       "  0.32677287,\n",
       "  0.29182738,\n",
       "  0.4025011,\n",
       "  0.44717357,\n",
       "  0.39492097,\n",
       "  0.34161538,\n",
       "  0.377431,\n",
       "  0.41126505,\n",
       "  0.30058914,\n",
       "  0.42082605,\n",
       "  0.3607023,\n",
       "  0.30626047,\n",
       "  0.36186773,\n",
       "  0.33581606,\n",
       "  0.65484613,\n",
       "  0.3792373,\n",
       "  0.3196314,\n",
       "  0.45462653,\n",
       "  0.26782402,\n",
       "  0.4794932,\n",
       "  0.43140513,\n",
       "  0.32070696,\n",
       "  0.31266123,\n",
       "  0.38986683,\n",
       "  0.36841536,\n",
       "  0.2942354,\n",
       "  0.25435108,\n",
       "  0.3606065,\n",
       "  0.40010127,\n",
       "  0.298075,\n",
       "  0.31232917,\n",
       "  0.42423669,\n",
       "  0.35611895,\n",
       "  0.4133738,\n",
       "  0.37956688,\n",
       "  0.3640293,\n",
       "  0.29446742,\n",
       "  0.30991238,\n",
       "  0.35009798,\n",
       "  0.345231,\n",
       "  0.29366237,\n",
       "  0.3510899,\n",
       "  0.43924046,\n",
       "  0.38163465,\n",
       "  0.35004622,\n",
       "  0.35630265,\n",
       "  0.347193,\n",
       "  0.33460182,\n",
       "  0.33957922,\n",
       "  0.34321177,\n",
       "  0.40920198,\n",
       "  0.32673752,\n",
       "  0.395213,\n",
       "  0.3137775,\n",
       "  0.29151157,\n",
       "  0.34230256,\n",
       "  0.33496502,\n",
       "  0.37150022,\n",
       "  0.28383452,\n",
       "  0.32878888,\n",
       "  0.33327183,\n",
       "  0.25913364,\n",
       "  0.2928269,\n",
       "  0.4374851,\n",
       "  0.3737389,\n",
       "  0.38975132,\n",
       "  0.46274403,\n",
       "  0.41273218,\n",
       "  0.68819547,\n",
       "  0.34731105,\n",
       "  0.33906215,\n",
       "  0.37139404,\n",
       "  0.7032964,\n",
       "  0.6481538,\n",
       "  0.33436894,\n",
       "  0.55964446,\n",
       "  0.49240702,\n",
       "  0.3063155,\n",
       "  0.39079362,\n",
       "  0.468408,\n",
       "  0.29373968,\n",
       "  0.32115573,\n",
       "  0.31017476,\n",
       "  0.29347092,\n",
       "  0.28301018,\n",
       "  0.45975935,\n",
       "  0.5054563,\n",
       "  0.50845623,\n",
       "  0.30822763,\n",
       "  0.2764837,\n",
       "  0.41505522,\n",
       "  0.5617101,\n",
       "  0.38863164,\n",
       "  0.40108037,\n",
       "  0.40735948,\n",
       "  0.41187292,\n",
       "  0.35507533,\n",
       "  0.30054674,\n",
       "  0.36770713,\n",
       "  0.3645322,\n",
       "  0.38869327,\n",
       "  0.42099515,\n",
       "  0.30347466,\n",
       "  0.31473464,\n",
       "  0.35187247,\n",
       "  0.33734202,\n",
       "  0.31154546,\n",
       "  0.3420257,\n",
       "  0.3400636,\n",
       "  0.32461745,\n",
       "  0.33434874,\n",
       "  0.27585113,\n",
       "  0.3361743,\n",
       "  0.2674699,\n",
       "  0.3279441,\n",
       "  0.39792246,\n",
       "  0.35142183,\n",
       "  0.31868443,\n",
       "  0.34737396,\n",
       "  0.40744388,\n",
       "  0.3363231,\n",
       "  0.39254576,\n",
       "  0.4366682,\n",
       "  0.38232592,\n",
       "  0.30743077,\n",
       "  0.31916294,\n",
       "  0.4303757,\n",
       "  0.29790685,\n",
       "  0.3749656,\n",
       "  0.8371222,\n",
       "  0.5841209,\n",
       "  0.35054556,\n",
       "  0.32833767,\n",
       "  0.41528583,\n",
       "  0.40026367,\n",
       "  0.26234454,\n",
       "  0.35050377,\n",
       "  0.34386873,\n",
       "  0.38446534,\n",
       "  0.34908336,\n",
       "  0.34428868,\n",
       "  0.35579824,\n",
       "  0.28839892,\n",
       "  0.36234203,\n",
       "  0.35843328,\n",
       "  0.48900333,\n",
       "  0.4251233,\n",
       "  0.2656393,\n",
       "  0.31763458,\n",
       "  0.35182926,\n",
       "  0.31902844,\n",
       "  0.38045222,\n",
       "  0.54932255,\n",
       "  0.33595356,\n",
       "  0.28057295,\n",
       "  0.42639172,\n",
       "  0.39075303,\n",
       "  0.31114167,\n",
       "  0.2544822,\n",
       "  0.3416008,\n",
       "  0.39364067,\n",
       "  0.47737247,\n",
       "  0.3858334,\n",
       "  0.5484719,\n",
       "  0.33399168,\n",
       "  0.30425704,\n",
       "  0.31776196,\n",
       "  0.48399597,\n",
       "  0.36543366,\n",
       "  0.2883755,\n",
       "  0.35023257,\n",
       "  0.48847902,\n",
       "  0.64263403,\n",
       "  0.36329693,\n",
       "  0.3047726,\n",
       "  0.3919869,\n",
       "  0.34876063,\n",
       "  0.45350924,\n",
       "  0.30306393,\n",
       "  0.29869017,\n",
       "  0.34172702,\n",
       "  0.24655226,\n",
       "  0.361108,\n",
       "  0.41116992,\n",
       "  0.4107648,\n",
       "  0.3167851,\n",
       "  0.34802634,\n",
       "  0.50026643,\n",
       "  0.4481678,\n",
       "  0.45642638,\n",
       "  0.3519879,\n",
       "  0.32951587,\n",
       "  0.39019915,\n",
       "  0.41213298,\n",
       "  0.2833731,\n",
       "  0.40068975,\n",
       "  0.22145243,\n",
       "  0.5485502,\n",
       "  0.6046735,\n",
       "  0.32449692,\n",
       "  0.47111934,\n",
       "  0.32541448,\n",
       "  0.4067713,\n",
       "  0.34411576,\n",
       "  0.25198498,\n",
       "  0.42510986,\n",
       "  0.32288414,\n",
       "  0.26677313,\n",
       "  0.3805738,\n",
       "  0.3971883,\n",
       "  0.3338194,\n",
       "  0.38744003,\n",
       "  0.2755815,\n",
       "  0.5257171,\n",
       "  0.26044485,\n",
       "  0.38049698,\n",
       "  0.31557208,\n",
       "  0.30897215,\n",
       "  0.4602341,\n",
       "  0.45993495,\n",
       "  0.26224804,\n",
       "  0.38829404,\n",
       "  0.35666817,\n",
       "  0.34432387,\n",
       "  0.3401752,\n",
       "  0.2996489,\n",
       "  0.33035624,\n",
       "  0.3431661,\n",
       "  0.33627722,\n",
       "  0.31892896,\n",
       "  0.36527196,\n",
       "  0.27438688,\n",
       "  0.29780394,\n",
       "  0.3168438,\n",
       "  0.56163186,\n",
       "  0.40500164,\n",
       "  0.26817012,\n",
       "  0.38841057,\n",
       "  0.506462,\n",
       "  0.40564865,\n",
       "  0.35956433,\n",
       "  0.37119657,\n",
       "  0.40971118,\n",
       "  0.36112288,\n",
       "  0.39318103,\n",
       "  0.33788803,\n",
       "  0.4067762,\n",
       "  0.34138632,\n",
       "  0.85913384,\n",
       "  0.6219476,\n",
       "  0.52909505,\n",
       "  0.29940027,\n",
       "  0.32332414,\n",
       "  0.43022746,\n",
       "  0.3494347,\n",
       "  0.37209147,\n",
       "  0.3111454,\n",
       "  0.3907627,\n",
       "  0.33451727,\n",
       "  0.39116323,\n",
       "  0.35425574,\n",
       "  0.31788245,\n",
       "  0.22053567,\n",
       "  0.4603116,\n",
       "  0.36975813,\n",
       "  0.32837468,\n",
       "  0.37807393,\n",
       "  0.37192902,\n",
       "  0.402692,\n",
       "  0.3027205,\n",
       "  0.24968123,\n",
       "  0.3921818,\n",
       "  0.28429973,\n",
       "  0.25537318,\n",
       "  0.35285765,\n",
       "  0.34399602,\n",
       "  0.34574148,\n",
       "  0.399975,\n",
       "  0.41379505,\n",
       "  0.3316747,\n",
       "  0.32598364,\n",
       "  0.31450653,\n",
       "  0.2839291,\n",
       "  0.26585525,\n",
       "  0.3195933,\n",
       "  0.34240872,\n",
       "  0.39405167,\n",
       "  0.45258242,\n",
       "  0.39321896,\n",
       "  0.3259909,\n",
       "  0.35305604,\n",
       "  0.35077915,\n",
       "  0.33941248,\n",
       "  0.3384603,\n",
       "  0.2996474,\n",
       "  0.3774132,\n",
       "  0.36894917,\n",
       "  0.39601234,\n",
       "  0.33001775,\n",
       "  0.28690693,\n",
       "  0.35322958,\n",
       "  0.3471714,\n",
       "  0.34495753,\n",
       "  0.34202802,\n",
       "  0.34441334,\n",
       "  0.33988127,\n",
       "  0.3080901,\n",
       "  0.44400635,\n",
       "  0.3672686,\n",
       "  0.5658771,\n",
       "  0.27677763,\n",
       "  0.3885445,\n",
       "  0.5182339,\n",
       "  0.34967342,\n",
       "  0.44129202,\n",
       "  0.30056113,\n",
       "  0.30766428,\n",
       "  0.33176947,\n",
       "  0.39641562,\n",
       "  0.27730703,\n",
       "  0.3194344,\n",
       "  0.30822778,\n",
       "  0.5170976,\n",
       "  0.36014274,\n",
       "  0.32659662,\n",
       "  0.37738043,\n",
       "  0.31484053,\n",
       "  0.4211655,\n",
       "  0.32545334,\n",
       "  0.36675033,\n",
       "  0.387542,\n",
       "  0.4200276,\n",
       "  0.50057214,\n",
       "  0.33029515,\n",
       "  0.3946734,\n",
       "  0.4588916,\n",
       "  0.29258,\n",
       "  0.3327844,\n",
       "  0.3204553,\n",
       "  0.30859986,\n",
       "  0.33718818,\n",
       "  0.27346748,\n",
       "  0.32030585,\n",
       "  0.3540587,\n",
       "  0.3554824,\n",
       "  0.3331316,\n",
       "  0.3277679,\n",
       "  0.34888595,\n",
       "  0.26896727,\n",
       "  0.33219182,\n",
       "  0.2639044,\n",
       "  0.3412434,\n",
       "  0.29364696,\n",
       "  0.49661404,\n",
       "  0.32541454,\n",
       "  0.31778163,\n",
       "  0.32679513,\n",
       "  0.3132351,\n",
       "  0.3503969,\n",
       "  0.40498734,\n",
       "  0.3987524,\n",
       "  0.35989535,\n",
       "  0.33058763,\n",
       "  0.4192761,\n",
       "  0.43691993,\n",
       "  0.38208652,\n",
       "  0.4390872,\n",
       "  0.36408317,\n",
       "  0.50452626,\n",
       "  0.4759128,\n",
       "  0.3279211,\n",
       "  0.5035988,\n",
       "  0.3322159,\n",
       "  0.39322013,\n",
       "  0.33626103,\n",
       "  0.33614758,\n",
       "  0.26980916,\n",
       "  0.40276754,\n",
       "  0.34311634,\n",
       "  0.46220368,\n",
       "  0.34558198,\n",
       "  0.4507947,\n",
       "  0.32261565,\n",
       "  0.30138457,\n",
       "  0.31386888,\n",
       "  0.3576606,\n",
       "  0.24632832,\n",
       "  0.3589337,\n",
       "  0.33944917,\n",
       "  0.5609969,\n",
       "  0.35019523,\n",
       "  0.5007311,\n",
       "  0.52546597,\n",
       "  0.6455319,\n",
       "  0.3571616,\n",
       "  0.2929126,\n",
       "  0.5733762,\n",
       "  0.40368262,\n",
       "  0.35373336,\n",
       "  0.5045876,\n",
       "  0.5054751,\n",
       "  0.32843757,\n",
       "  0.36934453,\n",
       "  0.34045902,\n",
       "  0.44076133,\n",
       "  0.4047134,\n",
       "  0.42376736,\n",
       "  0.38677606,\n",
       "  0.31013268,\n",
       "  0.3620284,\n",
       "  0.3515054,\n",
       "  0.37946558,\n",
       "  0.3023721,\n",
       "  0.34825683,\n",
       "  0.43002743,\n",
       "  0.44256923,\n",
       "  0.31586418,\n",
       "  0.38328266,\n",
       "  0.39368445,\n",
       "  0.35399026,\n",
       "  0.43895912,\n",
       "  0.35329467,\n",
       "  0.29488674,\n",
       "  0.48426917,\n",
       "  0.3650893,\n",
       "  0.34861624,\n",
       "  0.28787968,\n",
       "  0.39294523,\n",
       "  0.35681474,\n",
       "  0.41987896,\n",
       "  0.3131996,\n",
       "  0.41554248,\n",
       "  0.43709815,\n",
       "  0.35918623,\n",
       "  0.35903555,\n",
       "  0.5919688,\n",
       "  0.37495583,\n",
       "  0.30812442,\n",
       "  0.607484,\n",
       "  0.3989334,\n",
       "  0.2927906,\n",
       "  0.40020463,\n",
       "  0.44403195,\n",
       "  0.34255692,\n",
       "  0.30596676,\n",
       "  0.30158472,\n",
       "  0.3569228,\n",
       "  0.5556954,\n",
       "  0.2508006,\n",
       "  0.21948761,\n",
       "  0.46717733,\n",
       "  0.4483294,\n",
       "  0.50417846,\n",
       "  0.3224249,\n",
       "  0.38684493,\n",
       "  0.32388604,\n",
       "  0.34979504,\n",
       "  0.35965216,\n",
       "  0.31555462,\n",
       "  0.40656096,\n",
       "  0.3454162,\n",
       "  0.35951728,\n",
       "  0.39702672,\n",
       "  0.3542971,\n",
       "  0.4386685,\n",
       "  0.3881187,\n",
       "  0.2835514,\n",
       "  0.34145874,\n",
       "  0.35196212,\n",
       "  0.24635811,\n",
       "  0.26899272,\n",
       "  0.377248,\n",
       "  0.57105017,\n",
       "  0.3271253,\n",
       "  0.4104663,\n",
       "  0.31324914,\n",
       "  0.30882037,\n",
       "  0.58474237,\n",
       "  0.36668333,\n",
       "  0.4216623,\n",
       "  0.49284625,\n",
       "  0.45632586,\n",
       "  0.445485,\n",
       "  0.4129691,\n",
       "  0.4701079,\n",
       "  0.37502164,\n",
       "  0.3467114,\n",
       "  0.42051476,\n",
       "  0.38050342,\n",
       "  0.30511993,\n",
       "  0.57893705,\n",
       "  0.28843942,\n",
       "  0.32781422,\n",
       "  0.25332975,\n",
       "  0.4776625,\n",
       "  0.30555642,\n",
       "  0.29712656,\n",
       "  0.31888792,\n",
       "  0.48589325,\n",
       "  0.4124924,\n",
       "  0.4255623,\n",
       "  0.36993366,\n",
       "  0.3740618,\n",
       "  0.38962403,\n",
       "  0.30925012,\n",
       "  0.47894844,\n",
       "  0.46619046,\n",
       "  0.3681618,\n",
       "  0.33857837,\n",
       "  0.4189089,\n",
       "  0.33182117,\n",
       "  0.33917633,\n",
       "  0.34984076,\n",
       "  0.33831227,\n",
       "  0.3729003,\n",
       "  0.31783134,\n",
       "  0.43579257,\n",
       "  0.43171105,\n",
       "  0.3969326,\n",
       "  0.2652968,\n",
       "  0.304958,\n",
       "  0.41312853,\n",
       "  0.26852778,\n",
       "  0.3029062,\n",
       "  0.29472214,\n",
       "  0.31277192,\n",
       "  0.35075793,\n",
       "  0.2987849,\n",
       "  0.425418,\n",
       "  0.3937989,\n",
       "  0.4623094,\n",
       "  0.26481074,\n",
       "  0.35920462,\n",
       "  0.3203466,\n",
       "  0.49086618,\n",
       "  0.5058651,\n",
       "  0.27669513,\n",
       "  0.32675815,\n",
       "  0.2885541,\n",
       "  0.40831876,\n",
       "  0.46677893,\n",
       "  0.5360196,\n",
       "  0.5611099,\n",
       "  0.37169865,\n",
       "  0.32565418,\n",
       "  0.49333206,\n",
       "  0.5121912,\n",
       "  0.26440403,\n",
       "  0.2940391,\n",
       "  0.3058501,\n",
       "  0.2976213,\n",
       "  0.24057838,\n",
       "  0.35710463,\n",
       "  0.45451516,\n",
       "  0.49049586,\n",
       "  0.49351323,\n",
       "  0.34533373,\n",
       "  0.5181758,\n",
       "  0.3534354,\n",
       "  0.40323314,\n",
       "  0.27498585,\n",
       "  0.34209433,\n",
       "  0.46137506,\n",
       "  0.48478943,\n",
       "  0.46277463,\n",
       "  0.30751655,\n",
       "  0.3014865,\n",
       "  0.35391593,\n",
       "  0.39323628,\n",
       "  0.45772052,\n",
       "  0.576291,\n",
       "  0.4823063,\n",
       "  0.30565506,\n",
       "  0.33763927,\n",
       "  0.528005,\n",
       "  0.33624274,\n",
       "  0.31016338,\n",
       "  0.37568858,\n",
       "  0.33395475,\n",
       "  0.32798922,\n",
       "  0.40920722,\n",
       "  0.42883688,\n",
       "  0.4147225,\n",
       "  0.3632689,\n",
       "  0.3388161,\n",
       "  0.38419423,\n",
       "  0.46170345,\n",
       "  0.34556153,\n",
       "  0.37163988,\n",
       "  0.34694588,\n",
       "  0.3835597,\n",
       "  0.32662365,\n",
       "  0.332216,\n",
       "  0.24855286,\n",
       "  0.33341584,\n",
       "  0.31695706,\n",
       "  0.3244292,\n",
       "  0.4401312,\n",
       "  0.31114995,\n",
       "  0.3767932,\n",
       "  0.48302186,\n",
       "  0.47525042,\n",
       "  0.4370502,\n",
       "  0.31775585,\n",
       "  0.36822262,\n",
       "  0.4026085,\n",
       "  0.35819092,\n",
       "  0.29435048,\n",
       "  0.36776862,\n",
       "  0.4753304,\n",
       "  0.36472014,\n",
       "  0.353274,\n",
       "  0.24013382,\n",
       "  0.40670896,\n",
       "  0.38357514,\n",
       "  0.44723225,\n",
       "  0.39595604,\n",
       "  0.47551924,\n",
       "  0.5053837,\n",
       "  0.4560753,\n",
       "  0.35618454,\n",
       "  0.4094695,\n",
       "  0.4803031,\n",
       "  0.2994393,\n",
       "  0.38506642,\n",
       "  0.3844924,\n",
       "  0.6743851,\n",
       "  0.8417259,\n",
       "  0.27888143,\n",
       "  0.27960315,\n",
       "  0.4495355,\n",
       "  0.3000186,\n",
       "  0.32856238,\n",
       "  0.39405447,\n",
       "  0.43941242,\n",
       "  0.40260744,\n",
       "  0.4267552,\n",
       "  0.40579647,\n",
       "  0.39519882,\n",
       "  0.40339115,\n",
       "  0.4254895,\n",
       "  0.375032,\n",
       "  0.3429888,\n",
       "  0.37081707,\n",
       "  0.29070768,\n",
       "  0.28432575,\n",
       "  0.3730088,\n",
       "  0.31229833,\n",
       "  0.34459168,\n",
       "  0.32682428,\n",
       "  0.44088584,\n",
       "  0.32640532,\n",
       "  0.35916567,\n",
       "  0.3104638,\n",
       "  0.51364,\n",
       "  0.40265375,\n",
       "  0.5097574,\n",
       "  0.29062033,\n",
       "  0.5983263,\n",
       "  0.583158,\n",
       "  0.60265005,\n",
       "  0.30467656,\n",
       "  0.4288702,\n",
       "  0.39292654,\n",
       "  0.3656634,\n",
       "  0.41372555,\n",
       "  0.35501054,\n",
       "  0.3518002,\n",
       "  0.42305517,\n",
       "  0.32140195,\n",
       "  0.39076746,\n",
       "  0.29486674,\n",
       "  0.41969544,\n",
       "  0.26947224,\n",
       "  0.47612485,\n",
       "  0.39493132,\n",
       "  0.37417215,\n",
       "  0.3722751,\n",
       "  0.33439788,\n",
       "  0.3438071,\n",
       "  0.44793934,\n",
       "  0.42500204,\n",
       "  0.42363557,\n",
       "  0.42520452,\n",
       "  0.3521108,\n",
       "  0.48673156,\n",
       "  0.42465603,\n",
       "  0.3841834,\n",
       "  0.32999346,\n",
       "  0.32366943,\n",
       "  0.26060832,\n",
       "  0.36058065,\n",
       "  0.23847573,\n",
       "  0.32919526,\n",
       "  0.3815177,\n",
       "  0.3707201,\n",
       "  0.3148101,\n",
       "  0.27971047,\n",
       "  0.47767895,\n",
       "  0.34852546,\n",
       "  0.37937677,\n",
       "  0.34688526,\n",
       "  0.3247269,\n",
       "  0.3590513,\n",
       "  0.37963527,\n",
       "  0.30041692,\n",
       "  0.33307567,\n",
       "  0.33932492,\n",
       "  0.2787014,\n",
       "  0.331474,\n",
       "  0.36670405,\n",
       "  0.28008747,\n",
       "  0.40169263,\n",
       "  0.347895,\n",
       "  0.2864788,\n",
       "  0.2923531,\n",
       "  0.3406183,\n",
       "  0.2999219,\n",
       "  0.36820236,\n",
       "  0.42691666,\n",
       "  0.43015224,\n",
       "  0.32441175,\n",
       "  0.35671377,\n",
       "  0.32692248,\n",
       "  0.4023648,\n",
       "  0.29004613,\n",
       "  0.33539504,\n",
       "  0.33843738,\n",
       "  0.34844196,\n",
       "  0.38177907,\n",
       "  0.4019287,\n",
       "  0.3435943,\n",
       "  0.42555058,\n",
       "  0.32896316,\n",
       "  0.35794693,\n",
       "  0.3245256,\n",
       "  0.36668938,\n",
       "  0.35368443,\n",
       "  0.4435967,\n",
       "  0.35976166,\n",
       "  0.3363558,\n",
       "  0.33372682,\n",
       "  0.3662659,\n",
       "  0.360331,\n",
       "  0.3742556,\n",
       "  0.37341335,\n",
       "  0.43818387,\n",
       "  0.51787686,\n",
       "  0.47861943,\n",
       "  0.44924924,\n",
       "  0.32483652,\n",
       "  0.30558136,\n",
       "  0.34821945,\n",
       "  0.3404442,\n",
       "  0.30934292,\n",
       "  0.36985478,\n",
       "  0.2978996,\n",
       "  0.3649831,\n",
       "  0.40289837,\n",
       "  0.32461026,\n",
       "  0.33435118,\n",
       "  0.30652684,\n",
       "  0.35207683,\n",
       "  0.35901195,\n",
       "  0.3607096,\n",
       "  0.32822964,\n",
       "  0.34913033,\n",
       "  0.25849247,\n",
       "  0.34791452,\n",
       "  0.40799525,\n",
       "  0.29610404,\n",
       "  0.36227587,\n",
       "  0.31212878,\n",
       "  0.25446022,\n",
       "  0.39414784,\n",
       "  0.3382891,\n",
       "  0.30046278,\n",
       "  0.31419563,\n",
       "  0.3131551,\n",
       "  0.22376348,\n",
       "  0.3502481,\n",
       "  0.3439225,\n",
       "  0.46648043,\n",
       "  0.3409995,\n",
       "  0.29472753,\n",
       "  0.4118693,\n",
       "  0.34630513,\n",
       "  0.42053536,\n",
       "  0.2856402,\n",
       "  0.31495175,\n",
       "  0.354806,\n",
       "  0.43027744,\n",
       "  0.43031237,\n",
       "  0.27597928,\n",
       "  0.2640723,\n",
       "  0.3298033,\n",
       "  0.24127492,\n",
       "  0.2811549,\n",
       "  0.35218206,\n",
       "  0.4202007,\n",
       "  0.34733546,\n",
       "  0.38017404,\n",
       "  0.31960177,\n",
       "  0.3694883,\n",
       "  0.38327783,\n",
       "  0.3056481,\n",
       "  0.3105453,\n",
       "  0.2877031,\n",
       "  0.32881102,\n",
       "  0.32746822,\n",
       "  0.3010286,\n",
       "  0.36325026,\n",
       "  0.28591657,\n",
       "  0.51664203,\n",
       "  0.35529563,\n",
       "  0.31795788,\n",
       "  0.4974862,\n",
       "  0.4946598,\n",
       "  0.6069566,\n",
       "  0.4778593,\n",
       "  0.41877502,\n",
       "  0.38955012,\n",
       "  0.3634157,\n",
       "  0.48002037,\n",
       "  0.33545485,\n",
       "  0.34180528,\n",
       "  0.3315196,\n",
       "  0.34526652,\n",
       "  0.41883284,\n",
       "  0.38128066,\n",
       "  0.29506606,\n",
       "  0.37060326,\n",
       "  0.41073328,\n",
       "  0.41940522,\n",
       "  0.41131014,\n",
       "  0.35151237,\n",
       "  0.5331725,\n",
       "  0.35635126,\n",
       "  0.298338,\n",
       "  0.4005124,\n",
       "  0.2469007,\n",
       "  0.56660736,\n",
       "  0.41856343,\n",
       "  0.2507339,\n",
       "  0.454072,\n",
       "  0.29279086,\n",
       "  0.31883967,\n",
       "  0.52310395,\n",
       "  0.2836937,\n",
       "  0.5319642,\n",
       "  0.35442695,\n",
       "  0.3847239,\n",
       "  0.35777617,\n",
       "  0.31891152,\n",
       "  0.37721902,\n",
       "  0.35894674,\n",
       "  0.31730834,\n",
       "  0.36495537,\n",
       "  0.34674978,\n",
       "  0.35512722,\n",
       "  0.3507497,\n",
       "  0.38424915,\n",
       "  0.43138143,\n",
       "  0.32312903,\n",
       "  0.524423,\n",
       "  0.43110347,\n",
       "  0.30288357,\n",
       "  0.38096228,\n",
       "  0.3448186,\n",
       "  0.37272662,\n",
       "  0.36301565,\n",
       "  0.41688818,\n",
       "  0.46767843,\n",
       "  0.44559938,\n",
       "  0.34823883,\n",
       "  0.4134968,\n",
       "  0.3929658,\n",
       "  0.3467214,\n",
       "  0.39973179,\n",
       "  0.37113106,\n",
       "  0.38532588,\n",
       "  0.30827615,\n",
       "  0.40088138,\n",
       "  0.41577244,\n",
       "  0.4038099,\n",
       "  0.3851684,\n",
       "  ...],\n",
       " 'pred_soft': [5742,\n",
       "  1422,\n",
       "  5934,\n",
       "  3754,\n",
       "  5811,\n",
       "  8757,\n",
       "  3576,\n",
       "  2471,\n",
       "  44,\n",
       "  1981,\n",
       "  5708,\n",
       "  7155,\n",
       "  5994,\n",
       "  8887,\n",
       "  980,\n",
       "  8921,\n",
       "  5790,\n",
       "  2916,\n",
       "  86,\n",
       "  5705,\n",
       "  100,\n",
       "  7426,\n",
       "  890,\n",
       "  7895,\n",
       "  8910,\n",
       "  6603,\n",
       "  2164,\n",
       "  148,\n",
       "  1471,\n",
       "  8599,\n",
       "  5583,\n",
       "  5359,\n",
       "  3583,\n",
       "  2715,\n",
       "  179,\n",
       "  6344,\n",
       "  6938,\n",
       "  4814,\n",
       "  8225,\n",
       "  229,\n",
       "  1927,\n",
       "  2563,\n",
       "  233,\n",
       "  5615,\n",
       "  4728,\n",
       "  2597,\n",
       "  4739,\n",
       "  8484,\n",
       "  2197,\n",
       "  4728,\n",
       "  2543,\n",
       "  2531,\n",
       "  2878,\n",
       "  9707,\n",
       "  4060,\n",
       "  4082,\n",
       "  8629,\n",
       "  9796,\n",
       "  4908,\n",
       "  5820,\n",
       "  9390,\n",
       "  9641,\n",
       "  2134,\n",
       "  6946,\n",
       "  2222,\n",
       "  2222,\n",
       "  3931,\n",
       "  3805,\n",
       "  4998,\n",
       "  783,\n",
       "  4954,\n",
       "  1241,\n",
       "  203,\n",
       "  2916,\n",
       "  9385,\n",
       "  6984,\n",
       "  3274,\n",
       "  2793,\n",
       "  7970,\n",
       "  5708,\n",
       "  9789,\n",
       "  8644,\n",
       "  7431,\n",
       "  6084,\n",
       "  5492,\n",
       "  3420,\n",
       "  2091,\n",
       "  7880,\n",
       "  7949,\n",
       "  9782,\n",
       "  2663,\n",
       "  2018,\n",
       "  1904,\n",
       "  5073,\n",
       "  3290,\n",
       "  5981,\n",
       "  2484,\n",
       "  742,\n",
       "  4638,\n",
       "  9311,\n",
       "  1933,\n",
       "  554,\n",
       "  8856,\n",
       "  8369,\n",
       "  1270,\n",
       "  4865,\n",
       "  576,\n",
       "  1807,\n",
       "  2664,\n",
       "  2664,\n",
       "  2135,\n",
       "  4246,\n",
       "  606,\n",
       "  4156,\n",
       "  4438,\n",
       "  8001,\n",
       "  5327,\n",
       "  2544,\n",
       "  632,\n",
       "  4836,\n",
       "  634,\n",
       "  5368,\n",
       "  2059,\n",
       "  8384,\n",
       "  2830,\n",
       "  8783,\n",
       "  2731,\n",
       "  655,\n",
       "  4990,\n",
       "  8995,\n",
       "  6980,\n",
       "  8731,\n",
       "  5675,\n",
       "  6050,\n",
       "  5442,\n",
       "  6043,\n",
       "  3877,\n",
       "  5573,\n",
       "  3073,\n",
       "  2197,\n",
       "  564,\n",
       "  7573,\n",
       "  7725,\n",
       "  4807,\n",
       "  743,\n",
       "  2683,\n",
       "  5623,\n",
       "  4433,\n",
       "  3661,\n",
       "  4958,\n",
       "  803,\n",
       "  4297,\n",
       "  819,\n",
       "  2770,\n",
       "  2728,\n",
       "  828,\n",
       "  2829,\n",
       "  4865,\n",
       "  5249,\n",
       "  3497,\n",
       "  7530,\n",
       "  844,\n",
       "  854,\n",
       "  2070,\n",
       "  858,\n",
       "  7984,\n",
       "  829,\n",
       "  2075,\n",
       "  2590,\n",
       "  2316,\n",
       "  5934,\n",
       "  5857,\n",
       "  907,\n",
       "  6636,\n",
       "  731,\n",
       "  890,\n",
       "  5939,\n",
       "  7222,\n",
       "  2075,\n",
       "  6856,\n",
       "  2150,\n",
       "  2945,\n",
       "  9935,\n",
       "  2316,\n",
       "  2999,\n",
       "  5587,\n",
       "  4090,\n",
       "  2036,\n",
       "  7540,\n",
       "  4090,\n",
       "  7276,\n",
       "  3806,\n",
       "  4959,\n",
       "  2075,\n",
       "  4819,\n",
       "  1033,\n",
       "  6716,\n",
       "  7331,\n",
       "  1049,\n",
       "  8698,\n",
       "  923,\n",
       "  5692,\n",
       "  2368,\n",
       "  8921,\n",
       "  9927,\n",
       "  2714,\n",
       "  405,\n",
       "  5358,\n",
       "  5281,\n",
       "  6413,\n",
       "  5577,\n",
       "  7717,\n",
       "  352,\n",
       "  6933,\n",
       "  2148,\n",
       "  2428,\n",
       "  2518,\n",
       "  5962,\n",
       "  2856,\n",
       "  1187,\n",
       "  2191,\n",
       "  9716,\n",
       "  2916,\n",
       "  4459,\n",
       "  2911,\n",
       "  4728,\n",
       "  1218,\n",
       "  2271,\n",
       "  5747,\n",
       "  6630,\n",
       "  5911,\n",
       "  6302,\n",
       "  2018,\n",
       "  5083,\n",
       "  7533,\n",
       "  5411,\n",
       "  8588,\n",
       "  5757,\n",
       "  5359,\n",
       "  8757,\n",
       "  9504,\n",
       "  2279,\n",
       "  6799,\n",
       "  322,\n",
       "  6031,\n",
       "  6361,\n",
       "  1700,\n",
       "  2932,\n",
       "  2325,\n",
       "  106,\n",
       "  1861,\n",
       "  1861,\n",
       "  7540,\n",
       "  1360,\n",
       "  9110,\n",
       "  1357,\n",
       "  8185,\n",
       "  2333,\n",
       "  1390,\n",
       "  2316,\n",
       "  667,\n",
       "  2124,\n",
       "  6366,\n",
       "  5817,\n",
       "  9565,\n",
       "  1408,\n",
       "  6839,\n",
       "  4976,\n",
       "  4976,\n",
       "  5796,\n",
       "  7808,\n",
       "  5883,\n",
       "  5734,\n",
       "  5156,\n",
       "  1020,\n",
       "  7164,\n",
       "  980,\n",
       "  7141,\n",
       "  2863,\n",
       "  2877,\n",
       "  6479,\n",
       "  7880,\n",
       "  873,\n",
       "  3820,\n",
       "  2585,\n",
       "  1073,\n",
       "  2696,\n",
       "  7392,\n",
       "  2059,\n",
       "  1516,\n",
       "  9838,\n",
       "  6356,\n",
       "  8384,\n",
       "  7989,\n",
       "  4406,\n",
       "  9711,\n",
       "  9222,\n",
       "  8469,\n",
       "  986,\n",
       "  2507,\n",
       "  1595,\n",
       "  967,\n",
       "  4821,\n",
       "  967,\n",
       "  4958,\n",
       "  9035,\n",
       "  1847,\n",
       "  7530,\n",
       "  1655,\n",
       "  2666,\n",
       "  1234,\n",
       "  242,\n",
       "  242,\n",
       "  8671,\n",
       "  5739,\n",
       "  4814,\n",
       "  5739,\n",
       "  2153,\n",
       "  7956,\n",
       "  5356,\n",
       "  5620,\n",
       "  3931,\n",
       "  3497,\n",
       "  1725,\n",
       "  1690,\n",
       "  8920,\n",
       "  3485,\n",
       "  8354,\n",
       "  5719,\n",
       "  1461,\n",
       "  4964,\n",
       "  3606,\n",
       "  2593,\n",
       "  8790,\n",
       "  8111,\n",
       "  5536,\n",
       "  8835,\n",
       "  1844,\n",
       "  1700,\n",
       "  2823,\n",
       "  2066,\n",
       "  3661,\n",
       "  1887,\n",
       "  1895,\n",
       "  7097,\n",
       "  2020,\n",
       "  8350,\n",
       "  8935,\n",
       "  5580,\n",
       "  1941,\n",
       "  4405,\n",
       "  1961,\n",
       "  2873,\n",
       "  9875,\n",
       "  7863,\n",
       "  7863,\n",
       "  184,\n",
       "  2655,\n",
       "  2639,\n",
       "  1664,\n",
       "  4952,\n",
       "  2070,\n",
       "  1640,\n",
       "  5193,\n",
       "  2690,\n",
       "  9770,\n",
       "  5585,\n",
       "  1738,\n",
       "  2316,\n",
       "  2633,\n",
       "  2176,\n",
       "  2388,\n",
       "  2162,\n",
       "  2770,\n",
       "  5767,\n",
       "  4829,\n",
       "  7613,\n",
       "  184,\n",
       "  6674,\n",
       "  3461,\n",
       "  2240,\n",
       "  8834,\n",
       "  2698,\n",
       "  7981,\n",
       "  7387,\n",
       "  2277,\n",
       "  352,\n",
       "  8805,\n",
       "  6203,\n",
       "  2426,\n",
       "  4589,\n",
       "  2315,\n",
       "  3854,\n",
       "  6838,\n",
       "  2324,\n",
       "  901,\n",
       "  6895,\n",
       "  2343,\n",
       "  2347,\n",
       "  1980,\n",
       "  3796,\n",
       "  2112,\n",
       "  2375,\n",
       "  5800,\n",
       "  3640,\n",
       "  8413,\n",
       "  1904,\n",
       "  5497,\n",
       "  5979,\n",
       "  2444,\n",
       "  2829,\n",
       "  5631,\n",
       "  661,\n",
       "  7901,\n",
       "  2638,\n",
       "  2547,\n",
       "  2370,\n",
       "  5382,\n",
       "  5083,\n",
       "  2565,\n",
       "  4779,\n",
       "  849,\n",
       "  7880,\n",
       "  2734,\n",
       "  3750,\n",
       "  2585,\n",
       "  7809,\n",
       "  5251,\n",
       "  2556,\n",
       "  2623,\n",
       "  8507,\n",
       "  4528,\n",
       "  2630,\n",
       "  6421,\n",
       "  2642,\n",
       "  2919,\n",
       "  2677,\n",
       "  6938,\n",
       "  2067,\n",
       "  5640,\n",
       "  155,\n",
       "  2728,\n",
       "  7441,\n",
       "  2748,\n",
       "  631,\n",
       "  926,\n",
       "  8984,\n",
       "  3931,\n",
       "  8910,\n",
       "  8834,\n",
       "  7703,\n",
       "  282,\n",
       "  2814,\n",
       "  2024,\n",
       "  853,\n",
       "  2831,\n",
       "  2852,\n",
       "  2874,\n",
       "  6584,\n",
       "  3497,\n",
       "  5005,\n",
       "  9949,\n",
       "  2916,\n",
       "  8496,\n",
       "  9085,\n",
       "  2801,\n",
       "  4565,\n",
       "  2962,\n",
       "  2971,\n",
       "  7273,\n",
       "  2977,\n",
       "  2996,\n",
       "  562,\n",
       "  8746,\n",
       "  918,\n",
       "  918,\n",
       "  918,\n",
       "  2319,\n",
       "  4930,\n",
       "  4841,\n",
       "  2176,\n",
       "  3819,\n",
       "  2170,\n",
       "  5141,\n",
       "  930,\n",
       "  1445,\n",
       "  2018,\n",
       "  9955,\n",
       "  8234,\n",
       "  5636,\n",
       "  1346,\n",
       "  5939,\n",
       "  7480,\n",
       "  1073,\n",
       "  5810,\n",
       "  4244,\n",
       "  3039,\n",
       "  9459,\n",
       "  7222,\n",
       "  9898,\n",
       "  7420,\n",
       "  4007,\n",
       "  3138,\n",
       "  3931,\n",
       "  3152,\n",
       "  1076,\n",
       "  9424,\n",
       "  4670,\n",
       "  2091,\n",
       "  3205,\n",
       "  2315,\n",
       "  5570,\n",
       "  2655,\n",
       "  2997,\n",
       "  2497,\n",
       "  3236,\n",
       "  4000,\n",
       "  203,\n",
       "  7803,\n",
       "  5052,\n",
       "  1754,\n",
       "  6367,\n",
       "  2991,\n",
       "  5383,\n",
       "  5738,\n",
       "  8904,\n",
       "  7296,\n",
       "  9315,\n",
       "  5593,\n",
       "  4401,\n",
       "  3331,\n",
       "  1882,\n",
       "  9653,\n",
       "  5515,\n",
       "  5118,\n",
       "  5800,\n",
       "  2997,\n",
       "  2319,\n",
       "  5542,\n",
       "  986,\n",
       "  6785,\n",
       "  1898,\n",
       "  5368,\n",
       "  2103,\n",
       "  2063,\n",
       "  3931,\n",
       "  8034,\n",
       "  2309,\n",
       "  4425,\n",
       "  742,\n",
       "  9992,\n",
       "  2344,\n",
       "  5820,\n",
       "  5676,\n",
       "  3183,\n",
       "  1866,\n",
       "  7573,\n",
       "  4867,\n",
       "  7863,\n",
       "  9835,\n",
       "  606,\n",
       "  7542,\n",
       "  4029,\n",
       "  8550,\n",
       "  7936,\n",
       "  6588,\n",
       "  6742,\n",
       "  6159,\n",
       "  2587,\n",
       "  3535,\n",
       "  4743,\n",
       "  3555,\n",
       "  4627,\n",
       "  5448,\n",
       "  9474,\n",
       "  6726,\n",
       "  8851,\n",
       "  859,\n",
       "  8844,\n",
       "  8844,\n",
       "  3620,\n",
       "  1933,\n",
       "  1963,\n",
       "  3631,\n",
       "  6092,\n",
       "  867,\n",
       "  5368,\n",
       "  3659,\n",
       "  3660,\n",
       "  9219,\n",
       "  6932,\n",
       "  2501,\n",
       "  2923,\n",
       "  2148,\n",
       "  5515,\n",
       "  5561,\n",
       "  2354,\n",
       "  2963,\n",
       "  9769,\n",
       "  2316,\n",
       "  3506,\n",
       "  7809,\n",
       "  3755,\n",
       "  1476,\n",
       "  9496,\n",
       "  2581,\n",
       "  5356,\n",
       "  42,\n",
       "  5493,\n",
       "  7681,\n",
       "  4931,\n",
       "  742,\n",
       "  6788,\n",
       "  6788,\n",
       "  4207,\n",
       "  3820,\n",
       "  5556,\n",
       "  3828,\n",
       "  9458,\n",
       "  6241,\n",
       "  5999,\n",
       "  3870,\n",
       "  1610,\n",
       "  5480,\n",
       "  8798,\n",
       "  5848,\n",
       "  4057,\n",
       "  9663,\n",
       "  5923,\n",
       "  691,\n",
       "  3976,\n",
       "  2990,\n",
       "  3979,\n",
       "  2763,\n",
       "  8525,\n",
       "  6729,\n",
       "  894,\n",
       "  607,\n",
       "  3952,\n",
       "  5417,\n",
       "  2481,\n",
       "  853,\n",
       "  8572,\n",
       "  7644,\n",
       "  2881,\n",
       "  2476,\n",
       "  2867,\n",
       "  5631,\n",
       "  909,\n",
       "  9138,\n",
       "  2373,\n",
       "  5191,\n",
       "  2395,\n",
       "  5556,\n",
       "  5820,\n",
       "  5770,\n",
       "  6648,\n",
       "  8709,\n",
       "  2858,\n",
       "  8517,\n",
       "  2496,\n",
       "  2316,\n",
       "  912,\n",
       "  5004,\n",
       "  2829,\n",
       "  6092,\n",
       "  8801,\n",
       "  8885,\n",
       "  9678,\n",
       "  1721,\n",
       "  4866,\n",
       "  1697,\n",
       "  7797,\n",
       "  4829,\n",
       "  1444,\n",
       "  6624,\n",
       "  2829,\n",
       "  2484,\n",
       "  8828,\n",
       "  4207,\n",
       "  2316,\n",
       "  8985,\n",
       "  8693,\n",
       "  6127,\n",
       "  4196,\n",
       "  2886,\n",
       "  2691,\n",
       "  5525,\n",
       "  2923,\n",
       "  6194,\n",
       "  5629,\n",
       "  2197,\n",
       "  2852,\n",
       "  9317,\n",
       "  116,\n",
       "  5972,\n",
       "  5972,\n",
       "  4882,\n",
       "  8605,\n",
       "  1097,\n",
       "  2593,\n",
       "  5686,\n",
       "  4789,\n",
       "  5560,\n",
       "  7608,\n",
       "  7880,\n",
       "  912,\n",
       "  2754,\n",
       "  6419,\n",
       "  2555,\n",
       "  7887,\n",
       "  67,\n",
       "  5542,\n",
       "  3986,\n",
       "  5979,\n",
       "  2316,\n",
       "  3820,\n",
       "  2661,\n",
       "  1922,\n",
       "  7638,\n",
       "  4292,\n",
       "  962,\n",
       "  4501,\n",
       "  6470,\n",
       "  4182,\n",
       "  1445,\n",
       "  8834,\n",
       "  2829,\n",
       "  2315,\n",
       "  9251,\n",
       "  5492,\n",
       "  9583,\n",
       "  2609,\n",
       "  795,\n",
       "  951,\n",
       "  8656,\n",
       "  9921,\n",
       "  4433,\n",
       "  2410,\n",
       "  2410,\n",
       "  9916,\n",
       "  5417,\n",
       "  2159,\n",
       "  2997,\n",
       "  4658,\n",
       "  4544,\n",
       "  5480,\n",
       "  3931,\n",
       "  2896,\n",
       "  5812,\n",
       "  7998,\n",
       "  5812,\n",
       "  2583,\n",
       "  696,\n",
       "  5674,\n",
       "  5753,\n",
       "  940,\n",
       "  4752,\n",
       "  9360,\n",
       "  8569,\n",
       "  1593,\n",
       "  5817,\n",
       "  4784,\n",
       "  5433,\n",
       "  2084,\n",
       "  7670,\n",
       "  6740,\n",
       "  4992,\n",
       "  6321,\n",
       "  5277,\n",
       "  9961,\n",
       "  7965,\n",
       "  5382,\n",
       "  13,\n",
       "  13,\n",
       "  2998,\n",
       "  2344,\n",
       "  2531,\n",
       "  6788,\n",
       "  8977,\n",
       "  8599,\n",
       "  2445,\n",
       "  1308,\n",
       "  4925,\n",
       "  2713,\n",
       "  5585,\n",
       "  6360,\n",
       "  2018,\n",
       "  8569,\n",
       "  4988,\n",
       "  5937,\n",
       "  2809,\n",
       "  5018,\n",
       "  820,\n",
       "  2091,\n",
       "  2091,\n",
       "  8797,\n",
       "  7113,\n",
       "  2081,\n",
       "  5515,\n",
       "  2091,\n",
       "  2316,\n",
       "  2937,\n",
       "  2764,\n",
       "  7639,\n",
       "  4103,\n",
       "  4562,\n",
       "  2834,\n",
       "  302,\n",
       "  8821,\n",
       "  2484,\n",
       "  5745,\n",
       "  9328,\n",
       "  8834,\n",
       "  5163,\n",
       "  7019,\n",
       "  6655,\n",
       "  5556,\n",
       "  5183,\n",
       "  8959,\n",
       "  2091,\n",
       "  5415,\n",
       "  9259,\n",
       "  8502,\n",
       "  5211,\n",
       "  4207,\n",
       "  5216,\n",
       "  9371,\n",
       "  5235,\n",
       "  5235,\n",
       "  4711,\n",
       "  2754,\n",
       "  999,\n",
       "  9949,\n",
       "  7998,\n",
       "  4977,\n",
       "  5278,\n",
       "  5289,\n",
       "  8726,\n",
       "  6768,\n",
       "  4924,\n",
       "  8664,\n",
       "  9949,\n",
       "  2091,\n",
       "  5368,\n",
       "  420,\n",
       "  2091,\n",
       "  2091,\n",
       "  5396,\n",
       "  9105,\n",
       "  1141,\n",
       "  2664,\n",
       "  5434,\n",
       "  5438,\n",
       "  8855,\n",
       "  7792,\n",
       "  2289,\n",
       "  6428,\n",
       "  5360,\n",
       "  5031,\n",
       "  2532,\n",
       "  5262,\n",
       "  5795,\n",
       "  2916,\n",
       "  5699,\n",
       "  2697,\n",
       "  4589,\n",
       "  2232,\n",
       "  3683,\n",
       "  3931,\n",
       "  8572,\n",
       "  7880,\n",
       "  5999,\n",
       "  5567,\n",
       "  5567,\n",
       "  2106,\n",
       "  5601,\n",
       "  5597,\n",
       "  5604,\n",
       "  3313,\n",
       "  4912,\n",
       "  5647,\n",
       "  2091,\n",
       "  5658,\n",
       "  4533,\n",
       "  5585,\n",
       "  993,\n",
       "  8234,\n",
       "  5448,\n",
       "  5684,\n",
       "  5684,\n",
       "  9949,\n",
       "  9487,\n",
       "  8034,\n",
       "  4944,\n",
       "  2597,\n",
       "  5136,\n",
       "  4938,\n",
       "  5483,\n",
       "  4726,\n",
       "  9624,\n",
       "  5368,\n",
       "  352,\n",
       "  352,\n",
       "  8369,\n",
       "  7863,\n",
       "  5811,\n",
       "  2930,\n",
       "  8818,\n",
       "  2597,\n",
       "  1752,\n",
       "  7863,\n",
       "  8666,\n",
       "  5871,\n",
       "  8457,\n",
       "  242,\n",
       "  3871,\n",
       "  8484,\n",
       "  2829,\n",
       "  999,\n",
       "  3697,\n",
       "  4976,\n",
       "  4729,\n",
       "  8768,\n",
       "  7833,\n",
       "  4224,\n",
       "  3854,\n",
       "  7962,\n",
       "  5986,\n",
       "  4670,\n",
       "  5999,\n",
       "  6043,\n",
       "  3495,\n",
       "  6043,\n",
       "  2770,\n",
       "  7670,\n",
       "  2429,\n",
       "  896,\n",
       "  2380,\n",
       "  6765,\n",
       "  4992,\n",
       "  6080,\n",
       "  890,\n",
       "  5418,\n",
       "  5358,\n",
       "  9571,\n",
       "  9791,\n",
       "  980,\n",
       "  9013,\n",
       "  5709,\n",
       "  5338,\n",
       "  2890,\n",
       "  7693,\n",
       "  6131,\n",
       "  2360,\n",
       "  6983,\n",
       "  5098,\n",
       "  1336,\n",
       "  1904,\n",
       "  9196,\n",
       "  6731,\n",
       "  5833,\n",
       "  2626,\n",
       "  926,\n",
       "  9805,\n",
       "  9497,\n",
       "  6864,\n",
       "  4957,\n",
       "  2061,\n",
       "  5820,\n",
       "  1895,\n",
       "  2998,\n",
       "  7513,\n",
       "  79,\n",
       "  3922,\n",
       "  5820,\n",
       "  8904,\n",
       "  8898,\n",
       "  940,\n",
       "  5477,\n",
       "  2829,\n",
       "  6275,\n",
       "  7545,\n",
       "  9403,\n",
       "  9763,\n",
       "  6555,\n",
       "  1498,\n",
       "  2997,\n",
       "  5734,\n",
       "  4654,\n",
       "  8026,\n",
       "  5456,\n",
       "  5541,\n",
       "  8214,\n",
       "  8087,\n",
       "  6338,\n",
       "  5297,\n",
       "  9229,\n",
       "  2868,\n",
       "  2868,\n",
       "  ...],\n",
       " 'bad_soft': [True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  ...],\n",
       " 'soft_val': [0.27831315994262695,\n",
       "  0.332141250371933,\n",
       "  0.2668454647064209,\n",
       "  0.315265417098999,\n",
       "  0.23430435359477997,\n",
       "  0.24133679270744324,\n",
       "  0.22918349504470825,\n",
       "  0.22246502339839935,\n",
       "  0.2767032980918884,\n",
       "  0.2668970823287964,\n",
       "  0.31250321865081787,\n",
       "  0.25223731994628906,\n",
       "  0.2938522398471832,\n",
       "  0.31701165437698364,\n",
       "  0.2553049325942993,\n",
       "  0.26961711049079895,\n",
       "  0.27480077743530273,\n",
       "  0.30548468232154846,\n",
       "  0.2933461666107178,\n",
       "  0.2758784592151642,\n",
       "  0.3640541136264801,\n",
       "  0.2585009038448334,\n",
       "  0.2628743648529053,\n",
       "  0.21683335304260254,\n",
       "  0.23778976500034332,\n",
       "  0.2731461226940155,\n",
       "  0.26893532276153564,\n",
       "  0.40389150381088257,\n",
       "  0.2735306918621063,\n",
       "  0.3239707052707672,\n",
       "  0.2845207452774048,\n",
       "  0.26146838068962097,\n",
       "  0.33476102352142334,\n",
       "  0.3089408278465271,\n",
       "  0.3400764763355255,\n",
       "  0.22545242309570312,\n",
       "  0.27689164876937866,\n",
       "  0.2866087555885315,\n",
       "  0.31908389925956726,\n",
       "  0.4209824800491333,\n",
       "  0.26989424228668213,\n",
       "  0.2718641459941864,\n",
       "  0.3268992602825165,\n",
       "  0.2380364090204239,\n",
       "  0.2552562952041626,\n",
       "  0.352816104888916,\n",
       "  0.25702232122421265,\n",
       "  0.2812115550041199,\n",
       "  0.2669782042503357,\n",
       "  0.27246129512786865,\n",
       "  0.2919471859931946,\n",
       "  0.36429327726364136,\n",
       "  0.2614280581474304,\n",
       "  0.3206617534160614,\n",
       "  0.24606864154338837,\n",
       "  0.25370240211486816,\n",
       "  0.2540692985057831,\n",
       "  0.23393894731998444,\n",
       "  0.3095668852329254,\n",
       "  0.32269543409347534,\n",
       "  0.28559020161628723,\n",
       "  0.27308282256126404,\n",
       "  0.28988760709762573,\n",
       "  0.33460208773612976,\n",
       "  0.3499893844127655,\n",
       "  0.26361092925071716,\n",
       "  0.24001462757587433,\n",
       "  0.3428209722042084,\n",
       "  0.3008204996585846,\n",
       "  0.30995699763298035,\n",
       "  0.3191962242126465,\n",
       "  0.23060263693332672,\n",
       "  0.25756707787513733,\n",
       "  0.2606159448623657,\n",
       "  0.2854018211364746,\n",
       "  0.29150959849357605,\n",
       "  0.2616107761859894,\n",
       "  0.2523101270198822,\n",
       "  0.21290884912014008,\n",
       "  0.3285391926765442,\n",
       "  0.25161808729171753,\n",
       "  0.22335001826286316,\n",
       "  0.26547351479530334,\n",
       "  0.30911800265312195,\n",
       "  0.2503766715526581,\n",
       "  0.23728878796100616,\n",
       "  0.279217392206192,\n",
       "  0.2376558482646942,\n",
       "  0.26311978697776794,\n",
       "  0.24677011370658875,\n",
       "  0.25366881489753723,\n",
       "  0.31116753816604614,\n",
       "  0.26977458596229553,\n",
       "  0.24773633480072021,\n",
       "  0.2515939772129059,\n",
       "  0.282495379447937,\n",
       "  0.31089192628860474,\n",
       "  0.2721165716648102,\n",
       "  0.27723225951194763,\n",
       "  0.27241072058677673,\n",
       "  0.30875182151794434,\n",
       "  0.319789856672287,\n",
       "  0.3421138823032379,\n",
       "  0.2974783778190613,\n",
       "  0.2815355062484741,\n",
       "  0.2808704972267151,\n",
       "  0.30732348561286926,\n",
       "  0.25443992018699646,\n",
       "  0.3498670160770416,\n",
       "  0.36403515934944153,\n",
       "  0.2712809443473816,\n",
       "  0.3742621839046478,\n",
       "  0.3815360963344574,\n",
       "  0.22285325825214386,\n",
       "  0.23717187345027924,\n",
       "  0.22816143929958344,\n",
       "  0.280635803937912,\n",
       "  0.30589911341667175,\n",
       "  0.285795122385025,\n",
       "  0.24995014071464539,\n",
       "  0.3100288510322571,\n",
       "  0.3393060863018036,\n",
       "  0.30353474617004395,\n",
       "  0.3192029595375061,\n",
       "  0.28494319319725037,\n",
       "  0.22719678282737732,\n",
       "  0.3019482493400574,\n",
       "  0.2746003568172455,\n",
       "  0.30105844140052795,\n",
       "  0.2712458074092865,\n",
       "  0.2536053955554962,\n",
       "  0.3489939272403717,\n",
       "  0.23886598646640778,\n",
       "  0.36026254296302795,\n",
       "  0.2784855365753174,\n",
       "  0.2823196351528168,\n",
       "  0.26347100734710693,\n",
       "  0.2415160834789276,\n",
       "  0.30909639596939087,\n",
       "  0.286236435174942,\n",
       "  0.25366732478141785,\n",
       "  0.2650567889213562,\n",
       "  0.2658126652240753,\n",
       "  0.24777571856975555,\n",
       "  0.27682584524154663,\n",
       "  0.21805840730667114,\n",
       "  0.299039751291275,\n",
       "  0.2871882915496826,\n",
       "  0.3499350845813751,\n",
       "  0.2750532329082489,\n",
       "  0.2809184789657593,\n",
       "  0.2787710726261139,\n",
       "  0.2633574306964874,\n",
       "  0.2872542440891266,\n",
       "  0.2434135228395462,\n",
       "  0.2593238949775696,\n",
       "  0.3021645247936249,\n",
       "  0.2336338609457016,\n",
       "  0.28257423639297485,\n",
       "  0.2630676329135895,\n",
       "  0.21694417297840118,\n",
       "  0.307254433631897,\n",
       "  0.26535096764564514,\n",
       "  0.30822494626045227,\n",
       "  0.29728877544403076,\n",
       "  0.2786587178707123,\n",
       "  0.26733946800231934,\n",
       "  0.28375086188316345,\n",
       "  0.26721930503845215,\n",
       "  0.3020108938217163,\n",
       "  0.3016989231109619,\n",
       "  0.2591303884983063,\n",
       "  0.39115455746650696,\n",
       "  0.24873629212379456,\n",
       "  0.2636268138885498,\n",
       "  0.2608312964439392,\n",
       "  0.2482168972492218,\n",
       "  0.3057127892971039,\n",
       "  0.2378198653459549,\n",
       "  0.26438677310943604,\n",
       "  0.32037436962127686,\n",
       "  0.22833414375782013,\n",
       "  0.31459519267082214,\n",
       "  0.27559348940849304,\n",
       "  0.2921195328235626,\n",
       "  0.3052946925163269,\n",
       "  0.310944527387619,\n",
       "  0.28765514492988586,\n",
       "  0.2648904323577881,\n",
       "  0.2824779450893402,\n",
       "  0.3247670531272888,\n",
       "  0.2429543137550354,\n",
       "  0.297813355922699,\n",
       "  0.296968936920166,\n",
       "  0.26830050349235535,\n",
       "  0.34445348381996155,\n",
       "  0.2479643076658249,\n",
       "  0.2569648027420044,\n",
       "  0.24620278179645538,\n",
       "  0.33605438470840454,\n",
       "  0.27584367990493774,\n",
       "  0.2754243016242981,\n",
       "  0.25398731231689453,\n",
       "  0.28812646865844727,\n",
       "  0.28148847818374634,\n",
       "  0.26860490441322327,\n",
       "  0.2950318455696106,\n",
       "  0.2567620277404785,\n",
       "  0.28304794430732727,\n",
       "  0.27841487526893616,\n",
       "  0.28129714727401733,\n",
       "  0.2601527273654938,\n",
       "  0.23049508035182953,\n",
       "  0.2861148715019226,\n",
       "  0.25843778252601624,\n",
       "  0.2708878219127655,\n",
       "  0.2401231825351715,\n",
       "  0.2890755236148834,\n",
       "  0.2635440230369568,\n",
       "  0.3713580369949341,\n",
       "  0.2548356056213379,\n",
       "  0.24422495067119598,\n",
       "  0.2645756006240845,\n",
       "  0.23446957767009735,\n",
       "  0.2733076512813568,\n",
       "  0.2685224115848541,\n",
       "  0.26663899421691895,\n",
       "  0.21966952085494995,\n",
       "  0.27406448125839233,\n",
       "  0.3365078568458557,\n",
       "  0.22471171617507935,\n",
       "  0.27779412269592285,\n",
       "  0.25183266401290894,\n",
       "  0.2761287987232208,\n",
       "  0.2501692473888397,\n",
       "  0.2669689953327179,\n",
       "  0.3374708592891693,\n",
       "  0.26522982120513916,\n",
       "  0.23644590377807617,\n",
       "  0.22366464138031006,\n",
       "  0.3059658110141754,\n",
       "  0.2653547525405884,\n",
       "  0.24237388372421265,\n",
       "  0.2565813660621643,\n",
       "  0.2997785210609436,\n",
       "  0.2632814049720764,\n",
       "  0.2379039078950882,\n",
       "  0.25863945484161377,\n",
       "  0.283811092376709,\n",
       "  0.254748672246933,\n",
       "  0.30828654766082764,\n",
       "  0.2608727216720581,\n",
       "  0.3396104574203491,\n",
       "  0.3004978597164154,\n",
       "  0.2808225154876709,\n",
       "  0.2561705708503723,\n",
       "  0.24197709560394287,\n",
       "  0.25839993357658386,\n",
       "  0.28624722361564636,\n",
       "  0.3622901737689972,\n",
       "  0.29635030031204224,\n",
       "  0.23101671040058136,\n",
       "  0.20783928036689758,\n",
       "  0.25172725319862366,\n",
       "  0.24502705037593842,\n",
       "  0.35028114914894104,\n",
       "  0.24091418087482452,\n",
       "  0.23896817862987518,\n",
       "  0.2855369448661804,\n",
       "  0.3191908597946167,\n",
       "  0.3049750030040741,\n",
       "  0.26783397793769836,\n",
       "  0.242184579372406,\n",
       "  0.27737557888031006,\n",
       "  0.30853497982025146,\n",
       "  0.3589569926261902,\n",
       "  0.29568785429000854,\n",
       "  0.2687152326107025,\n",
       "  0.2998213768005371,\n",
       "  0.28044459223747253,\n",
       "  0.2835071086883545,\n",
       "  0.3278029263019562,\n",
       "  0.2624388039112091,\n",
       "  0.26247429847717285,\n",
       "  0.232106551527977,\n",
       "  0.3399766683578491,\n",
       "  0.2893798351287842,\n",
       "  0.2648119330406189,\n",
       "  0.28927043080329895,\n",
       "  0.3149055540561676,\n",
       "  0.2561261057853699,\n",
       "  0.30244937539100647,\n",
       "  0.23156118392944336,\n",
       "  0.27664247155189514,\n",
       "  0.2524193227291107,\n",
       "  0.21390682458877563,\n",
       "  0.24752306938171387,\n",
       "  0.3054361045360565,\n",
       "  0.2590506076812744,\n",
       "  0.2672533094882965,\n",
       "  0.2865414023399353,\n",
       "  0.2771869897842407,\n",
       "  0.294720858335495,\n",
       "  0.344291627407074,\n",
       "  0.25714394450187683,\n",
       "  0.25365984439849854,\n",
       "  0.3346211314201355,\n",
       "  0.31766214966773987,\n",
       "  0.3689998984336853,\n",
       "  0.31213048100471497,\n",
       "  0.21689021587371826,\n",
       "  0.40793320536613464,\n",
       "  0.43551158905029297,\n",
       "  0.2606857419013977,\n",
       "  0.3027461767196655,\n",
       "  0.26324132084846497,\n",
       "  0.30173248052597046,\n",
       "  0.24512578547000885,\n",
       "  0.24176223576068878,\n",
       "  0.24145202338695526,\n",
       "  0.303654283285141,\n",
       "  0.22781187295913696,\n",
       "  0.33230510354042053,\n",
       "  0.32048913836479187,\n",
       "  0.24905376136302948,\n",
       "  0.2798868715763092,\n",
       "  0.258571058511734,\n",
       "  0.26557403802871704,\n",
       "  0.20939992368221283,\n",
       "  0.2718145251274109,\n",
       "  0.23397526144981384,\n",
       "  0.25584688782691956,\n",
       "  0.2844982147216797,\n",
       "  0.33469730615615845,\n",
       "  0.24575234949588776,\n",
       "  0.3415852189064026,\n",
       "  0.28892314434051514,\n",
       "  0.3371971547603607,\n",
       "  0.23069940507411957,\n",
       "  0.26105767488479614,\n",
       "  0.320050448179245,\n",
       "  0.26749175786972046,\n",
       "  0.2550501823425293,\n",
       "  0.3209807276725769,\n",
       "  0.30495986342430115,\n",
       "  0.2277480661869049,\n",
       "  0.29609215259552,\n",
       "  0.27596554160118103,\n",
       "  0.3811907172203064,\n",
       "  0.3927614986896515,\n",
       "  0.22532349824905396,\n",
       "  0.38790199160575867,\n",
       "  0.2530772089958191,\n",
       "  0.31506025791168213,\n",
       "  0.2868555784225464,\n",
       "  0.3307325839996338,\n",
       "  0.26953020691871643,\n",
       "  0.3291740119457245,\n",
       "  0.2659768760204315,\n",
       "  0.26482832431793213,\n",
       "  0.2838861644268036,\n",
       "  0.2848936915397644,\n",
       "  0.33000829815864563,\n",
       "  0.29068201780319214,\n",
       "  0.30347225069999695,\n",
       "  0.26891353726387024,\n",
       "  0.2731800079345703,\n",
       "  0.25871041417121887,\n",
       "  0.2574703097343445,\n",
       "  0.2747568190097809,\n",
       "  0.3109874129295349,\n",
       "  0.3076130747795105,\n",
       "  0.26120948791503906,\n",
       "  0.28044480085372925,\n",
       "  0.31575629115104675,\n",
       "  0.2705532908439636,\n",
       "  0.21693699061870575,\n",
       "  0.2970733940601349,\n",
       "  0.2673519253730774,\n",
       "  0.24706700444221497,\n",
       "  0.2680503726005554,\n",
       "  0.31994497776031494,\n",
       "  0.2694934010505676,\n",
       "  0.25856485962867737,\n",
       "  0.20396897196769714,\n",
       "  0.3095870912075043,\n",
       "  0.21470999717712402,\n",
       "  0.22855429351329803,\n",
       "  0.2649373710155487,\n",
       "  0.2530028820037842,\n",
       "  0.27112114429473877,\n",
       "  0.3634139895439148,\n",
       "  0.2700730860233307,\n",
       "  0.2774710953235626,\n",
       "  0.30450546741485596,\n",
       "  0.28769180178642273,\n",
       "  0.2772262394428253,\n",
       "  0.22879576683044434,\n",
       "  0.4051186442375183,\n",
       "  0.28068217635154724,\n",
       "  0.2590450644493103,\n",
       "  0.24586121737957,\n",
       "  0.29375168681144714,\n",
       "  0.2493557631969452,\n",
       "  0.2880832254886627,\n",
       "  0.2695496380329132,\n",
       "  0.263596773147583,\n",
       "  0.30955174565315247,\n",
       "  0.23588083684444427,\n",
       "  0.33788272738456726,\n",
       "  0.2703641355037689,\n",
       "  0.2892429232597351,\n",
       "  0.28152236342430115,\n",
       "  0.2776813805103302,\n",
       "  0.2557925879955292,\n",
       "  0.35683122277259827,\n",
       "  0.25727787613868713,\n",
       "  0.2928623855113983,\n",
       "  0.25933927297592163,\n",
       "  0.27558067440986633,\n",
       "  0.27360373735427856,\n",
       "  0.2744825780391693,\n",
       "  0.20717144012451172,\n",
       "  0.3621284067630768,\n",
       "  0.23046228289604187,\n",
       "  0.3212888538837433,\n",
       "  0.46792367100715637,\n",
       "  0.2892272472381592,\n",
       "  0.43110910058021545,\n",
       "  0.2500361204147339,\n",
       "  0.29400116205215454,\n",
       "  0.30478689074516296,\n",
       "  0.3260100483894348,\n",
       "  0.25807708501815796,\n",
       "  0.25577953457832336,\n",
       "  0.28483569622039795,\n",
       "  0.3627372086048126,\n",
       "  0.32125020027160645,\n",
       "  0.26158225536346436,\n",
       "  0.31557586789131165,\n",
       "  0.25084683299064636,\n",
       "  0.3831741511821747,\n",
       "  0.2735622823238373,\n",
       "  0.3245488107204437,\n",
       "  0.2625712752342224,\n",
       "  0.30494168400764465,\n",
       "  0.38587695360183716,\n",
       "  0.26932933926582336,\n",
       "  0.30381059646606445,\n",
       "  0.39872726798057556,\n",
       "  0.2530718147754669,\n",
       "  0.2968311309814453,\n",
       "  0.26749128103256226,\n",
       "  0.2679685354232788,\n",
       "  0.26674777269363403,\n",
       "  0.2230217605829239,\n",
       "  0.29039767384529114,\n",
       "  0.2982615828514099,\n",
       "  0.37871047854423523,\n",
       "  0.24428802728652954,\n",
       "  0.28255316615104675,\n",
       "  0.3380044102668762,\n",
       "  0.292018324136734,\n",
       "  0.235208198428154,\n",
       "  0.22135132551193237,\n",
       "  0.2693197429180145,\n",
       "  0.25849980115890503,\n",
       "  0.3841734230518341,\n",
       "  0.28310176730155945,\n",
       "  0.3294390141963959,\n",
       "  0.32650724053382874,\n",
       "  0.300327330827713,\n",
       "  0.2700946629047394,\n",
       "  0.24747952818870544,\n",
       "  0.3658157289028168,\n",
       "  0.3356476426124573,\n",
       "  0.2967591881752014,\n",
       "  0.2797262370586395,\n",
       "  0.37438294291496277,\n",
       "  0.28258347511291504,\n",
       "  0.2667163014411926,\n",
       "  0.2791517376899719,\n",
       "  0.2533077299594879,\n",
       "  0.28106728196144104,\n",
       "  0.2608616352081299,\n",
       "  0.3262426257133484,\n",
       "  0.2654955983161926,\n",
       "  0.322064071893692,\n",
       "  0.2546844482421875,\n",
       "  0.22254323959350586,\n",
       "  0.2500050961971283,\n",
       "  0.2812596559524536,\n",
       "  0.26001960039138794,\n",
       "  0.2772504687309265,\n",
       "  0.2629329562187195,\n",
       "  0.3503456711769104,\n",
       "  0.2916510999202728,\n",
       "  0.22481180727481842,\n",
       "  0.24959184229373932,\n",
       "  0.26839908957481384,\n",
       "  0.22908258438110352,\n",
       "  0.2972865402698517,\n",
       "  0.28604400157928467,\n",
       "  0.3481532335281372,\n",
       "  0.259371817111969,\n",
       "  0.41884520649909973,\n",
       "  0.3159000277519226,\n",
       "  0.31729039549827576,\n",
       "  0.21516640484333038,\n",
       "  0.27800610661506653,\n",
       "  0.2811715602874756,\n",
       "  0.31317660212516785,\n",
       "  0.25803130865097046,\n",
       "  0.46879127621650696,\n",
       "  0.32430151104927063,\n",
       "  0.27903878688812256,\n",
       "  0.3218306601047516,\n",
       "  0.2689216732978821,\n",
       "  0.32717466354370117,\n",
       "  0.32865384221076965,\n",
       "  0.3023644983768463,\n",
       "  0.2521853744983673,\n",
       "  0.281991571187973,\n",
       "  0.2642320990562439,\n",
       "  0.26836898922920227,\n",
       "  0.2995110750198364,\n",
       "  0.28995946049690247,\n",
       "  0.295886754989624,\n",
       "  0.2924172282218933,\n",
       "  0.27225860953330994,\n",
       "  0.2649892568588257,\n",
       "  0.29319342970848083,\n",
       "  0.273632675409317,\n",
       "  0.27150627970695496,\n",
       "  0.29371294379234314,\n",
       "  0.25614914298057556,\n",
       "  0.261385977268219,\n",
       "  0.31950655579566956,\n",
       "  0.2899344265460968,\n",
       "  0.27021822333335876,\n",
       "  0.23916006088256836,\n",
       "  0.29522109031677246,\n",
       "  0.2950988709926605,\n",
       "  0.28566762804985046,\n",
       "  0.25920218229293823,\n",
       "  0.36418628692626953,\n",
       "  0.306615948677063,\n",
       "  0.23414954543113708,\n",
       "  0.2424851655960083,\n",
       "  0.28691011667251587,\n",
       "  0.23684963583946228,\n",
       "  0.2624810039997101,\n",
       "  0.3248341679573059,\n",
       "  0.266786128282547,\n",
       "  0.23095625638961792,\n",
       "  0.2620173692703247,\n",
       "  0.2775457799434662,\n",
       "  0.24206385016441345,\n",
       "  0.2394474595785141,\n",
       "  0.2563716173171997,\n",
       "  0.3091162443161011,\n",
       "  0.3921178877353668,\n",
       "  0.24411243200302124,\n",
       "  0.1996697038412094,\n",
       "  0.2910924553871155,\n",
       "  0.2644746005535126,\n",
       "  0.25909703969955444,\n",
       "  0.2491108477115631,\n",
       "  0.29580986499786377,\n",
       "  0.2551519572734833,\n",
       "  0.28671208024024963,\n",
       "  0.2294609248638153,\n",
       "  0.26950332522392273,\n",
       "  0.26019373536109924,\n",
       "  0.25442588329315186,\n",
       "  0.26758676767349243,\n",
       "  0.3135709762573242,\n",
       "  0.2758542597293854,\n",
       "  0.2614750862121582,\n",
       "  0.266785204410553,\n",
       "  0.2711479663848877,\n",
       "  0.2989664375782013,\n",
       "  0.2961674928665161,\n",
       "  0.23455622792243958,\n",
       "  0.24529512226581573,\n",
       "  0.35318171977996826,\n",
       "  0.3140544295310974,\n",
       "  0.3055895268917084,\n",
       "  0.38291800022125244,\n",
       "  0.24462270736694336,\n",
       "  0.2650853097438812,\n",
       "  0.42721477150917053,\n",
       "  0.3044041395187378,\n",
       "  0.2535565197467804,\n",
       "  0.4587480127811432,\n",
       "  0.36972174048423767,\n",
       "  0.3500101864337921,\n",
       "  0.24073012173175812,\n",
       "  0.2401048094034195,\n",
       "  0.2654007375240326,\n",
       "  0.2721846103668213,\n",
       "  0.3185475766658783,\n",
       "  0.3134748935699463,\n",
       "  0.2369932234287262,\n",
       "  0.3371051251888275,\n",
       "  0.29142504930496216,\n",
       "  0.2260635495185852,\n",
       "  0.23860737681388855,\n",
       "  0.32458677887916565,\n",
       "  0.23272471129894257,\n",
       "  0.23704186081886292,\n",
       "  0.27615034580230713,\n",
       "  0.3980424106121063,\n",
       "  0.33863362669944763,\n",
       "  0.28454336524009705,\n",
       "  0.3173643946647644,\n",
       "  0.3438495099544525,\n",
       "  0.3935515284538269,\n",
       "  0.26408833265304565,\n",
       "  0.3473946154117584,\n",
       "  0.3311089277267456,\n",
       "  0.4425603449344635,\n",
       "  0.2872146964073181,\n",
       "  0.3332488238811493,\n",
       "  0.23866230249404907,\n",
       "  0.2634679675102234,\n",
       "  0.3117380738258362,\n",
       "  0.28200244903564453,\n",
       "  0.262933611869812,\n",
       "  0.3222188651561737,\n",
       "  0.4779724180698395,\n",
       "  0.2269420027732849,\n",
       "  0.32044413685798645,\n",
       "  0.24470725655555725,\n",
       "  0.22140252590179443,\n",
       "  0.2797079086303711,\n",
       "  0.2380608767271042,\n",
       "  0.24465778470039368,\n",
       "  0.2506519854068756,\n",
       "  0.27383187413215637,\n",
       "  0.2864517271518707,\n",
       "  0.2858303487300873,\n",
       "  0.27496638894081116,\n",
       "  0.27482983469963074,\n",
       "  0.29975903034210205,\n",
       "  0.23236028850078583,\n",
       "  0.27320602536201477,\n",
       "  0.2690321207046509,\n",
       "  0.2514236867427826,\n",
       "  0.4132475256919861,\n",
       "  0.2129666805267334,\n",
       "  0.23473499715328217,\n",
       "  0.2645157277584076,\n",
       "  0.2815634310245514,\n",
       "  0.3188234269618988,\n",
       "  0.295198529958725,\n",
       "  0.25997135043144226,\n",
       "  0.2544131577014923,\n",
       "  0.2803741991519928,\n",
       "  0.26472511887550354,\n",
       "  0.2699156105518341,\n",
       "  0.2102838009595871,\n",
       "  0.2832716107368469,\n",
       "  0.317634254693985,\n",
       "  0.27028965950012207,\n",
       "  0.22556211054325104,\n",
       "  0.30616408586502075,\n",
       "  0.3027622699737549,\n",
       "  0.3034321963787079,\n",
       "  0.27858439087867737,\n",
       "  0.27574971318244934,\n",
       "  0.29365479946136475,\n",
       "  0.22136946022510529,\n",
       "  0.25856077671051025,\n",
       "  0.25809958577156067,\n",
       "  0.29500046372413635,\n",
       "  0.2557871639728546,\n",
       "  0.36502453684806824,\n",
       "  0.26963090896606445,\n",
       "  0.2861574590206146,\n",
       "  0.2483004331588745,\n",
       "  0.26783719658851624,\n",
       "  0.28357329964637756,\n",
       "  0.2879788875579834,\n",
       "  0.3001076579093933,\n",
       "  0.2827056050300598,\n",
       "  0.27766066789627075,\n",
       "  0.30363428592681885,\n",
       "  0.3116750121116638,\n",
       "  0.2658330798149109,\n",
       "  0.24844804406166077,\n",
       "  0.33890336751937866,\n",
       "  0.26964402198791504,\n",
       "  0.26707062125205994,\n",
       "  0.2125004678964615,\n",
       "  0.24674095213413239,\n",
       "  0.22106266021728516,\n",
       "  0.28827473521232605,\n",
       "  0.25136566162109375,\n",
       "  0.30436643958091736,\n",
       "  0.2260824739933014,\n",
       "  0.28051066398620605,\n",
       "  0.3186528980731964,\n",
       "  0.26300516724586487,\n",
       "  0.283593088388443,\n",
       "  0.18543140590190887,\n",
       "  0.27495619654655457,\n",
       "  0.24162332713603973,\n",
       "  0.26067498326301575,\n",
       "  0.2751433849334717,\n",
       "  0.2532918155193329,\n",
       "  0.2658784091472626,\n",
       "  0.26252681016921997,\n",
       "  0.3387226462364197,\n",
       "  0.3457040786743164,\n",
       "  0.38601717352867126,\n",
       "  0.2476118505001068,\n",
       "  0.2529492974281311,\n",
       "  0.2730109989643097,\n",
       "  0.31955522298812866,\n",
       "  0.3224586248397827,\n",
       "  0.24798831343650818,\n",
       "  0.2515876889228821,\n",
       "  0.29542040824890137,\n",
       "  0.23592214286327362,\n",
       "  0.2784919738769531,\n",
       "  0.2301349937915802,\n",
       "  0.2877058684825897,\n",
       "  0.3427136540412903,\n",
       "  0.3364199101924896,\n",
       "  0.26539337635040283,\n",
       "  0.3768729269504547,\n",
       "  0.2995222210884094,\n",
       "  0.4066469371318817,\n",
       "  0.3142898678779602,\n",
       "  0.30402475595474243,\n",
       "  0.3047142028808594,\n",
       "  0.21240192651748657,\n",
       "  0.24623803794384003,\n",
       "  0.2730146646499634,\n",
       "  0.3218027949333191,\n",
       "  0.3057997524738312,\n",
       "  0.2314990907907486,\n",
       "  0.2807336151599884,\n",
       "  0.293111115694046,\n",
       "  0.21437062323093414,\n",
       "  0.27558690309524536,\n",
       "  0.2071874588727951,\n",
       "  0.33670172095298767,\n",
       "  0.27086880803108215,\n",
       "  0.28662949800491333,\n",
       "  0.28164002299308777,\n",
       "  0.29415255784988403,\n",
       "  0.2919628322124481,\n",
       "  0.3396153450012207,\n",
       "  0.26037269830703735,\n",
       "  0.2906447649002075,\n",
       "  0.3133162260055542,\n",
       "  0.2564482092857361,\n",
       "  0.23480689525604248,\n",
       "  0.3126591145992279,\n",
       "  0.22271381318569183,\n",
       "  0.30136677622795105,\n",
       "  0.25166529417037964,\n",
       "  0.3870372772216797,\n",
       "  0.2611809968948364,\n",
       "  0.26739808917045593,\n",
       "  0.24315986037254333,\n",
       "  0.2719683051109314,\n",
       "  0.36233246326446533,\n",
       "  0.24356761574745178,\n",
       "  0.20952942967414856,\n",
       "  0.4122723340988159,\n",
       "  0.36183053255081177,\n",
       "  0.2991417944431305,\n",
       "  0.25798019766807556,\n",
       "  0.3123423159122467,\n",
       "  0.30269142985343933,\n",
       "  0.2606951594352722,\n",
       "  0.27607059478759766,\n",
       "  0.27324527502059937,\n",
       "  0.2975938022136688,\n",
       "  0.3308973014354706,\n",
       "  0.3650827705860138,\n",
       "  0.3749403655529022,\n",
       "  0.27895089983940125,\n",
       "  0.292181134223938,\n",
       "  0.25289762020111084,\n",
       "  0.4001852571964264,\n",
       "  0.3622179627418518,\n",
       "  0.27130600810050964,\n",
       "  0.3400440216064453,\n",
       "  0.3074277937412262,\n",
       "  0.2470470368862152,\n",
       "  0.29068678617477417,\n",
       "  0.2601793110370636,\n",
       "  0.20520547032356262,\n",
       "  0.2919614911079407,\n",
       "  0.27675846219062805,\n",
       "  0.40292665362358093,\n",
       "  0.36089926958084106,\n",
       "  0.3485947847366333,\n",
       "  0.2835385203361511,\n",
       "  0.2743872106075287,\n",
       "  0.22312894463539124,\n",
       "  0.259513795375824,\n",
       "  0.20322446525096893,\n",
       "  0.2590548098087311,\n",
       "  0.23077847063541412,\n",
       "  0.28566044569015503,\n",
       "  0.2519644796848297,\n",
       "  0.2615985870361328,\n",
       "  0.3177882134914398,\n",
       "  0.27852845191955566,\n",
       "  0.25301164388656616,\n",
       "  0.2612403333187103,\n",
       "  0.2372140884399414,\n",
       "  0.3057482838630676,\n",
       "  0.2820022404193878,\n",
       "  0.29368165135383606,\n",
       "  0.2865838408470154,\n",
       "  0.2662312388420105,\n",
       "  0.23935005068778992,\n",
       "  0.2655078172683716,\n",
       "  0.28678983449935913,\n",
       "  0.25977596640586853,\n",
       "  0.32355645298957825,\n",
       "  0.2605538070201874,\n",
       "  0.34222880005836487,\n",
       "  0.3092292249202728,\n",
       "  0.28989294171333313,\n",
       "  0.2839438319206238,\n",
       "  0.3098500072956085,\n",
       "  0.3956606686115265,\n",
       "  0.33797788619995117,\n",
       "  0.21108712255954742,\n",
       "  0.30323857069015503,\n",
       "  0.2769213914871216,\n",
       "  0.28328120708465576,\n",
       "  0.2676810026168823,\n",
       "  0.27641671895980835,\n",
       "  0.3072851002216339,\n",
       "  0.2191523015499115,\n",
       "  0.25357916951179504,\n",
       "  0.3733774721622467,\n",
       "  0.2299392819404602,\n",
       "  0.3939376771450043,\n",
       "  0.2903704345226288,\n",
       "  0.2989410161972046,\n",
       "  0.27928638458251953,\n",
       "  0.301224946975708,\n",
       "  0.2859690487384796,\n",
       "  0.45130524039268494,\n",
       "  0.3216744065284729,\n",
       "  0.24441201984882355,\n",
       "  0.2953276038169861,\n",
       "  0.3234543204307556,\n",
       "  0.36076104640960693,\n",
       "  0.3280513286590576,\n",
       "  0.28077390789985657,\n",
       "  0.31319165229797363,\n",
       "  0.43476781249046326,\n",
       "  0.25089800357818604,\n",
       "  0.40231481194496155,\n",
       "  0.2622348964214325,\n",
       "  0.24588029086589813,\n",
       "  0.25163429975509644,\n",
       "  0.27327752113342285,\n",
       "  0.26181715726852417,\n",
       "  0.29049190878868103,\n",
       "  0.2812962234020233,\n",
       "  0.24330522119998932,\n",
       "  0.33145737648010254,\n",
       "  0.29552632570266724,\n",
       "  0.28983381390571594,\n",
       "  0.2575400769710541,\n",
       "  0.28928807377815247,\n",
       "  0.2810375988483429,\n",
       "  0.3462880253791809,\n",
       "  0.23984725773334503,\n",
       "  0.313865065574646,\n",
       "  0.29399144649505615,\n",
       "  0.3038741946220398,\n",
       "  0.3193371891975403,\n",
       "  0.2230415940284729,\n",
       "  0.27536293864250183,\n",
       "  0.26254087686538696,\n",
       "  0.2716886103153229,\n",
       "  0.31773877143859863,\n",
       "  0.30451127886772156,\n",
       "  0.2609819769859314,\n",
       "  0.1966663897037506,\n",
       "  0.2471996545791626,\n",
       "  0.22301509976387024,\n",
       "  0.2868747413158417,\n",
       "  0.2954736053943634,\n",
       "  0.35049036145210266,\n",
       "  0.2745944857597351,\n",
       "  0.21357041597366333,\n",
       "  0.251455694437027,\n",
       "  0.19571930170059204,\n",
       "  0.2839718759059906,\n",
       "  0.25205162167549133,\n",
       "  0.25819146633148193,\n",
       "  0.25310373306274414,\n",
       "  0.27107158303260803,\n",
       "  0.24170136451721191,\n",
       "  0.25674349069595337,\n",
       "  0.26746657490730286,\n",
       "  0.26974427700042725,\n",
       "  0.20692870020866394,\n",
       "  0.23605912923812866,\n",
       "  0.23989735543727875,\n",
       "  0.29315027594566345,\n",
       "  0.287802129983902,\n",
       "  0.25793391466140747,\n",
       "  0.2625856399536133,\n",
       "  0.27559345960617065,\n",
       "  0.3010343313217163,\n",
       "  0.22441479563713074,\n",
       "  0.25150737166404724,\n",
       "  0.22371000051498413,\n",
       "  0.3051682114601135,\n",
       "  0.26957717537879944,\n",
       "  0.20201227068901062,\n",
       "  0.2827788293361664,\n",
       "  0.26802489161491394,\n",
       "  0.3396678566932678,\n",
       "  0.2713465988636017,\n",
       "  0.24857546389102936,\n",
       "  0.46161478757858276,\n",
       "  0.28381702303886414,\n",
       "  0.26520150899887085,\n",
       "  0.29414859414100647,\n",
       "  0.3675205707550049,\n",
       "  0.2827090322971344,\n",
       "  0.29131028056144714,\n",
       "  0.32248377799987793,\n",
       "  0.2878583073616028,\n",
       "  0.284244179725647,\n",
       "  0.2501988410949707,\n",
       "  0.32168594002723694,\n",
       "  0.3130059540271759,\n",
       "  0.3535133898258209,\n",
       "  0.21995709836483002,\n",
       "  0.26212719082832336,\n",
       "  0.2770296335220337,\n",
       "  0.26342782378196716,\n",
       "  0.2791382670402527,\n",
       "  0.3070929944515228,\n",
       "  0.33205217123031616,\n",
       "  0.2556171715259552,\n",
       "  0.2130037099123001,\n",
       "  0.33339715003967285,\n",
       "  0.22623102366924286,\n",
       "  0.33651426434516907,\n",
       "  0.30715224146842957,\n",
       "  0.25025075674057007,\n",
       "  0.24923057854175568,\n",
       "  0.2280227094888687,\n",
       "  0.3023049235343933,\n",
       "  0.348662793636322,\n",
       "  0.2538406252861023,\n",
       "  0.28467315435409546,\n",
       "  0.29723334312438965,\n",
       "  0.300944060087204,\n",
       "  0.27011507749557495,\n",
       "  0.2402288317680359,\n",
       "  0.34034764766693115,\n",
       "  0.2711320221424103,\n",
       "  0.2304055243730545,\n",
       "  0.28428253531455994,\n",
       "  0.26293087005615234,\n",
       "  0.28255605697631836,\n",
       "  0.27639657258987427,\n",
       "  0.2856449782848358,\n",
       "  0.3153194487094879,\n",
       "  0.2551068663597107,\n",
       "  0.2831076979637146,\n",
       "  0.2864810526371002,\n",
       "  0.2825224697589874,\n",
       "  0.27007612586021423,\n",
       "  0.2857573330402374,\n",
       "  0.2907448709011078,\n",
       "  0.2564791738986969,\n",
       "  0.2674732804298401,\n",
       "  0.27905404567718506,\n",
       "  0.28738218545913696,\n",
       "  0.27218905091285706,\n",
       "  0.25483667850494385,\n",
       "  0.26571136713027954,\n",
       "  0.2477533221244812,\n",
       "  0.3017323911190033,\n",
       "  0.2882789373397827,\n",
       "  0.277159184217453,\n",
       "  0.2606671452522278,\n",
       "  0.2593744099140167,\n",
       "  0.32105594873428345,\n",
       "  0.36389783024787903,\n",
       "  0.3440539538860321,\n",
       "  ...],\n",
       " 'sim_soft': ['0.25589907',\n",
       "  '0.42919788',\n",
       "  '0.27725807',\n",
       "  '0.2993618',\n",
       "  '0.2473363',\n",
       "  '0.2677138',\n",
       "  '0.3012818',\n",
       "  '0.28370714',\n",
       "  '0.2767033',\n",
       "  '0.3799205',\n",
       "  '0.3207671',\n",
       "  '0.23442489',\n",
       "  '0.35655278',\n",
       "  '0.38433865',\n",
       "  '0.2474509',\n",
       "  '0.3034425',\n",
       "  '0.28940135',\n",
       "  '0.2880761',\n",
       "  '0.29334617',\n",
       "  '0.27012256',\n",
       "  '0.3640541',\n",
       "  '0.21405734',\n",
       "  '0.27744195',\n",
       "  '0.2204422',\n",
       "  '0.2224376',\n",
       "  '0.29226983',\n",
       "  '0.28474015',\n",
       "  '0.4038915',\n",
       "  '0.26955202',\n",
       "  '0.38904876',\n",
       "  '0.3028926',\n",
       "  '0.2986697',\n",
       "  '0.35084665',\n",
       "  '0.39708388',\n",
       "  '0.34007648',\n",
       "  '0.27839756',\n",
       "  '0.32258517',\n",
       "  '0.30327904',\n",
       "  '0.29951888',\n",
       "  '0.42098248',\n",
       "  '0.2728123',\n",
       "  '0.26068908',\n",
       "  '0.32689926',\n",
       "  '0.21647805',\n",
       "  '0.29062805',\n",
       "  '0.41527522',\n",
       "  '0.23324984',\n",
       "  '0.1820311',\n",
       "  '0.2786783',\n",
       "  '0.3100204',\n",
       "  '0.31939608',\n",
       "  '0.41817093',\n",
       "  '0.2464604',\n",
       "  '0.30393612',\n",
       "  '0.31228778',\n",
       "  '0.3215031',\n",
       "  '0.27788126',\n",
       "  '0.2754858',\n",
       "  '0.34359264',\n",
       "  '0.3366799',\n",
       "  '0.34568763',\n",
       "  '0.29485366',\n",
       "  '0.31903297',\n",
       "  '0.28037155',\n",
       "  '0.41094923',\n",
       "  '0.3023617',\n",
       "  '0.2874872',\n",
       "  '0.38006932',\n",
       "  '0.32810998',\n",
       "  '0.3180944',\n",
       "  '0.3324328',\n",
       "  '0.247502',\n",
       "  '0.30213663',\n",
       "  '0.26503813',\n",
       "  '0.29027334',\n",
       "  '0.32644165',\n",
       "  '0.2437264',\n",
       "  '0.2649673',\n",
       "  '0.21412744',\n",
       "  '0.31371439',\n",
       "  '0.2914613',\n",
       "  '0.2012903',\n",
       "  '0.28804162',\n",
       "  '0.3305541',\n",
       "  '0.28096572',\n",
       "  '0.22995694',\n",
       "  '0.37307107',\n",
       "  '0.2812924',\n",
       "  '0.36471772',\n",
       "  '0.23359475',\n",
       "  '0.28265858',\n",
       "  '0.36349416',\n",
       "  '0.30082986',\n",
       "  '0.2741093',\n",
       "  '0.14135942',\n",
       "  '0.39314497',\n",
       "  '0.35239452',\n",
       "  '0.29341695',\n",
       "  '0.39057454',\n",
       "  '0.30456233',\n",
       "  '0.3696918',\n",
       "  '0.31978986',\n",
       "  '0.38148302',\n",
       "  '0.25325263',\n",
       "  '0.3792409',\n",
       "  '0.30264634',\n",
       "  '0.3073235',\n",
       "  '0.25331587',\n",
       "  '0.36641818',\n",
       "  '0.38569582',\n",
       "  '0.33596358',\n",
       "  '0.4964652',\n",
       "  '0.3815361',\n",
       "  '0.25238436',\n",
       "  '0.28707558',\n",
       "  '0.29182738',\n",
       "  '0.27088773',\n",
       "  '0.3494075',\n",
       "  '0.28579512',\n",
       "  '0.19646609',\n",
       "  '0.31002885',\n",
       "  '0.38523132',\n",
       "  '0.30058914',\n",
       "  '0.35515726',\n",
       "  '0.2887946',\n",
       "  '0.1948601',\n",
       "  '0.2974936',\n",
       "  '0.27460036',\n",
       "  '0.34813237',\n",
       "  '0.29010963',\n",
       "  '0.27432442',\n",
       "  '0.35530317',\n",
       "  '0.2437652',\n",
       "  '0.4794932',\n",
       "  '0.30444917',\n",
       "  '0.32070696',\n",
       "  '0.22400054',\n",
       "  '0.26432246',\n",
       "  '0.30117774',\n",
       "  '0.28845897',\n",
       "  '0.24550444',\n",
       "  '0.2981649',\n",
       "  '0.32480264',\n",
       "  '0.24077913',\n",
       "  '0.27682585',\n",
       "  '0.19448876',\n",
       "  '0.3155112',\n",
       "  '0.35135025',\n",
       "  '0.37956688',\n",
       "  '0.29591438',\n",
       "  '0.28091848',\n",
       "  '0.24781984',\n",
       "  '0.26335743',\n",
       "  '0.3098411',\n",
       "  '0.21840258',\n",
       "  '0.2593239',\n",
       "  '0.32729948',\n",
       "  '0.28421262',\n",
       "  '0.35004622',\n",
       "  '0.22432509',\n",
       "  '0.2454982',\n",
       "  '0.30725443',\n",
       "  '0.26535097',\n",
       "  '0.34321177',\n",
       "  '0.29728878',\n",
       "  '0.30738044',\n",
       "  '0.25932235',\n",
       "  '0.3137775',\n",
       "  '0.25527173',\n",
       "  '0.33082065',\n",
       "  '0.33496502',\n",
       "  '0.2158011',\n",
       "  '0.39115456',\n",
       "  '0.2181294',\n",
       "  '0.25573272',\n",
       "  '0.25913364',\n",
       "  '0.28244925',\n",
       "  '0.39162597',\n",
       "  '0.26995844',\n",
       "  '0.24685164',\n",
       "  '0.35858747',\n",
       "  '0.22819656',\n",
       "  '0.3991685',\n",
       "  '0.2846735',\n",
       "  '0.29639256',\n",
       "  '0.29248226',\n",
       "  '0.3336339',\n",
       "  '0.34924182',\n",
       "  '0.2528184',\n",
       "  '0.29214054',\n",
       "  '0.3366379',\n",
       "  '0.27284795',\n",
       "  '0.31603062',\n",
       "  '0.32986826',\n",
       "  '0.29323155',\n",
       "  '0.34445348',\n",
       "  '0.31017476',\n",
       "  '0.2619921',\n",
       "  '0.24620278',\n",
       "  '0.3749837',\n",
       "  '0.28871524',\n",
       "  '0.2863747',\n",
       "  '0.30822763',\n",
       "  '0.22755808',\n",
       "  '0.3363787',\n",
       "  '0.2692001',\n",
       "  '0.30077255',\n",
       "  '0.24912852',\n",
       "  '0.3053693',\n",
       "  '0.38714156',\n",
       "  '0.30961078',\n",
       "  '0.21575643',\n",
       "  '0.30107766',\n",
       "  '0.3645322',\n",
       "  '0.25316304',\n",
       "  '0.3329685',\n",
       "  '0.25804663',\n",
       "  '0.27502596',\n",
       "  '0.33995977',\n",
       "  '0.37135804',\n",
       "  '0.30375955',\n",
       "  '0.2663318',\n",
       "  '0.271717',\n",
       "  '0.22591357',\n",
       "  '0.2894264',\n",
       "  '0.27585113',\n",
       "  '0.266639',\n",
       "  '0.2071678',\n",
       "  '0.2749793',\n",
       "  '0.37644967',\n",
       "  '0.2596982',\n",
       "  '0.29153776',\n",
       "  '0.34737396',\n",
       "  '0.33124712',\n",
       "  '0.3363231',\n",
       "  '0.28458464',\n",
       "  '0.4366682',\n",
       "  '0.32468042',\n",
       "  '0.2759769',\n",
       "  '0.21323937',\n",
       "  '0.33699456',\n",
       "  '0.29790685',\n",
       "  '0.31270072',\n",
       "  '0.23427686',\n",
       "  '0.3192636',\n",
       "  '0.31372893',\n",
       "  '0.25196093',\n",
       "  '0.2671618',\n",
       "  '0.29445136',\n",
       "  '0.22187506',\n",
       "  '0.3251792',\n",
       "  '0.30354106',\n",
       "  '0.38446534',\n",
       "  '0.30049786',\n",
       "  '0.3259037',\n",
       "  '0.3442021',\n",
       "  '0.28839892',\n",
       "  '0.36234203',\n",
       "  '0.28624722',\n",
       "  '0.4065346',\n",
       "  '0.31920004',\n",
       "  '0.2603269',\n",
       "  '0.25754738',\n",
       "  '0.35182926',\n",
       "  '0.2384412',\n",
       "  '0.35028115',\n",
       "  '0.27286452',\n",
       "  '0.23236516',\n",
       "  '0.28057295',\n",
       "  '0.3788429',\n",
       "  '0.35060614',\n",
       "  '0.26718938',\n",
       "  '0.2331365',\n",
       "  '0.28082776',\n",
       "  '0.347503',\n",
       "  '0.3555043',\n",
       "  '0.29801214',\n",
       "  '0.29722863',\n",
       "  '0.27400666',\n",
       "  '0.29542053',\n",
       "  '0.31776196',\n",
       "  '0.4003235',\n",
       "  '0.2647109',\n",
       "  '0.24398717',\n",
       "  '0.32638162',\n",
       "  '0.39256808',\n",
       "  '0.28256682',\n",
       "  '0.34355378',\n",
       "  '0.27931714',\n",
       "  '0.31490555',\n",
       "  '0.24671248',\n",
       "  '0.27580845',\n",
       "  '0.23486897',\n",
       "  '0.27697474',\n",
       "  '0.32741898',\n",
       "  '0.20415048',\n",
       "  '0.24442293',\n",
       "  '0.37812737',\n",
       "  '0.22877902',\n",
       "  '0.3167851',\n",
       "  '0.2865414',\n",
       "  '0.31576264',\n",
       "  '0.29784372',\n",
       "  '0.3816577',\n",
       "  '0.27269647',\n",
       "  '0.32951587',\n",
       "  '0.34615004',\n",
       "  '0.365296',\n",
       "  '0.3689999',\n",
       "  '0.34692448',\n",
       "  '0.22145243',\n",
       "  '0.42132664',\n",
       "  '0.4526949',\n",
       "  '0.30822688',\n",
       "  '0.34025955',\n",
       "  '0.3005273',\n",
       "  '0.3263717',\n",
       "  '0.24322519',\n",
       "  '0.21423072',\n",
       "  '0.29177484',\n",
       "  '0.32288414',\n",
       "  '0.23697719',\n",
       "  '0.3805738',\n",
       "  '0.32048914',\n",
       "  '0.25892195',\n",
       "  '0.32030046',\n",
       "  '0.2755815',\n",
       "  '0.3297124',\n",
       "  '0.21419919',\n",
       "  '0.3062418',\n",
       "  '0.22233236',\n",
       "  '0.27285',\n",
       "  '0.4602341',\n",
       "  '0.34972182',\n",
       "  '0.24418096',\n",
       "  '0.38829404',\n",
       "  '0.25322',\n",
       "  '0.33719715',\n",
       "  '0.283866',\n",
       "  '0.27704006',\n",
       "  '0.30049515',\n",
       "  '0.33137953',\n",
       "  '0.25505018',\n",
       "  '0.32098073',\n",
       "  '0.34300297',\n",
       "  '0.23146681',\n",
       "  '0.2806533',\n",
       "  '0.3113005',\n",
       "  '0.42408657',\n",
       "  '0.3927615',\n",
       "  '0.23395614',\n",
       "  '0.387902',\n",
       "  '0.25600082',\n",
       "  '0.36172092',\n",
       "  '0.31229204',\n",
       "  '0.3382282',\n",
       "  '0.32763273',\n",
       "  '0.36112288',\n",
       "  '0.25617743',\n",
       "  '0.25807527',\n",
       "  '0.4067762',\n",
       "  '0.2848937',\n",
       "  '0.36435777',\n",
       "  '0.3301181',\n",
       "  '0.35485566',\n",
       "  '0.28365964',\n",
       "  '0.286887',\n",
       "  '0.30347967',\n",
       "  '0.2838418',\n",
       "  '0.27007985',\n",
       "  '0.3109874',\n",
       "  '0.33757043',\n",
       "  '0.24765348',\n",
       "  '0.2947197',\n",
       "  '0.3432629',\n",
       "  '0.26310584',\n",
       "  '0.20755658',\n",
       "  '0.3611647',\n",
       "  '0.32695043',\n",
       "  '0.3005566',\n",
       "  '0.26805037',\n",
       "  '0.29279777',\n",
       "  '0.402692',\n",
       "  '0.29036474',\n",
       "  '0.16299713',\n",
       "  '0.3095871',\n",
       "  '0.2440002',\n",
       "  '0.19360997',\n",
       "  '0.30305701',\n",
       "  '0.25285113',\n",
       "  '0.34574148',\n",
       "  '0.363414',\n",
       "  '0.26797694',\n",
       "  '0.27669293',\n",
       "  '0.30450547',\n",
       "  '0.31121433',\n",
       "  '0.2839291',\n",
       "  '0.22879577',\n",
       "  '0.40511864',\n",
       "  '0.28699923',\n",
       "  '0.39405167',\n",
       "  '0.254459',\n",
       "  '0.2937517',\n",
       "  '0.24088314',\n",
       "  '0.27174285',\n",
       "  '0.33168626',\n",
       "  '0.26854146',\n",
       "  '0.29375443',\n",
       "  '0.25578174',\n",
       "  '0.33788273',\n",
       "  '0.3345766',\n",
       "  '0.39601234',\n",
       "  '0.25155148',\n",
       "  '0.2468872',\n",
       "  '0.24971256',\n",
       "  '0.35683122',\n",
       "  '0.2807144',\n",
       "  '0.28248936',\n",
       "  '0.30173647',\n",
       "  '0.27558067',\n",
       "  '0.3080901',\n",
       "  '0.25102141',\n",
       "  '0.2521964',\n",
       "  '0.3854841',\n",
       "  '0.22572672',\n",
       "  '0.32128885',\n",
       "  '0.5182339',\n",
       "  '0.340104',\n",
       "  '0.44129202',\n",
       "  '0.25003612',\n",
       "  '0.28330633',\n",
       "  '0.33176947',\n",
       "  '0.32601005',\n",
       "  '0.27730703',\n",
       "  '0.25577953',\n",
       "  '0.26992586',\n",
       "  '0.3627372',\n",
       "  '0.3420332',\n",
       "  '0.2544266',\n",
       "  '0.34107387',\n",
       "  '0.2901935',\n",
       "  '0.38317415',\n",
       "  '0.32545334',\n",
       "  '0.3245488',\n",
       "  '0.33152848',\n",
       "  '0.30554384',\n",
       "  '0.38127702',\n",
       "  '0.33029515',\n",
       "  '0.30655205',\n",
       "  '0.40731505',\n",
       "  '0.29258',\n",
       "  '0.30403322',\n",
       "  '0.26749128',\n",
       "  '0.28628945',\n",
       "  '0.31917357',\n",
       "  '0.22302176',\n",
       "  '0.29039767',\n",
       "  '0.29826158',\n",
       "  '0.3554824',\n",
       "  '0.26067883',\n",
       "  '0.30363765',\n",
       "  '0.34888595',\n",
       "  '0.29201832',\n",
       "  '0.23033306',\n",
       "  '0.2012102',\n",
       "  '0.28878236',\n",
       "  '0.26348653',\n",
       "  '0.38417342',\n",
       "  '0.28310177',\n",
       "  '0.31778163',\n",
       "  '0.32650724',\n",
       "  '0.30032733',\n",
       "  '0.27077132',\n",
       "  '0.31559262',\n",
       "  '0.3987524',\n",
       "  '0.35989535',\n",
       "  '0.2877313',\n",
       "  '0.30538222',\n",
       "  '0.37831587',\n",
       "  '0.31981918',\n",
       "  '0.26821238',\n",
       "  '0.34173042',\n",
       "  '0.29962337',\n",
       "  '0.29160506',\n",
       "  '0.28581294',\n",
       "  '0.336959',\n",
       "  '0.31928337',\n",
       "  '0.39322013',\n",
       "  '0.21091652',\n",
       "  '0.21944295',\n",
       "  '0.2604161',\n",
       "  '0.30142218',\n",
       "  '0.21015474',\n",
       "  '0.38314998',\n",
       "  '0.3193384',\n",
       "  '0.41670442',\n",
       "  '0.32261565',\n",
       "  '0.214753',\n",
       "  '0.30012518',\n",
       "  '0.29140088',\n",
       "  '0.22886707',\n",
       "  '0.3589337',\n",
       "  '0.286044',\n",
       "  '0.41906637',\n",
       "  '0.25937182',\n",
       "  '0.44991624',\n",
       "  '0.3762917',\n",
       "  '0.4017313',\n",
       "  '0.26109445',\n",
       "  '0.2780061',\n",
       "  '0.31288853',\n",
       "  '0.35056943',\n",
       "  '0.2452874',\n",
       "  '0.45052004',\n",
       "  '0.3671943',\n",
       "  '0.2790388',\n",
       "  '0.3648705',\n",
       "  '0.3026145',\n",
       "  '0.35408443',\n",
       "  '0.37541664',\n",
       "  '0.33901665',\n",
       "  '0.36768925',\n",
       "  '0.29949376',\n",
       "  '0.31966275',\n",
       "  '0.31400335',\n",
       "  '0.32775548',\n",
       "  '0.26156342',\n",
       "  '0.27523518',\n",
       "  '0.3092131',\n",
       "  '0.30499163',\n",
       "  '0.26498926',\n",
       "  '0.31160688',\n",
       "  '0.26762444',\n",
       "  '0.28469655',\n",
       "  '0.39215553',\n",
       "  '0.24938448',\n",
       "  '0.23260292',\n",
       "  '0.31257313',\n",
       "  '0.31823242',\n",
       "  '0.24451643',\n",
       "  '0.25673652',\n",
       "  '0.308487',\n",
       "  '0.2677798',\n",
       "  '0.41987896',\n",
       "  '0.22438896',\n",
       "  '0.41554248',\n",
       "  '0.31294787',\n",
       "  '0.25305074',\n",
       "  '0.2909823',\n",
       "  '0.24317728',\n",
       "  '0.29587966',\n",
       "  '0.28205746',\n",
       "  '0.33939725',\n",
       "  '0.2729519',\n",
       "  '0.26326936',\n",
       "  '0.2378995',\n",
       "  '0.34278327',\n",
       "  '0.22684053',\n",
       "  '0.2890486',\n",
       "  '0.30158472',\n",
       "  '0.31829342',\n",
       "  '0.4178871',\n",
       "  '0.2064611',\n",
       "  '0.2113796',\n",
       "  '0.3466273',\n",
       "  '0.24483807',\n",
       "  '0.30566376',\n",
       "  '0.3184901',\n",
       "  '0.3595537',\n",
       "  '0.25515196',\n",
       "  '0.34979504',\n",
       "  '0.22946092',\n",
       "  '0.2800395',\n",
       "  '0.2549764',\n",
       "  '0.3454162',\n",
       "  '0.31919295',\n",
       "  '0.31929046',\n",
       "  '0.28199047',\n",
       "  '0.30914515',\n",
       "  '0.3226986',\n",
       "  '0.27114797',\n",
       "  '0.34145874',\n",
       "  '0.35196212',\n",
       "  '0.23455623',\n",
       "  '0.24196362',\n",
       "  '0.3405545',\n",
       "  '0.3485002',\n",
       "  '0.30558953',\n",
       "  '0.382918',\n",
       "  '0.2520916',\n",
       "  '0.25993407',\n",
       "  '0.51849365',\n",
       "  '0.2766574',\n",
       "  '0.25368667',\n",
       "  '0.49284625',\n",
       "  '0.4385287',\n",
       "  '0.3953805',\n",
       "  '0.26326782',\n",
       "  '0.32886523',\n",
       "  '0.27756864',\n",
       "  '0.28883827',\n",
       "  '0.30828947',\n",
       "  '0.3134749',\n",
       "  '0.26113033',\n",
       "  '0.3566535',\n",
       "  '0.28843942',\n",
       "  '0.2725821',\n",
       "  '0.13876833',\n",
       "  '0.41412234',\n",
       "  '0.2891686',\n",
       "  '0.24702573',\n",
       "  '0.31888792',\n",
       "  '0.42129648',\n",
       "  '0.3837799',\n",
       "  '0.2762918',\n",
       "  '0.3173644',\n",
       "  '0.31223464',\n",
       "  '0.39355153',\n",
       "  '0.26316997',\n",
       "  '0.40205395',\n",
       "  '0.38382268',\n",
       "  '0.44256034',\n",
       "  '0.2797972',\n",
       "  '0.4189089',\n",
       "  '0.28489992',\n",
       "  '0.29170376',\n",
       "  '0.34984076',\n",
       "  '0.28943956',\n",
       "  '0.31487468',\n",
       "  '0.31783134',\n",
       "  '0.47797242',\n",
       "  '0.20843524',\n",
       "  '0.32044414',\n",
       "  '0.24360837',\n",
       "  '0.2590882',\n",
       "  '0.34492305',\n",
       "  '0.26852778',\n",
       "  '0.2797212',\n",
       "  '0.25787795',\n",
       "  '0.28100166',\n",
       "  '0.31837413',\n",
       "  '0.2987849',\n",
       "  '0.28818333',\n",
       "  '0.31524518',\n",
       "  '0.31314465',\n",
       "  '0.25057858',\n",
       "  '0.26591617',\n",
       "  '0.3203466',\n",
       "  '0.28139457',\n",
       "  '0.43676275',\n",
       "  '0.20874459',\n",
       "  '0.23253167',\n",
       "  '0.25765628',\n",
       "  '0.28911185',\n",
       "  '0.3432135',\n",
       "  '0.3301385',\n",
       "  '0.29162657',\n",
       "  '0.32688004',\n",
       "  '0.32565418',\n",
       "  '0.26807743',\n",
       "  '0.2727738',\n",
       "  '0.20277765',\n",
       "  '0.27769625',\n",
       "  '0.30130303',\n",
       "  '0.2976213',\n",
       "  '0.21283397',\n",
       "  '0.3332597',\n",
       "  '0.35653526',\n",
       "  '0.37254995',\n",
       "  '0.29842412',\n",
       "  '0.31080157',\n",
       "  '0.40618932',\n",
       "  '0.23146734',\n",
       "  '0.28154486',\n",
       "  '0.24364507',\n",
       "  '0.2626909',\n",
       "  '0.2868162',\n",
       "  '0.3740648',\n",
       "  '0.2968536',\n",
       "  '0.28615746',\n",
       "  '0.23888493',\n",
       "  '0.2956251',\n",
       "  '0.33535522',\n",
       "  '0.45772052',\n",
       "  '0.40397513',\n",
       "  '0.24134907',\n",
       "  '0.2788481',\n",
       "  '0.33763927',\n",
       "  '0.32516086',\n",
       "  '0.27832675',\n",
       "  '0.27116227',\n",
       "  '0.3355737',\n",
       "  '0.27019987',\n",
       "  '0.32703114',\n",
       "  '0.24350013',\n",
       "  '0.26483822',\n",
       "  '0.21601753',\n",
       "  '0.3324865',\n",
       "  '0.26352763',\n",
       "  '0.2610611',\n",
       "  '0.33645165',\n",
       "  '0.32455558',\n",
       "  '0.36416',\n",
       "  '0.23450583',\n",
       "  '0.34161168',\n",
       "  '0.22259748',\n",
       "  '0.29201812',\n",
       "  '0.23060477',\n",
       "  '0.3055513',\n",
       "  '0.31695706',\n",
       "  '0.2625891',\n",
       "  '0.33510184',\n",
       "  '0.2895735',\n",
       "  '0.3767932',\n",
       "  '0.40889814',\n",
       "  '0.3675872',\n",
       "  '0.25545308',\n",
       "  '0.24895765',\n",
       "  '0.34117982',\n",
       "  '0.4026085',\n",
       "  '0.33373296',\n",
       "  '0.29435048',\n",
       "  '0.2515877',\n",
       "  '0.4753304',\n",
       "  '0.24445418',\n",
       "  '0.280235',\n",
       "  '0.21691035',\n",
       "  '0.31046826',\n",
       "  '0.35865813',\n",
       "  '0.43517226',\n",
       "  '0.33334273',\n",
       "  '0.47551924',\n",
       "  '0.3765791',\n",
       "  '0.4560753',\n",
       "  '0.3156268',\n",
       "  '0.27607298',\n",
       "  '0.39153892',\n",
       "  '0.18837401',\n",
       "  '0.38506642',\n",
       "  '0.3844924',\n",
       "  '0.3439678',\n",
       "  '0.33311915',\n",
       "  '0.26013732',\n",
       "  '0.27470595',\n",
       "  '0.29311112',\n",
       "  '0.24099085',\n",
       "  '0.30524144',\n",
       "  '0.24326527',\n",
       "  '0.3513563',\n",
       "  '0.2762559',\n",
       "  '0.33511013',\n",
       "  '0.28369844',\n",
       "  '0.31455588',\n",
       "  '0.40339115',\n",
       "  '0.36948103',\n",
       "  '0.2606451',\n",
       "  '0.32691133',\n",
       "  '0.31331623',\n",
       "  '0.2777984',\n",
       "  '0.2353974',\n",
       "  '0.3730088',\n",
       "  '0.2145459',\n",
       "  '0.30136678',\n",
       "  '0.32682428',\n",
       "  '0.35907602',\n",
       "  '0.28136745',\n",
       "  '0.28989848',\n",
       "  '0.2747507',\n",
       "  '0.30271798',\n",
       "  '0.40265375',\n",
       "  '0.3156038',\n",
       "  '0.22117871',\n",
       "  '0.40566355',\n",
       "  '0.38568592',\n",
       "  '0.26492944',\n",
       "  '0.2602617',\n",
       "  '0.30575764',\n",
       "  '0.35681403',\n",
       "  '0.30123332',\n",
       "  '0.29437655',\n",
       "  '0.33077163',\n",
       "  '0.3119639',\n",
       "  '0.42305517',\n",
       "  '0.36508277',\n",
       "  '0.39076746',\n",
       "  '0.29486674',\n",
       "  '0.41969544',\n",
       "  '0.18912804',\n",
       "  '0.47612485',\n",
       "  '0.36221796',\n",
       "  '0.34131354',\n",
       "  '0.34240538',\n",
       "  '0.3074278',\n",
       "  '0.24805981',\n",
       "  '0.3628897',\n",
       "  '0.32332557',\n",
       "  '0.20199189',\n",
       "  '0.29190844',\n",
       "  '0.29871345',\n",
       "  '0.47262534',\n",
       "  '0.39985466',\n",
       "  '0.31904316',\n",
       "  '0.28701422',\n",
       "  '0.25439805',\n",
       "  '0.23329544',\n",
       "  '0.36058065',\n",
       "  '0.23312539',\n",
       "  '0.32919526',\n",
       "  '0.30658862',\n",
       "  '0.3707201',\n",
       "  '0.23619862',\n",
       "  '0.26670617',\n",
       "  '0.34704727',\n",
       "  '0.24518758',\n",
       "  '0.25301164',\n",
       "  '0.1941348',\n",
       "  '0.3247269',\n",
       "  '0.2569636',\n",
       "  '0.28200224',\n",
       "  '0.30041692',\n",
       "  '0.33307567',\n",
       "  '0.30719572',\n",
       "  '0.26120186',\n",
       "  '0.28649992',\n",
       "  '0.28678983',\n",
       "  '0.267538',\n",
       "  '0.32355645',\n",
       "  '0.347895',\n",
       "  '0.3422288',\n",
       "  '0.30922922',\n",
       "  '0.3406183',\n",
       "  '0.2999219',\n",
       "  '0.35403934',\n",
       "  '0.42691666',\n",
       "  '0.35436487',\n",
       "  '0.26786065',\n",
       "  '0.30323857',\n",
       "  '0.2769214',\n",
       "  '0.31779212',\n",
       "  '0.24462289',\n",
       "  '0.236493',\n",
       "  '0.33843738',\n",
       "  '0.2011362',\n",
       "  '0.3097539',\n",
       "  '0.37337747',\n",
       "  '0.3108744',\n",
       "  '0.42555058',\n",
       "  '0.3196643',\n",
       "  '0.29894102',\n",
       "  '0.3245256',\n",
       "  '0.36668938',\n",
       "  '0.35368443',\n",
       "  '0.45130524',\n",
       "  '0.3216744',\n",
       "  '0.3363558',\n",
       "  '0.2435893',\n",
       "  '0.3662659',\n",
       "  '0.360331',\n",
       "  '0.3742556',\n",
       "  '0.29785624',\n",
       "  '0.43818387',\n",
       "  '0.4290873',\n",
       "  '0.31999147',\n",
       "  '0.3795607',\n",
       "  '0.30811173',\n",
       "  '0.25685224',\n",
       "  '0.34821945',\n",
       "  '0.26017708',\n",
       "  '0.30934292',\n",
       "  '0.32317492',\n",
       "  '0.2978996',\n",
       "  '0.2879215',\n",
       "  '0.3811497',\n",
       "  '0.29552633',\n",
       "  '0.2898338',\n",
       "  '0.30652684',\n",
       "  '0.2736744',\n",
       "  '0.2810376',\n",
       "  '0.34628803',\n",
       "  '0.22622886',\n",
       "  '0.34913033',\n",
       "  '0.29399145',\n",
       "  '0.31006867',\n",
       "  '0.3193372',\n",
       "  '0.29610404',\n",
       "  '0.3049789',\n",
       "  '0.29558438',\n",
       "  '0.23480006',\n",
       "  '0.33623123',\n",
       "  '0.30451128',\n",
       "  '0.26098198',\n",
       "  '0.20212394',\n",
       "  '0.3131551',\n",
       "  '0.20788863',\n",
       "  '0.3137556',\n",
       "  '0.3347939',\n",
       "  '0.32490507',\n",
       "  '0.28458226',\n",
       "  '0.22325867',\n",
       "  '0.25694606',\n",
       "  '0.20164573',\n",
       "  '0.3136716',\n",
       "  '0.2856402',\n",
       "  '0.29843402',\n",
       "  '0.26716962',\n",
       "  '0.27055782',\n",
       "  '0.34948963',\n",
       "  '0.27597928',\n",
       "  '0.229858',\n",
       "  '0.29448035',\n",
       "  '0.18332608',\n",
       "  '0.24787523',\n",
       "  '0.23270002',\n",
       "  '0.29315028',\n",
       "  '0.34733546',\n",
       "  '0.29012585',\n",
       "  '0.2867856',\n",
       "  '0.24406102',\n",
       "  '0.35911065',\n",
       "  '0.28408945',\n",
       "  '0.29513115',\n",
       "  '0.27146608',\n",
       "  '0.32502776',\n",
       "  '0.2925427',\n",
       "  '0.23096532',\n",
       "  '0.36325026',\n",
       "  '0.28591657',\n",
       "  '0.51664203',\n",
       "  '0.2713466',\n",
       "  '0.29194158',\n",
       "  '0.4616148',\n",
       "  '0.3061484',\n",
       "  '0.2861686',\n",
       "  '0.29871213',\n",
       "  '0.38173804',\n",
       "  '0.38955012',\n",
       "  '0.22706924',\n",
       "  '0.35819626',\n",
       "  '0.31826305',\n",
       "  '0.26733685',\n",
       "  '0.26134592',\n",
       "  '0.32168594',\n",
       "  '0.32522297',\n",
       "  '0.35051435',\n",
       "  '0.21447347',\n",
       "  '0.25143838',\n",
       "  '0.28005415',\n",
       "  '0.25318548',\n",
       "  '0.29277885',\n",
       "  '0.3187726',\n",
       "  '0.36113757',\n",
       "  '0.30518717',\n",
       "  '0.2492125',\n",
       "  '0.33339715',\n",
       "  '0.19263151',\n",
       "  '0.36194998',\n",
       "  '0.31434575',\n",
       "  '0.2507339',\n",
       "  '0.26695168',\n",
       "  '0.27050382',\n",
       "  '0.31407842',\n",
       "  '0.38419205',\n",
       "  '0.23919678',\n",
       "  '0.3549154',\n",
       "  '0.35442695',\n",
       "  '0.34074968',\n",
       "  '0.32398093',\n",
       "  '0.31891152',\n",
       "  '0.37721902',\n",
       "  '0.26164815',\n",
       "  '0.22637478',\n",
       "  '0.2948779',\n",
       "  '0.24404661',\n",
       "  '0.35512722',\n",
       "  '0.31188297',\n",
       "  '0.27499664',\n",
       "  '0.4039907',\n",
       "  '0.2467089',\n",
       "  '0.30612183',\n",
       "  '0.3310845',\n",
       "  '0.2928507',\n",
       "  '0.27007613',\n",
       "  '0.270868',\n",
       "  '0.37272662',\n",
       "  '0.24435031',\n",
       "  '0.31065625',\n",
       "  '0.34004772',\n",
       "  '0.28405195',\n",
       "  '0.2502097',\n",
       "  '0.4134968',\n",
       "  '0.29003465',\n",
       "  '0.26826775',\n",
       "  '0.29774046',\n",
       "  '0.23981315',\n",
       "  '0.26050803',\n",
       "  '0.26066715',\n",
       "  '0.3586624',\n",
       "  '0.3973869',\n",
       "  '0.4038099',\n",
       "  '0.3851684',\n",
       "  ...],\n",
       " 'pred_max_max': ['NaN',\n",
       "  1422,\n",
       "  1014,\n",
       "  'NaN',\n",
       "  17,\n",
       "  7303,\n",
       "  3576,\n",
       "  8988,\n",
       "  44,\n",
       "  3526,\n",
       "  5708,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  4882,\n",
       "  'NaN',\n",
       "  5790,\n",
       "  2916,\n",
       "  86,\n",
       "  9935,\n",
       "  6107,\n",
       "  8561,\n",
       "  890,\n",
       "  1371,\n",
       "  138,\n",
       "  3389,\n",
       "  1184,\n",
       "  148,\n",
       "  149,\n",
       "  8599,\n",
       "  'NaN',\n",
       "  9225,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  179,\n",
       "  7949,\n",
       "  'NaN',\n",
       "  205,\n",
       "  'NaN',\n",
       "  229,\n",
       "  'NaN',\n",
       "  232,\n",
       "  233,\n",
       "  'NaN',\n",
       "  4728,\n",
       "  5159,\n",
       "  7420,\n",
       "  1534,\n",
       "  2197,\n",
       "  'NaN',\n",
       "  2543,\n",
       "  2531,\n",
       "  297,\n",
       "  'NaN',\n",
       "  4060,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  1581,\n",
       "  'NaN',\n",
       "  1392,\n",
       "  'NaN',\n",
       "  9641,\n",
       "  3034,\n",
       "  6544,\n",
       "  2222,\n",
       "  'NaN',\n",
       "  2075,\n",
       "  3805,\n",
       "  4998,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  1820,\n",
       "  1610,\n",
       "  2916,\n",
       "  5924,\n",
       "  6984,\n",
       "  693,\n",
       "  1098,\n",
       "  7970,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  7075,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  2044,\n",
       "  2091,\n",
       "  1214,\n",
       "  1493,\n",
       "  9782,\n",
       "  3827,\n",
       "  2018,\n",
       "  2829,\n",
       "  'NaN',\n",
       "  4342,\n",
       "  487,\n",
       "  2484,\n",
       "  4562,\n",
       "  4638,\n",
       "  'NaN',\n",
       "  1933,\n",
       "  554,\n",
       "  8856,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  6776,\n",
       "  576,\n",
       "  1807,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  4246,\n",
       "  606,\n",
       "  8372,\n",
       "  2028,\n",
       "  160,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  632,\n",
       "  4587,\n",
       "  1134,\n",
       "  5368,\n",
       "  2059,\n",
       "  8384,\n",
       "  'NaN',\n",
       "  642,\n",
       "  'NaN',\n",
       "  1441,\n",
       "  'NaN',\n",
       "  3408,\n",
       "  6980,\n",
       "  'NaN',\n",
       "  5514,\n",
       "  689,\n",
       "  'NaN',\n",
       "  6043,\n",
       "  7884,\n",
       "  5573,\n",
       "  'NaN',\n",
       "  2197,\n",
       "  564,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  5358,\n",
       "  743,\n",
       "  1534,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  3661,\n",
       "  6374,\n",
       "  803,\n",
       "  'NaN',\n",
       "  819,\n",
       "  3401,\n",
       "  590,\n",
       "  828,\n",
       "  6584,\n",
       "  4674,\n",
       "  2891,\n",
       "  591,\n",
       "  7360,\n",
       "  9669,\n",
       "  2075,\n",
       "  5573,\n",
       "  7198,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  64,\n",
       "  2590,\n",
       "  2316,\n",
       "  1445,\n",
       "  7423,\n",
       "  1308,\n",
       "  9875,\n",
       "  3261,\n",
       "  890,\n",
       "  5939,\n",
       "  'NaN',\n",
       "  2075,\n",
       "  1134,\n",
       "  'NaN',\n",
       "  9065,\n",
       "  'NaN',\n",
       "  8263,\n",
       "  6528,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  3806,\n",
       "  4959,\n",
       "  'NaN',\n",
       "  1031,\n",
       "  1033,\n",
       "  7677,\n",
       "  7331,\n",
       "  1049,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  1095,\n",
       "  1098,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  405,\n",
       "  4271,\n",
       "  4202,\n",
       "  2381,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  352,\n",
       "  6933,\n",
       "  'NaN',\n",
       "  2428,\n",
       "  1178,\n",
       "  'NaN',\n",
       "  2064,\n",
       "  3278,\n",
       "  3242,\n",
       "  1193,\n",
       "  1203,\n",
       "  20,\n",
       "  3856,\n",
       "  4728,\n",
       "  1218,\n",
       "  532,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  539,\n",
       "  6302,\n",
       "  2018,\n",
       "  1632,\n",
       "  7533,\n",
       "  'NaN',\n",
       "  8588,\n",
       "  'NaN',\n",
       "  7222,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  9017,\n",
       "  3577,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  6361,\n",
       "  8731,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  9047,\n",
       "  1861,\n",
       "  1346,\n",
       "  7540,\n",
       "  1360,\n",
       "  9110,\n",
       "  1357,\n",
       "  8185,\n",
       "  3881,\n",
       "  6022,\n",
       "  'NaN',\n",
       "  667,\n",
       "  2124,\n",
       "  1399,\n",
       "  101,\n",
       "  8561,\n",
       "  1408,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  1415,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  3399,\n",
       "  5734,\n",
       "  2793,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  2863,\n",
       "  1473,\n",
       "  6479,\n",
       "  352,\n",
       "  'NaN',\n",
       "  4865,\n",
       "  1445,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  587,\n",
       "  2059,\n",
       "  8306,\n",
       "  1239,\n",
       "  'NaN',\n",
       "  7120,\n",
       "  7989,\n",
       "  4406,\n",
       "  4359,\n",
       "  1569,\n",
       "  8469,\n",
       "  870,\n",
       "  2507,\n",
       "  7036,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  967,\n",
       "  'NaN',\n",
       "  9035,\n",
       "  'NaN',\n",
       "  7530,\n",
       "  1655,\n",
       "  2666,\n",
       "  1234,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  1573,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  5739,\n",
       "  'NaN',\n",
       "  1501,\n",
       "  1706,\n",
       "  5938,\n",
       "  3409,\n",
       "  8234,\n",
       "  4007,\n",
       "  8290,\n",
       "  'NaN',\n",
       "  2199,\n",
       "  'NaN',\n",
       "  4117,\n",
       "  7926,\n",
       "  4674,\n",
       "  3606,\n",
       "  9749,\n",
       "  'NaN',\n",
       "  8111,\n",
       "  7331,\n",
       "  9212,\n",
       "  1844,\n",
       "  1700,\n",
       "  1870,\n",
       "  'NaN',\n",
       "  5062,\n",
       "  3283,\n",
       "  1895,\n",
       "  7097,\n",
       "  1913,\n",
       "  'NaN',\n",
       "  8935,\n",
       "  'NaN',\n",
       "  1941,\n",
       "  1943,\n",
       "  1961,\n",
       "  3024,\n",
       "  3929,\n",
       "  1994,\n",
       "  7863,\n",
       "  'NaN',\n",
       "  2655,\n",
       "  'NaN',\n",
       "  2124,\n",
       "  8599,\n",
       "  2070,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  4379,\n",
       "  1738,\n",
       "  6305,\n",
       "  'NaN',\n",
       "  1475,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  2199,\n",
       "  5767,\n",
       "  8183,\n",
       "  7613,\n",
       "  4401,\n",
       "  736,\n",
       "  'NaN',\n",
       "  741,\n",
       "  8834,\n",
       "  2698,\n",
       "  3903,\n",
       "  9495,\n",
       "  2277,\n",
       "  352,\n",
       "  6116,\n",
       "  6049,\n",
       "  4036,\n",
       "  4589,\n",
       "  2315,\n",
       "  'NaN',\n",
       "  4530,\n",
       "  2324,\n",
       "  901,\n",
       "  6895,\n",
       "  2343,\n",
       "  2347,\n",
       "  'NaN',\n",
       "  3796,\n",
       "  'NaN',\n",
       "  2375,\n",
       "  894,\n",
       "  3640,\n",
       "  4473,\n",
       "  3880,\n",
       "  5497,\n",
       "  1214,\n",
       "  2444,\n",
       "  2028,\n",
       "  2507,\n",
       "  3439,\n",
       "  1214,\n",
       "  'NaN',\n",
       "  2547,\n",
       "  5484,\n",
       "  2623,\n",
       "  5083,\n",
       "  5083,\n",
       "  4779,\n",
       "  2572,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  3750,\n",
       "  2585,\n",
       "  7809,\n",
       "  1600,\n",
       "  2608,\n",
       "  2623,\n",
       "  4882,\n",
       "  4528,\n",
       "  2630,\n",
       "  4728,\n",
       "  9201,\n",
       "  2653,\n",
       "  1747,\n",
       "  'NaN',\n",
       "  1315,\n",
       "  5640,\n",
       "  155,\n",
       "  2728,\n",
       "  7441,\n",
       "  2748,\n",
       "  1371,\n",
       "  'NaN',\n",
       "  8274,\n",
       "  3931,\n",
       "  1617,\n",
       "  8834,\n",
       "  7703,\n",
       "  'NaN',\n",
       "  2814,\n",
       "  7530,\n",
       "  2343,\n",
       "  1610,\n",
       "  2852,\n",
       "  3713,\n",
       "  6584,\n",
       "  6420,\n",
       "  5005,\n",
       "  9949,\n",
       "  2916,\n",
       "  8496,\n",
       "  9507,\n",
       "  1503,\n",
       "  'NaN',\n",
       "  2962,\n",
       "  2971,\n",
       "  2972,\n",
       "  2977,\n",
       "  2996,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  918,\n",
       "  918,\n",
       "  8493,\n",
       "  9574,\n",
       "  64,\n",
       "  4841,\n",
       "  6490,\n",
       "  3819,\n",
       "  'NaN',\n",
       "  5141,\n",
       "  930,\n",
       "  'NaN',\n",
       "  4489,\n",
       "  9955,\n",
       "  3424,\n",
       "  3523,\n",
       "  1346,\n",
       "  'NaN',\n",
       "  1277,\n",
       "  'NaN',\n",
       "  5810,\n",
       "  4244,\n",
       "  8260,\n",
       "  8306,\n",
       "  7222,\n",
       "  'NaN',\n",
       "  7420,\n",
       "  4007,\n",
       "  2680,\n",
       "  'NaN',\n",
       "  6387,\n",
       "  'NaN',\n",
       "  9424,\n",
       "  'NaN',\n",
       "  2091,\n",
       "  2805,\n",
       "  'NaN',\n",
       "  5570,\n",
       "  'NaN',\n",
       "  2997,\n",
       "  'NaN',\n",
       "  3096,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  4726,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  2991,\n",
       "  5383,\n",
       "  7764,\n",
       "  1799,\n",
       "  8384,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  3327,\n",
       "  2770,\n",
       "  1882,\n",
       "  7420,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  2997,\n",
       "  'NaN',\n",
       "  3581,\n",
       "  886,\n",
       "  8731,\n",
       "  'NaN',\n",
       "  5368,\n",
       "  2103,\n",
       "  4557,\n",
       "  3931,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  5303,\n",
       "  7487,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  1445,\n",
       "  'NaN',\n",
       "  7573,\n",
       "  'NaN',\n",
       "  7863,\n",
       "  4627,\n",
       "  9524,\n",
       "  7542,\n",
       "  5408,\n",
       "  8550,\n",
       "  'NaN',\n",
       "  4244,\n",
       "  'NaN',\n",
       "  2246,\n",
       "  'NaN',\n",
       "  3535,\n",
       "  4743,\n",
       "  8168,\n",
       "  4627,\n",
       "  'NaN',\n",
       "  9474,\n",
       "  3951,\n",
       "  1862,\n",
       "  'NaN',\n",
       "  8844,\n",
       "  579,\n",
       "  1933,\n",
       "  1933,\n",
       "  1963,\n",
       "  9282,\n",
       "  6092,\n",
       "  867,\n",
       "  'NaN',\n",
       "  12,\n",
       "  3660,\n",
       "  3662,\n",
       "  3184,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  1399,\n",
       "  3700,\n",
       "  5561,\n",
       "  6480,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  3506,\n",
       "  'NaN',\n",
       "  3755,\n",
       "  9167,\n",
       "  'NaN',\n",
       "  2581,\n",
       "  6320,\n",
       "  6570,\n",
       "  'NaN',\n",
       "  7681,\n",
       "  4931,\n",
       "  742,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  3820,\n",
       "  5556,\n",
       "  3828,\n",
       "  3294,\n",
       "  6241,\n",
       "  'NaN',\n",
       "  3870,\n",
       "  1610,\n",
       "  3880,\n",
       "  8354,\n",
       "  2063,\n",
       "  4057,\n",
       "  9663,\n",
       "  6205,\n",
       "  691,\n",
       "  3976,\n",
       "  4562,\n",
       "  3979,\n",
       "  3752,\n",
       "  4438,\n",
       "  'NaN',\n",
       "  894,\n",
       "  607,\n",
       "  1862,\n",
       "  3620,\n",
       "  'NaN',\n",
       "  9209,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  4068,\n",
       "  'NaN',\n",
       "  5631,\n",
       "  'NaN',\n",
       "  9138,\n",
       "  4683,\n",
       "  'NaN',\n",
       "  8582,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  8259,\n",
       "  2858,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  8826,\n",
       "  7361,\n",
       "  'NaN',\n",
       "  2829,\n",
       "  2059,\n",
       "  8801,\n",
       "  8885,\n",
       "  9678,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  3518,\n",
       "  4184,\n",
       "  1444,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  4207,\n",
       "  1443,\n",
       "  3242,\n",
       "  8801,\n",
       "  6127,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  2691,\n",
       "  5525,\n",
       "  'NaN',\n",
       "  6194,\n",
       "  9683,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  9317,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  9579,\n",
       "  8026,\n",
       "  'NaN',\n",
       "  4350,\n",
       "  1008,\n",
       "  4386,\n",
       "  'NaN',\n",
       "  6018,\n",
       "  1066,\n",
       "  9493,\n",
       "  6419,\n",
       "  1174,\n",
       "  3546,\n",
       "  'NaN',\n",
       "  3743,\n",
       "  3986,\n",
       "  'NaN',\n",
       "  2316,\n",
       "  'NaN',\n",
       "  2124,\n",
       "  'NaN',\n",
       "  7638,\n",
       "  4292,\n",
       "  3359,\n",
       "  4501,\n",
       "  4502,\n",
       "  'NaN',\n",
       "  4505,\n",
       "  8834,\n",
       "  2829,\n",
       "  'NaN',\n",
       "  9251,\n",
       "  6716,\n",
       "  9583,\n",
       "  'NaN',\n",
       "  4555,\n",
       "  951,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  8168,\n",
       "  2410,\n",
       "  2410,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  6078,\n",
       "  2997,\n",
       "  4658,\n",
       "  4544,\n",
       "  4665,\n",
       "  4678,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  9882,\n",
       "  4701,\n",
       "  'NaN',\n",
       "  1572,\n",
       "  'NaN',\n",
       "  4985,\n",
       "  4753,\n",
       "  'NaN',\n",
       "  1593,\n",
       "  98,\n",
       "  4784,\n",
       "  3242,\n",
       "  2084,\n",
       "  7670,\n",
       "  8640,\n",
       "  352,\n",
       "  'NaN',\n",
       "  8354,\n",
       "  'NaN',\n",
       "  6018,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  8582,\n",
       "  7956,\n",
       "  4891,\n",
       "  4897,\n",
       "  4451,\n",
       "  8599,\n",
       "  'NaN',\n",
       "  1308,\n",
       "  4925,\n",
       "  2713,\n",
       "  7011,\n",
       "  6360,\n",
       "  'NaN',\n",
       "  8569,\n",
       "  4988,\n",
       "  5013,\n",
       "  'NaN',\n",
       "  3425,\n",
       "  'NaN',\n",
       "  2091,\n",
       "  3278,\n",
       "  4562,\n",
       "  3038,\n",
       "  1789,\n",
       "  2381,\n",
       "  2091,\n",
       "  2316,\n",
       "  5091,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  7965,\n",
       "  4562,\n",
       "  2834,\n",
       "  5117,\n",
       "  8821,\n",
       "  94,\n",
       "  6819,\n",
       "  'NaN',\n",
       "  5484,\n",
       "  806,\n",
       "  5165,\n",
       "  4373,\n",
       "  5556,\n",
       "  1550,\n",
       "  8959,\n",
       "  2091,\n",
       "  6083,\n",
       "  9259,\n",
       "  'NaN',\n",
       "  5211,\n",
       "  242,\n",
       "  5216,\n",
       "  9371,\n",
       "  5235,\n",
       "  5235,\n",
       "  4711,\n",
       "  2754,\n",
       "  999,\n",
       "  9949,\n",
       "  'NaN',\n",
       "  7256,\n",
       "  5278,\n",
       "  6275,\n",
       "  'NaN',\n",
       "  6768,\n",
       "  2284,\n",
       "  8664,\n",
       "  7794,\n",
       "  2091,\n",
       "  5368,\n",
       "  420,\n",
       "  2091,\n",
       "  2091,\n",
       "  5396,\n",
       "  9105,\n",
       "  1141,\n",
       "  5414,\n",
       "  5434,\n",
       "  5438,\n",
       "  5438,\n",
       "  375,\n",
       "  2180,\n",
       "  6428,\n",
       "  5453,\n",
       "  5031,\n",
       "  5460,\n",
       "  'NaN',\n",
       "  690,\n",
       "  2916,\n",
       "  9493,\n",
       "  5491,\n",
       "  4589,\n",
       "  2698,\n",
       "  3683,\n",
       "  3931,\n",
       "  8572,\n",
       "  2091,\n",
       "  2804,\n",
       "  2623,\n",
       "  3360,\n",
       "  5596,\n",
       "  3650,\n",
       "  5597,\n",
       "  5604,\n",
       "  2725,\n",
       "  5629,\n",
       "  5647,\n",
       "  'NaN',\n",
       "  4174,\n",
       "  1188,\n",
       "  5585,\n",
       "  4342,\n",
       "  8234,\n",
       "  5497,\n",
       "  7493,\n",
       "  5684,\n",
       "  6018,\n",
       "  9487,\n",
       "  8034,\n",
       "  4944,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  4042,\n",
       "  8731,\n",
       "  4104,\n",
       "  7528,\n",
       "  2091,\n",
       "  352,\n",
       "  352,\n",
       "  7120,\n",
       "  'NaN',\n",
       "  5811,\n",
       "  2930,\n",
       "  4227,\n",
       "  5841,\n",
       "  7794,\n",
       "  7863,\n",
       "  'NaN',\n",
       "  394,\n",
       "  5871,\n",
       "  242,\n",
       "  3871,\n",
       "  3712,\n",
       "  2829,\n",
       "  951,\n",
       "  5918,\n",
       "  4280,\n",
       "  'NaN',\n",
       "  8312,\n",
       "  6018,\n",
       "  4224,\n",
       "  3854,\n",
       "  7962,\n",
       "  7031,\n",
       "  6587,\n",
       "  1836,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  2770,\n",
       "  6017,\n",
       "  'NaN',\n",
       "  896,\n",
       "  2084,\n",
       "  'NaN',\n",
       "  7264,\n",
       "  6080,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  9824,\n",
       "  886,\n",
       "  'NaN',\n",
       "  6420,\n",
       "  4179,\n",
       "  5709,\n",
       "  'NaN',\n",
       "  9136,\n",
       "  9136,\n",
       "  6131,\n",
       "  9673,\n",
       "  'NaN',\n",
       "  747,\n",
       "  1336,\n",
       "  'NaN',\n",
       "  1247,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  8953,\n",
       "  'NaN',\n",
       "  6196,\n",
       "  5368,\n",
       "  2881,\n",
       "  489,\n",
       "  6214,\n",
       "  8032,\n",
       "  9167,\n",
       "  3566,\n",
       "  1120,\n",
       "  484,\n",
       "  'NaN',\n",
       "  996,\n",
       "  7364,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  8007,\n",
       "  2829,\n",
       "  1765,\n",
       "  6278,\n",
       "  9403,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  506,\n",
       "  5734,\n",
       "  6305,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  6338,\n",
       "  1593,\n",
       "  9229,\n",
       "  6373,\n",
       "  6373,\n",
       "  ...],\n",
       " 'bad_max_max': ['NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  'NaN',\n",
       "  False,\n",
       "  'NaN',\n",
       "  False,\n",
       "  'NaN',\n",
       "  False,\n",
       "  False,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  False,\n",
       "  False,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  False,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  False,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  ...],\n",
       " 'max_max_val': ['NaN',\n",
       "  0.34811079502105713,\n",
       "  0.28383156657218933,\n",
       "  'NaN',\n",
       "  0.26537662744522095,\n",
       "  0.30067959427833557,\n",
       "  0.23530806601047516,\n",
       "  0.23516100645065308,\n",
       "  0.29168856143951416,\n",
       "  0.28002065420150757,\n",
       "  0.33652037382125854,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2547963261604309,\n",
       "  'NaN',\n",
       "  0.3088163137435913,\n",
       "  0.2881833016872406,\n",
       "  0.31359684467315674,\n",
       "  0.27317875623703003,\n",
       "  0.30596432089805603,\n",
       "  0.24497924745082855,\n",
       "  0.2687322795391083,\n",
       "  0.2496815323829651,\n",
       "  0.27611789107322693,\n",
       "  0.2572678327560425,\n",
       "  0.330926775932312,\n",
       "  0.3777574598789215,\n",
       "  0.3271561861038208,\n",
       "  0.3269253075122833,\n",
       "  'NaN',\n",
       "  0.28235819935798645,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.3466193675994873,\n",
       "  0.2716144025325775,\n",
       "  'NaN',\n",
       "  0.32490962743759155,\n",
       "  'NaN',\n",
       "  0.43487149477005005,\n",
       "  'NaN',\n",
       "  0.29307225346565247,\n",
       "  0.3501582145690918,\n",
       "  'NaN',\n",
       "  0.27231913805007935,\n",
       "  0.3502696454524994,\n",
       "  0.3181798458099365,\n",
       "  0.28174516558647156,\n",
       "  0.27879074215888977,\n",
       "  'NaN',\n",
       "  0.26890063285827637,\n",
       "  0.36510390043258667,\n",
       "  0.3369140326976776,\n",
       "  'NaN',\n",
       "  0.28595516085624695,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.23788659274578094,\n",
       "  'NaN',\n",
       "  0.3572539687156677,\n",
       "  'NaN',\n",
       "  0.27306970953941345,\n",
       "  0.26639536023139954,\n",
       "  0.30862367153167725,\n",
       "  0.3837742805480957,\n",
       "  'NaN',\n",
       "  0.24608540534973145,\n",
       "  0.3457079231739044,\n",
       "  0.30151256918907166,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2736181318759918,\n",
       "  0.27279338240623474,\n",
       "  0.2687302827835083,\n",
       "  0.27520862221717834,\n",
       "  0.28709423542022705,\n",
       "  0.254904180765152,\n",
       "  0.2936819791793823,\n",
       "  0.22645694017410278,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.22034621238708496,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.28772297501564026,\n",
       "  0.3406207859516144,\n",
       "  0.22524072229862213,\n",
       "  0.3041146993637085,\n",
       "  0.2298256903886795,\n",
       "  0.2605277895927429,\n",
       "  0.31018421053886414,\n",
       "  0.24902838468551636,\n",
       "  'NaN',\n",
       "  0.27090299129486084,\n",
       "  0.3259624242782593,\n",
       "  0.30806824564933777,\n",
       "  0.2869250476360321,\n",
       "  0.3243767023086548,\n",
       "  'NaN',\n",
       "  0.31017112731933594,\n",
       "  0.35990819334983826,\n",
       "  0.34492939710617065,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2773876488208771,\n",
       "  0.35838931798934937,\n",
       "  0.2725445032119751,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.46072694659233093,\n",
       "  0.29450494050979614,\n",
       "  0.22446781396865845,\n",
       "  0.28399205207824707,\n",
       "  0.24999991059303284,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.3286482095718384,\n",
       "  0.24830439686775208,\n",
       "  0.3242253065109253,\n",
       "  0.3623272180557251,\n",
       "  0.30370408296585083,\n",
       "  0.32138076424598694,\n",
       "  'NaN',\n",
       "  0.22744351625442505,\n",
       "  'NaN',\n",
       "  0.2790215313434601,\n",
       "  'NaN',\n",
       "  0.27736881375312805,\n",
       "  0.25672316551208496,\n",
       "  'NaN',\n",
       "  0.24482357501983643,\n",
       "  0.411643922328949,\n",
       "  'NaN',\n",
       "  0.3348855674266815,\n",
       "  0.2619401216506958,\n",
       "  0.23493856191635132,\n",
       "  'NaN',\n",
       "  0.26956525444984436,\n",
       "  0.26433753967285156,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.23316030204296112,\n",
       "  0.29574882984161377,\n",
       "  0.23697897791862488,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.37680548429489136,\n",
       "  0.29438817501068115,\n",
       "  0.29841703176498413,\n",
       "  'NaN',\n",
       "  0.26660066843032837,\n",
       "  0.3048592209815979,\n",
       "  0.23388536274433136,\n",
       "  0.3037964105606079,\n",
       "  0.3330021798610687,\n",
       "  0.29309529066085815,\n",
       "  0.311186820268631,\n",
       "  0.28789642453193665,\n",
       "  0.23955227434635162,\n",
       "  0.31675148010253906,\n",
       "  0.2818254232406616,\n",
       "  0.29846760630607605,\n",
       "  0.35252368450164795,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2722904086112976,\n",
       "  0.2504436671733856,\n",
       "  0.31132444739341736,\n",
       "  0.30277174711227417,\n",
       "  0.27069294452667236,\n",
       "  0.28583335876464844,\n",
       "  0.20738442242145538,\n",
       "  0.2700220048427582,\n",
       "  0.24025431275367737,\n",
       "  0.2557404339313507,\n",
       "  'NaN',\n",
       "  0.2501946985721588,\n",
       "  0.32591286301612854,\n",
       "  'NaN',\n",
       "  0.23233163356781006,\n",
       "  'NaN',\n",
       "  0.27043309807777405,\n",
       "  0.3362754285335541,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.24994376301765442,\n",
       "  0.2903444766998291,\n",
       "  'NaN',\n",
       "  0.3034577965736389,\n",
       "  0.3116713762283325,\n",
       "  0.26797670125961304,\n",
       "  0.25962233543395996,\n",
       "  0.23980507254600525,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2758239805698395,\n",
       "  0.3401329517364502,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.31813520193099976,\n",
       "  0.30702438950538635,\n",
       "  0.27620598673820496,\n",
       "  0.3200289011001587,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.24276743829250336,\n",
       "  0.3415313959121704,\n",
       "  'NaN',\n",
       "  0.26482832431793213,\n",
       "  0.2969021201133728,\n",
       "  'NaN',\n",
       "  0.28926098346710205,\n",
       "  0.29789999127388,\n",
       "  0.24312710762023926,\n",
       "  0.28178149461746216,\n",
       "  0.32745495438575745,\n",
       "  0.2435954362154007,\n",
       "  0.26272279024124146,\n",
       "  0.27303385734558105,\n",
       "  0.3373481035232544,\n",
       "  0.20834314823150635,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2431253343820572,\n",
       "  0.29032427072525024,\n",
       "  0.24694006145000458,\n",
       "  0.2953304350376129,\n",
       "  0.28076380491256714,\n",
       "  'NaN',\n",
       "  0.4009849727153778,\n",
       "  'NaN',\n",
       "  0.2692916989326477,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.253758043050766,\n",
       "  0.26510846614837646,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.291717529296875,\n",
       "  0.2521984875202179,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2208484560251236,\n",
       "  0.29404038190841675,\n",
       "  0.25389397144317627,\n",
       "  0.35150283575057983,\n",
       "  0.3360685110092163,\n",
       "  0.2459706962108612,\n",
       "  0.2803400158882141,\n",
       "  0.28600916266441345,\n",
       "  0.3157225549221039,\n",
       "  0.28717470169067383,\n",
       "  'NaN',\n",
       "  0.288463830947876,\n",
       "  0.24621212482452393,\n",
       "  0.30695223808288574,\n",
       "  0.30331185460090637,\n",
       "  0.2517150342464447,\n",
       "  0.40411582589149475,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.34646347165107727,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2713727355003357,\n",
       "  0.26251164078712463,\n",
       "  0.27588945627212524,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.32010728120803833,\n",
       "  0.2733875811100006,\n",
       "  0.3334353268146515,\n",
       "  0.32366490364074707,\n",
       "  'NaN',\n",
       "  0.2371465265750885,\n",
       "  0.2297656238079071,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.27734553813934326,\n",
       "  0.2901631295681,\n",
       "  0.33715906739234924,\n",
       "  0.27624890208244324,\n",
       "  'NaN',\n",
       "  0.2550053000450134,\n",
       "  0.24985048174858093,\n",
       "  0.2710196077823639,\n",
       "  0.24966400861740112,\n",
       "  0.31402575969696045,\n",
       "  0.31787964701652527,\n",
       "  0.2694314122200012,\n",
       "  0.2640902101993561,\n",
       "  0.2801820635795593,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.3462446331977844,\n",
       "  'NaN',\n",
       "  0.2719985246658325,\n",
       "  'NaN',\n",
       "  0.29980260133743286,\n",
       "  0.3201284110546112,\n",
       "  0.28966251015663147,\n",
       "  0.2531546950340271,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.27682679891586304,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2850642800331116,\n",
       "  'NaN',\n",
       "  0.2500290870666504,\n",
       "  0.28549885749816895,\n",
       "  0.290054589509964,\n",
       "  0.22552905976772308,\n",
       "  0.3298388719558716,\n",
       "  0.3708055019378662,\n",
       "  0.24628899991512299,\n",
       "  'NaN',\n",
       "  0.25335919857025146,\n",
       "  'NaN',\n",
       "  0.22263965010643005,\n",
       "  0.2776693105697632,\n",
       "  0.2249433398246765,\n",
       "  0.31533750891685486,\n",
       "  0.3387627601623535,\n",
       "  'NaN',\n",
       "  0.25691360235214233,\n",
       "  0.37137705087661743,\n",
       "  0.30262675881385803,\n",
       "  0.3663446307182312,\n",
       "  0.2260333001613617,\n",
       "  0.29433614015579224,\n",
       "  'NaN',\n",
       "  0.27533918619155884,\n",
       "  0.27156996726989746,\n",
       "  0.28313738107681274,\n",
       "  0.3463085889816284,\n",
       "  0.27918949723243713,\n",
       "  'NaN',\n",
       "  0.27092796564102173,\n",
       "  'NaN',\n",
       "  0.43533211946487427,\n",
       "  0.24528029561042786,\n",
       "  0.30560553073883057,\n",
       "  0.29743435978889465,\n",
       "  0.3274936079978943,\n",
       "  0.3035365343093872,\n",
       "  0.3256373107433319,\n",
       "  'NaN',\n",
       "  0.34754595160484314,\n",
       "  'NaN',\n",
       "  0.26161402463912964,\n",
       "  0.303977906703949,\n",
       "  0.29256585240364075,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.28041550517082214,\n",
       "  0.2793194055557251,\n",
       "  0.255687415599823,\n",
       "  'NaN',\n",
       "  0.28199175000190735,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2977364957332611,\n",
       "  0.31575679779052734,\n",
       "  0.24742868542671204,\n",
       "  0.19978897273540497,\n",
       "  0.327608585357666,\n",
       "  0.2937370836734772,\n",
       "  'NaN',\n",
       "  0.26983004808425903,\n",
       "  0.30321750044822693,\n",
       "  0.33347955346107483,\n",
       "  0.25869014859199524,\n",
       "  0.21870212256908417,\n",
       "  0.40682798624038696,\n",
       "  0.23751187324523926,\n",
       "  0.2308538258075714,\n",
       "  0.3037283420562744,\n",
       "  0.2622770071029663,\n",
       "  0.29911327362060547,\n",
       "  0.36069604754447937,\n",
       "  'NaN',\n",
       "  0.2869391143321991,\n",
       "  0.3223620355129242,\n",
       "  0.29433101415634155,\n",
       "  0.2833590805530548,\n",
       "  0.2789953351020813,\n",
       "  0.31685373187065125,\n",
       "  'NaN',\n",
       "  0.2873939871788025,\n",
       "  'NaN',\n",
       "  0.29245445132255554,\n",
       "  0.26638928055763245,\n",
       "  0.3136777877807617,\n",
       "  0.2807808518409729,\n",
       "  0.2732866704463959,\n",
       "  0.2885207235813141,\n",
       "  0.2726157605648041,\n",
       "  0.32341471314430237,\n",
       "  0.29331251978874207,\n",
       "  0.3662169873714447,\n",
       "  0.24054569005966187,\n",
       "  0.24936720728874207,\n",
       "  'NaN',\n",
       "  0.39878177642822266,\n",
       "  0.30034786462783813,\n",
       "  0.3001919090747833,\n",
       "  0.27254822850227356,\n",
       "  0.2749771475791931,\n",
       "  0.28091126680374146,\n",
       "  0.3042498528957367,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2579815685749054,\n",
       "  0.3496936857700348,\n",
       "  0.46767836809158325,\n",
       "  0.3307749629020691,\n",
       "  0.37303104996681213,\n",
       "  0.26853805780410767,\n",
       "  0.2823830246925354,\n",
       "  0.2814517915248871,\n",
       "  0.31811633706092834,\n",
       "  0.2638011574745178,\n",
       "  0.27885037660598755,\n",
       "  0.28655102849006653,\n",
       "  0.44875025749206543,\n",
       "  'NaN',\n",
       "  0.27062588930130005,\n",
       "  0.3031126856803894,\n",
       "  0.2879149317741394,\n",
       "  0.41574105620384216,\n",
       "  0.2791421115398407,\n",
       "  0.36867618560791016,\n",
       "  0.3112635314464569,\n",
       "  'NaN',\n",
       "  0.3640492260456085,\n",
       "  0.28607916831970215,\n",
       "  0.32622531056404114,\n",
       "  0.41749557852745056,\n",
       "  0.23504731059074402,\n",
       "  'NaN',\n",
       "  0.2476409524679184,\n",
       "  0.2801143229007721,\n",
       "  0.2638282775878906,\n",
       "  0.22894585132598877,\n",
       "  0.3242122232913971,\n",
       "  0.2877405881881714,\n",
       "  0.39473462104797363,\n",
       "  0.2716841697692871,\n",
       "  0.28105252981185913,\n",
       "  0.33697187900543213,\n",
       "  0.27326369285583496,\n",
       "  0.23552638292312622,\n",
       "  0.25510281324386597,\n",
       "  0.2483653426170349,\n",
       "  'NaN',\n",
       "  0.4375700056552887,\n",
       "  0.2814202606678009,\n",
       "  0.3221137523651123,\n",
       "  0.3512158989906311,\n",
       "  0.29770100116729736,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.3587934970855713,\n",
       "  0.3104255795478821,\n",
       "  0.3031380772590637,\n",
       "  0.29396355152130127,\n",
       "  0.36808010935783386,\n",
       "  0.28331032395362854,\n",
       "  0.3039569854736328,\n",
       "  0.315677672624588,\n",
       "  'NaN',\n",
       "  0.2816779911518097,\n",
       "  0.28364112973213196,\n",
       "  'NaN',\n",
       "  0.2937585115432739,\n",
       "  0.3539828062057495,\n",
       "  0.2700124979019165,\n",
       "  0.22821131348609924,\n",
       "  0.257839173078537,\n",
       "  'NaN',\n",
       "  0.25693726539611816,\n",
       "  'NaN',\n",
       "  0.24790161848068237,\n",
       "  0.3717711269855499,\n",
       "  0.32249149680137634,\n",
       "  0.21502996981143951,\n",
       "  0.2769281566143036,\n",
       "  'NaN',\n",
       "  0.2643643021583557,\n",
       "  0.3691120743751526,\n",
       "  0.31146419048309326,\n",
       "  'NaN',\n",
       "  0.28408342599868774,\n",
       "  'NaN',\n",
       "  0.3961735665798187,\n",
       "  'NaN',\n",
       "  0.24261026084423065,\n",
       "  0.26935386657714844,\n",
       "  'NaN',\n",
       "  0.3149474859237671,\n",
       "  'NaN',\n",
       "  0.4529709815979004,\n",
       "  'NaN',\n",
       "  0.2577158510684967,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.3165968060493469,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.28051379323005676,\n",
       "  0.27279067039489746,\n",
       "  0.26484185457229614,\n",
       "  0.2761354446411133,\n",
       "  0.2881093919277191,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.3108903169631958,\n",
       "  0.2874191403388977,\n",
       "  0.2871943712234497,\n",
       "  0.28700917959213257,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.28568774461746216,\n",
       "  'NaN',\n",
       "  0.28311076760292053,\n",
       "  0.2702327370643616,\n",
       "  0.25704970955848694,\n",
       "  'NaN',\n",
       "  0.2586139738559723,\n",
       "  0.33648523688316345,\n",
       "  0.3242090940475464,\n",
       "  0.368256151676178,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.3016088306903839,\n",
       "  0.25600510835647583,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2687070071697235,\n",
       "  'NaN',\n",
       "  0.28329917788505554,\n",
       "  'NaN',\n",
       "  0.26503124833106995,\n",
       "  0.27367985248565674,\n",
       "  0.31350651383399963,\n",
       "  0.42353326082229614,\n",
       "  0.2586096525192261,\n",
       "  0.21023035049438477,\n",
       "  'NaN',\n",
       "  0.28751999139785767,\n",
       "  'NaN',\n",
       "  0.2915426790714264,\n",
       "  'NaN',\n",
       "  0.2754998207092285,\n",
       "  0.32779815793037415,\n",
       "  0.2669266164302826,\n",
       "  0.2837899625301361,\n",
       "  'NaN',\n",
       "  0.28405335545539856,\n",
       "  0.2680720090866089,\n",
       "  0.2981197237968445,\n",
       "  'NaN',\n",
       "  0.27638763189315796,\n",
       "  0.3026707172393799,\n",
       "  0.2904149293899536,\n",
       "  0.33577996492385864,\n",
       "  0.3139939308166504,\n",
       "  0.23666790127754211,\n",
       "  0.27406415343284607,\n",
       "  0.34362101554870605,\n",
       "  'NaN',\n",
       "  0.30236589908599854,\n",
       "  0.3105127513408661,\n",
       "  0.31272318959236145,\n",
       "  0.2463633418083191,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.28858858346939087,\n",
       "  0.4569759964942932,\n",
       "  0.40168267488479614,\n",
       "  0.360528826713562,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2535221576690674,\n",
       "  'NaN',\n",
       "  0.34932801127433777,\n",
       "  0.271467387676239,\n",
       "  'NaN',\n",
       "  0.25048360228538513,\n",
       "  0.24032381176948547,\n",
       "  0.2498205453157425,\n",
       "  'NaN',\n",
       "  0.27762892842292786,\n",
       "  0.2703412175178528,\n",
       "  0.2888570725917816,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.3407018780708313,\n",
       "  0.3250678777694702,\n",
       "  0.38043269515037537,\n",
       "  0.2543136775493622,\n",
       "  0.35399991273880005,\n",
       "  'NaN',\n",
       "  0.3854145407676697,\n",
       "  0.2826364040374756,\n",
       "  0.30981263518333435,\n",
       "  0.28244030475616455,\n",
       "  0.2715691924095154,\n",
       "  0.30778178572654724,\n",
       "  0.2638154625892639,\n",
       "  0.2807025909423828,\n",
       "  0.2784292697906494,\n",
       "  0.46373388171195984,\n",
       "  0.25382542610168457,\n",
       "  0.36703673005104065,\n",
       "  0.26041409373283386,\n",
       "  0.2466905266046524,\n",
       "  'NaN',\n",
       "  0.24432167410850525,\n",
       "  0.2859874665737152,\n",
       "  0.24363520741462708,\n",
       "  0.2695121169090271,\n",
       "  'NaN',\n",
       "  0.25243112444877625,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2502274215221405,\n",
       "  'NaN',\n",
       "  0.2751155197620392,\n",
       "  'NaN',\n",
       "  0.4507635831832886,\n",
       "  0.22171561419963837,\n",
       "  'NaN',\n",
       "  0.27381187677383423,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.27121734619140625,\n",
       "  0.281735897064209,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.22516530752182007,\n",
       "  0.28210750222206116,\n",
       "  'NaN',\n",
       "  0.2754353880882263,\n",
       "  0.24882599711418152,\n",
       "  0.2972642481327057,\n",
       "  0.36380138993263245,\n",
       "  0.3679790198802948,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.24835585057735443,\n",
       "  0.32236671447753906,\n",
       "  0.21836397051811218,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2835001051425934,\n",
       "  0.25847432017326355,\n",
       "  0.31860387325286865,\n",
       "  0.26966577768325806,\n",
       "  0.3743472993373871,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.26841869950294495,\n",
       "  0.3161645531654358,\n",
       "  'NaN',\n",
       "  0.2496279925107956,\n",
       "  0.2487977147102356,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.30241358280181885,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2570701539516449,\n",
       "  0.2598627507686615,\n",
       "  'NaN',\n",
       "  0.28684306144714355,\n",
       "  0.30669084191322327,\n",
       "  0.31668519973754883,\n",
       "  'NaN',\n",
       "  0.20436707139015198,\n",
       "  0.3088992238044739,\n",
       "  0.2237788885831833,\n",
       "  0.2749732732772827,\n",
       "  0.25229302048683167,\n",
       "  0.282787948846817,\n",
       "  'NaN',\n",
       "  0.26790302991867065,\n",
       "  0.32162535190582275,\n",
       "  'NaN',\n",
       "  0.3886859714984894,\n",
       "  'NaN',\n",
       "  0.2580585777759552,\n",
       "  'NaN',\n",
       "  0.4006667733192444,\n",
       "  0.33927708864212036,\n",
       "  0.2533843219280243,\n",
       "  0.32372307777404785,\n",
       "  0.348843514919281,\n",
       "  'NaN',\n",
       "  0.3153192400932312,\n",
       "  0.24700260162353516,\n",
       "  0.2763690650463104,\n",
       "  'NaN',\n",
       "  0.3282482326030731,\n",
       "  0.27706378698349,\n",
       "  0.39662298560142517,\n",
       "  'NaN',\n",
       "  0.4002089500427246,\n",
       "  0.30196887254714966,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.24913303554058075,\n",
       "  0.25574183464050293,\n",
       "  0.27082422375679016,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.24388465285301208,\n",
       "  0.2919733226299286,\n",
       "  0.3395332992076874,\n",
       "  0.26747941970825195,\n",
       "  0.31089842319488525,\n",
       "  0.3013685345649719,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2996356189250946,\n",
       "  0.3204120099544525,\n",
       "  'NaN',\n",
       "  0.2601979970932007,\n",
       "  'NaN',\n",
       "  0.30884718894958496,\n",
       "  0.24081067740917206,\n",
       "  'NaN',\n",
       "  0.327086865901947,\n",
       "  0.28834637999534607,\n",
       "  0.34574174880981445,\n",
       "  0.28168851137161255,\n",
       "  0.39310675859451294,\n",
       "  0.2659534811973572,\n",
       "  0.27440595626831055,\n",
       "  0.2867775857448578,\n",
       "  'NaN',\n",
       "  0.36345386505126953,\n",
       "  'NaN',\n",
       "  0.2163991928100586,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.28360462188720703,\n",
       "  0.27389803528785706,\n",
       "  0.35095450282096863,\n",
       "  0.27643343806266785,\n",
       "  0.2981756627559662,\n",
       "  0.29396694898605347,\n",
       "  'NaN',\n",
       "  0.36857670545578003,\n",
       "  0.32438939809799194,\n",
       "  0.3215642273426056,\n",
       "  0.2517675757408142,\n",
       "  0.3137459456920624,\n",
       "  'NaN',\n",
       "  0.4066252112388611,\n",
       "  0.4355822801589966,\n",
       "  0.3104875683784485,\n",
       "  'NaN',\n",
       "  0.2734031677246094,\n",
       "  'NaN',\n",
       "  0.3075387477874756,\n",
       "  0.2876276969909668,\n",
       "  0.23243962228298187,\n",
       "  0.3769099712371826,\n",
       "  0.2764585614204407,\n",
       "  0.40943604707717896,\n",
       "  0.40321823954582214,\n",
       "  0.34138375520706177,\n",
       "  0.297986775636673,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.3055344223976135,\n",
       "  0.20175695419311523,\n",
       "  0.2970215082168579,\n",
       "  0.26669812202453613,\n",
       "  0.2992768883705139,\n",
       "  0.2592780292034149,\n",
       "  0.27380990982055664,\n",
       "  'NaN',\n",
       "  0.2737857699394226,\n",
       "  0.2849459648132324,\n",
       "  0.2642722725868225,\n",
       "  0.217670276761055,\n",
       "  0.27785372734069824,\n",
       "  0.26931050419807434,\n",
       "  0.3186340034008026,\n",
       "  0.31389206647872925,\n",
       "  0.3028234839439392,\n",
       "  0.2457604557275772,\n",
       "  'NaN',\n",
       "  0.35112446546554565,\n",
       "  0.25888437032699585,\n",
       "  0.34539762139320374,\n",
       "  0.33428168296813965,\n",
       "  0.36072322726249695,\n",
       "  0.34696972370147705,\n",
       "  0.30606937408447266,\n",
       "  0.2812096178531647,\n",
       "  0.29583311080932617,\n",
       "  0.42262184619903564,\n",
       "  'NaN',\n",
       "  0.19558607041835785,\n",
       "  0.35730504989624023,\n",
       "  0.29831093549728394,\n",
       "  'NaN',\n",
       "  0.2803303897380829,\n",
       "  0.2530716061592102,\n",
       "  0.32937881350517273,\n",
       "  0.31114834547042847,\n",
       "  0.2848424017429352,\n",
       "  0.3755975365638733,\n",
       "  0.23211826384067535,\n",
       "  0.42778903245925903,\n",
       "  0.31606751680374146,\n",
       "  0.2758500874042511,\n",
       "  0.2761494517326355,\n",
       "  0.3320021331310272,\n",
       "  0.3594212532043457,\n",
       "  0.39115583896636963,\n",
       "  0.36144018173217773,\n",
       "  0.2975727617740631,\n",
       "  0.28606143593788147,\n",
       "  0.2611736059188843,\n",
       "  0.35555851459503174,\n",
       "  0.3257094919681549,\n",
       "  0.2531108260154724,\n",
       "  0.30812203884124756,\n",
       "  'NaN',\n",
       "  0.262786328792572,\n",
       "  0.3905293941497803,\n",
       "  0.27963170409202576,\n",
       "  0.26481756567955017,\n",
       "  0.27804994583129883,\n",
       "  0.2768803536891937,\n",
       "  0.25078755617141724,\n",
       "  0.3012655973434448,\n",
       "  0.2937473654747009,\n",
       "  0.22264394164085388,\n",
       "  0.3024938404560089,\n",
       "  0.2643783390522003,\n",
       "  0.2606373429298401,\n",
       "  0.23908504843711853,\n",
       "  0.33099403977394104,\n",
       "  0.3058856427669525,\n",
       "  0.29433172941207886,\n",
       "  0.23859384655952454,\n",
       "  0.3518543839454651,\n",
       "  0.2855622172355652,\n",
       "  'NaN',\n",
       "  0.34326112270355225,\n",
       "  0.26363465189933777,\n",
       "  0.27082401514053345,\n",
       "  0.28083088994026184,\n",
       "  0.2785992920398712,\n",
       "  0.3156447410583496,\n",
       "  0.3395777642726898,\n",
       "  0.2598380446434021,\n",
       "  0.20811669528484344,\n",
       "  0.27669090032577515,\n",
       "  0.20793217420578003,\n",
       "  0.30432480573654175,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2753106355667114,\n",
       "  0.2070622444152832,\n",
       "  0.3045373260974884,\n",
       "  0.226423978805542,\n",
       "  0.3339316248893738,\n",
       "  0.27507758140563965,\n",
       "  0.2876594066619873,\n",
       "  0.24980539083480835,\n",
       "  'NaN',\n",
       "  0.26797619462013245,\n",
       "  0.27012884616851807,\n",
       "  0.2577275335788727,\n",
       "  0.2711290419101715,\n",
       "  0.2655278146266937,\n",
       "  0.2636248469352722,\n",
       "  'NaN',\n",
       "  0.3002873361110687,\n",
       "  0.33323463797569275,\n",
       "  0.3067617118358612,\n",
       "  0.2708760201931,\n",
       "  0.2831142246723175,\n",
       "  0.32200247049331665,\n",
       "  0.2527918517589569,\n",
       "  0.31677281856536865,\n",
       "  0.250698983669281,\n",
       "  'NaN',\n",
       "  0.25595173239707947,\n",
       "  0.2440330982208252,\n",
       "  0.3258481025695801,\n",
       "  0.2631896138191223,\n",
       "  0.40274590253829956,\n",
       "  0.29810941219329834,\n",
       "  0.27638816833496094,\n",
       "  0.44729769229888916,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.35084253549575806,\n",
       "  0.3181929290294647,\n",
       "  'NaN',\n",
       "  0.35907796025276184,\n",
       "  0.3087857961654663,\n",
       "  'NaN',\n",
       "  0.28639912605285645,\n",
       "  0.36709752678871155,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.23325584828853607,\n",
       "  0.3021697998046875,\n",
       "  'NaN',\n",
       "  0.3382003605365753,\n",
       "  0.3565421402454376,\n",
       "  0.3270149827003479,\n",
       "  'NaN',\n",
       "  0.263519287109375,\n",
       "  0.22461888194084167,\n",
       "  0.3666601777076721,\n",
       "  0.2151242345571518,\n",
       "  'NaN',\n",
       "  0.32061225175857544,\n",
       "  0.23264461755752563,\n",
       "  'NaN',\n",
       "  0.24946177005767822,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2504074275493622,\n",
       "  'NaN',\n",
       "  0.33556365966796875,\n",
       "  0.29695773124694824,\n",
       "  0.2874802350997925,\n",
       "  0.28565722703933716,\n",
       "  0.4044515788555145,\n",
       "  0.27995654940605164,\n",
       "  0.25584179162979126,\n",
       "  0.3069903552532196,\n",
       "  0.25309324264526367,\n",
       "  0.26350143551826477,\n",
       "  'NaN',\n",
       "  0.27053964138031006,\n",
       "  0.31212934851646423,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.29024824500083923,\n",
       "  0.2960096597671509,\n",
       "  0.3346787393093109,\n",
       "  0.3016630709171295,\n",
       "  0.3178483843803406,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.36970171332359314,\n",
       "  0.2730577886104584,\n",
       "  0.3163450062274933,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2811586558818817,\n",
       "  0.28124862909317017,\n",
       "  0.34548574686050415,\n",
       "  0.35485517978668213,\n",
       "  0.3746485412120819,\n",
       "  ...],\n",
       " 'sim_max_max': ['NaN',\n",
       "  0.42919788,\n",
       "  0.33405167,\n",
       "  'NaN',\n",
       "  0.2897963,\n",
       "  0.31270564,\n",
       "  0.3012818,\n",
       "  0.23942345,\n",
       "  0.2730968,\n",
       "  0.34626275,\n",
       "  0.3207671,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.24300031,\n",
       "  'NaN',\n",
       "  0.28940135,\n",
       "  0.2880761,\n",
       "  0.3890313,\n",
       "  0.2695074,\n",
       "  0.32845914,\n",
       "  0.38469598,\n",
       "  0.27744195,\n",
       "  0.29226097,\n",
       "  0.30783913,\n",
       "  0.26767415,\n",
       "  0.32597142,\n",
       "  0.31122804,\n",
       "  0.33221495,\n",
       "  0.38904876,\n",
       "  'NaN',\n",
       "  0.2797945,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.36337793,\n",
       "  0.2460793,\n",
       "  'NaN',\n",
       "  0.34958708,\n",
       "  'NaN',\n",
       "  0.44238305,\n",
       "  'NaN',\n",
       "  0.37488732,\n",
       "  0.327809,\n",
       "  'NaN',\n",
       "  0.29062805,\n",
       "  0.43066204,\n",
       "  0.30620003,\n",
       "  0.32864124,\n",
       "  0.2786783,\n",
       "  'NaN',\n",
       "  0.31939608,\n",
       "  0.41817093,\n",
       "  0.39443552,\n",
       "  'NaN',\n",
       "  0.31228778,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.25360644,\n",
       "  'NaN',\n",
       "  0.38613564,\n",
       "  'NaN',\n",
       "  0.29485366,\n",
       "  0.282371,\n",
       "  0.33644602,\n",
       "  0.41094923,\n",
       "  'NaN',\n",
       "  0.25673977,\n",
       "  0.38006932,\n",
       "  0.32810998,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.30890834,\n",
       "  0.2746388,\n",
       "  0.26503813,\n",
       "  0.3261434,\n",
       "  0.32644165,\n",
       "  0.21490486,\n",
       "  0.27687544,\n",
       "  0.21412744,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.21098179,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.3102197,\n",
       "  0.37307107,\n",
       "  0.26099735,\n",
       "  0.3514688,\n",
       "  0.23359475,\n",
       "  0.3052403,\n",
       "  0.36349416,\n",
       "  0.24912369,\n",
       "  'NaN',\n",
       "  0.2386936,\n",
       "  0.39314497,\n",
       "  0.35239452,\n",
       "  0.32929462,\n",
       "  0.39057454,\n",
       "  'NaN',\n",
       "  0.3696918,\n",
       "  0.37468138,\n",
       "  0.38148302,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2954437,\n",
       "  0.35047406,\n",
       "  0.25331587,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.4964652,\n",
       "  0.3097785,\n",
       "  0.22970791,\n",
       "  0.32677287,\n",
       "  0.27549845,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.39492097,\n",
       "  0.24773984,\n",
       "  0.377431,\n",
       "  0.38523132,\n",
       "  0.30058914,\n",
       "  0.35515726,\n",
       "  'NaN',\n",
       "  0.23323749,\n",
       "  'NaN',\n",
       "  0.31885198,\n",
       "  'NaN',\n",
       "  0.28736356,\n",
       "  0.27432442,\n",
       "  'NaN',\n",
       "  0.26152945,\n",
       "  0.4794932,\n",
       "  'NaN',\n",
       "  0.32070696,\n",
       "  0.23904467,\n",
       "  0.26432246,\n",
       "  'NaN',\n",
       "  0.28845897,\n",
       "  0.24550444,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.23314406,\n",
       "  0.31232917,\n",
       "  0.32076818,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.37956688,\n",
       "  0.3640293,\n",
       "  0.29446742,\n",
       "  'NaN',\n",
       "  0.35009798,\n",
       "  0.31935558,\n",
       "  0.28108823,\n",
       "  0.3510899,\n",
       "  0.2910222,\n",
       "  0.31902745,\n",
       "  0.32838637,\n",
       "  0.35630265,\n",
       "  0.25808924,\n",
       "  0.33460182,\n",
       "  0.3022717,\n",
       "  0.32509726,\n",
       "  0.40920198,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2493746,\n",
       "  0.25527173,\n",
       "  0.33082065,\n",
       "  0.3082361,\n",
       "  0.262343,\n",
       "  0.28383452,\n",
       "  0.14797795,\n",
       "  0.33327183,\n",
       "  0.25913364,\n",
       "  0.28244925,\n",
       "  'NaN',\n",
       "  0.26995844,\n",
       "  0.38975132,\n",
       "  'NaN',\n",
       "  0.21329029,\n",
       "  'NaN',\n",
       "  0.34653068,\n",
       "  0.33906215,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.27284795,\n",
       "  0.31603062,\n",
       "  'NaN',\n",
       "  0.29373968,\n",
       "  0.32115573,\n",
       "  0.29068533,\n",
       "  0.2619921,\n",
       "  0.28301018,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.30822763,\n",
       "  0.2764837,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.30077255,\n",
       "  0.2942241,\n",
       "  0.34086865,\n",
       "  0.41187292,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.30107766,\n",
       "  0.3645322,\n",
       "  'NaN',\n",
       "  0.3329685,\n",
       "  0.30347466,\n",
       "  'NaN',\n",
       "  0.34985167,\n",
       "  0.2903385,\n",
       "  0.2704966,\n",
       "  0.3420257,\n",
       "  0.3400636,\n",
       "  0.26472062,\n",
       "  0.31659257,\n",
       "  0.27585113,\n",
       "  0.3361743,\n",
       "  0.21533923,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.3220573,\n",
       "  0.29153776,\n",
       "  0.34737396,\n",
       "  0.40744388,\n",
       "  0.3363231,\n",
       "  'NaN',\n",
       "  0.4366682,\n",
       "  'NaN',\n",
       "  0.30743077,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2666819,\n",
       "  0.2880531,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.31372893,\n",
       "  0.18157177,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.21861798,\n",
       "  0.3251792,\n",
       "  0.28037977,\n",
       "  0.38446534,\n",
       "  0.34908336,\n",
       "  0.3259037,\n",
       "  0.3442021,\n",
       "  0.28839892,\n",
       "  0.30433917,\n",
       "  0.35843328,\n",
       "  'NaN',\n",
       "  0.31920004,\n",
       "  0.2603269,\n",
       "  0.31763458,\n",
       "  0.32730988,\n",
       "  0.31902844,\n",
       "  0.38045222,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.28057295,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2990048,\n",
       "  0.2331365,\n",
       "  0.24532928,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.27400666,\n",
       "  0.30425704,\n",
       "  0.31776196,\n",
       "  0.34592313,\n",
       "  'NaN',\n",
       "  0.27189776,\n",
       "  0.22268502,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.3202541,\n",
       "  0.27931714,\n",
       "  0.3919869,\n",
       "  0.2822835,\n",
       "  'NaN',\n",
       "  0.27936578,\n",
       "  0.27697474,\n",
       "  0.32741898,\n",
       "  0.23495914,\n",
       "  0.361108,\n",
       "  0.37812737,\n",
       "  0.29576147,\n",
       "  0.3167851,\n",
       "  0.34802634,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.3816577,\n",
       "  'NaN',\n",
       "  0.32951587,\n",
       "  'NaN',\n",
       "  0.365296,\n",
       "  0.2833731,\n",
       "  0.34692448,\n",
       "  0.22145243,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.32449692,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.3263717,\n",
       "  'NaN',\n",
       "  0.22700375,\n",
       "  0.42510986,\n",
       "  0.29435647,\n",
       "  0.25760606,\n",
       "  0.32625878,\n",
       "  0.3971883,\n",
       "  0.24240656,\n",
       "  'NaN',\n",
       "  0.25730222,\n",
       "  'NaN',\n",
       "  0.25551307,\n",
       "  0.28603753,\n",
       "  0.25256342,\n",
       "  0.27285,\n",
       "  0.3116881,\n",
       "  'NaN',\n",
       "  0.24418096,\n",
       "  0.3822407,\n",
       "  0.30791527,\n",
       "  0.34432387,\n",
       "  0.283866,\n",
       "  0.2996489,\n",
       "  'NaN',\n",
       "  0.30659392,\n",
       "  0.33627722,\n",
       "  0.31892896,\n",
       "  0.34300297,\n",
       "  0.27438688,\n",
       "  'NaN',\n",
       "  0.3113005,\n",
       "  'NaN',\n",
       "  0.40500164,\n",
       "  0.26817012,\n",
       "  0.38841057,\n",
       "  0.506462,\n",
       "  0.40564865,\n",
       "  0.35956433,\n",
       "  0.3382282,\n",
       "  'NaN',\n",
       "  0.36112288,\n",
       "  'NaN',\n",
       "  0.2935487,\n",
       "  0.31896445,\n",
       "  0.34138632,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.32332414,\n",
       "  0.30347967,\n",
       "  0.26295632,\n",
       "  'NaN',\n",
       "  0.2867322,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.39116323,\n",
       "  0.3432629,\n",
       "  0.28780174,\n",
       "  0.20755658,\n",
       "  0.39822257,\n",
       "  0.36975813,\n",
       "  'NaN',\n",
       "  0.33615524,\n",
       "  0.29279777,\n",
       "  0.402692,\n",
       "  0.27609342,\n",
       "  0.24968123,\n",
       "  0.3921818,\n",
       "  0.2440002,\n",
       "  0.25537318,\n",
       "  0.35285765,\n",
       "  0.2320166,\n",
       "  0.34574148,\n",
       "  0.399975,\n",
       "  'NaN',\n",
       "  0.3316747,\n",
       "  0.32598364,\n",
       "  0.31121433,\n",
       "  0.2839291,\n",
       "  0.26585525,\n",
       "  0.3195933,\n",
       "  'NaN',\n",
       "  0.39405167,\n",
       "  'NaN',\n",
       "  0.39321896,\n",
       "  0.28562474,\n",
       "  0.27174285,\n",
       "  0.35077915,\n",
       "  0.33941248,\n",
       "  0.29375443,\n",
       "  0.28842473,\n",
       "  0.3774132,\n",
       "  0.36894917,\n",
       "  0.39601234,\n",
       "  0.25882977,\n",
       "  0.23622316,\n",
       "  'NaN',\n",
       "  0.3471714,\n",
       "  0.34495753,\n",
       "  0.30122444,\n",
       "  0.30173647,\n",
       "  0.3099892,\n",
       "  0.3080901,\n",
       "  0.44400635,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.22572672,\n",
       "  0.3885445,\n",
       "  0.5182339,\n",
       "  0.34288478,\n",
       "  0.44129202,\n",
       "  0.30056113,\n",
       "  0.28803524,\n",
       "  0.33176947,\n",
       "  0.39641562,\n",
       "  0.2754938,\n",
       "  0.26891214,\n",
       "  0.30822778,\n",
       "  0.5170976,\n",
       "  'NaN',\n",
       "  0.265835,\n",
       "  0.34107387,\n",
       "  0.2901935,\n",
       "  0.4211655,\n",
       "  0.32545334,\n",
       "  0.36675033,\n",
       "  0.387542,\n",
       "  'NaN',\n",
       "  0.43510437,\n",
       "  0.33029515,\n",
       "  0.30885246,\n",
       "  0.40731505,\n",
       "  0.29258,\n",
       "  'NaN',\n",
       "  0.3204553,\n",
       "  0.28810403,\n",
       "  0.27126658,\n",
       "  0.19199178,\n",
       "  0.32030585,\n",
       "  0.3540587,\n",
       "  0.3554824,\n",
       "  0.3331316,\n",
       "  0.30363765,\n",
       "  0.34888595,\n",
       "  0.26896727,\n",
       "  0.23033306,\n",
       "  0.2571021,\n",
       "  0.25464153,\n",
       "  'NaN',\n",
       "  0.49661404,\n",
       "  0.32541454,\n",
       "  0.31778163,\n",
       "  0.32679513,\n",
       "  0.3132351,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.3987524,\n",
       "  0.35989535,\n",
       "  0.33058763,\n",
       "  0.4192761,\n",
       "  0.42407978,\n",
       "  0.31981918,\n",
       "  0.30354473,\n",
       "  0.34173042,\n",
       "  'NaN',\n",
       "  0.29160506,\n",
       "  0.28581294,\n",
       "  'NaN',\n",
       "  0.3322159,\n",
       "  0.39322013,\n",
       "  0.28518146,\n",
       "  0.22493446,\n",
       "  0.2604161,\n",
       "  'NaN',\n",
       "  0.34311634,\n",
       "  'NaN',\n",
       "  0.3193384,\n",
       "  0.41670442,\n",
       "  0.30924594,\n",
       "  0.24836105,\n",
       "  0.30012518,\n",
       "  'NaN',\n",
       "  0.22886707,\n",
       "  0.3589337,\n",
       "  0.33944917,\n",
       "  'NaN',\n",
       "  0.25312144,\n",
       "  'NaN',\n",
       "  0.3762917,\n",
       "  'NaN',\n",
       "  0.26109445,\n",
       "  0.26267856,\n",
       "  'NaN',\n",
       "  0.35056943,\n",
       "  'NaN',\n",
       "  0.45052004,\n",
       "  'NaN',\n",
       "  0.25255176,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.31504232,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.29949376,\n",
       "  0.31966275,\n",
       "  0.29411557,\n",
       "  0.24684352,\n",
       "  0.27127212,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.44256923,\n",
       "  0.26685825,\n",
       "  0.31160688,\n",
       "  0.2806307,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.23260292,\n",
       "  'NaN',\n",
       "  0.2973789,\n",
       "  0.34778914,\n",
       "  0.18692356,\n",
       "  'NaN',\n",
       "  0.2677798,\n",
       "  0.41987896,\n",
       "  0.3131996,\n",
       "  0.41554248,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.33619565,\n",
       "  0.2651207,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2927906,\n",
       "  'NaN',\n",
       "  0.34278327,\n",
       "  'NaN',\n",
       "  0.2890486,\n",
       "  0.24444672,\n",
       "  0.31944394,\n",
       "  0.4178871,\n",
       "  0.2508006,\n",
       "  0.2113796,\n",
       "  'NaN',\n",
       "  0.29852462,\n",
       "  'NaN',\n",
       "  0.32055002,\n",
       "  'NaN',\n",
       "  0.32388604,\n",
       "  0.34979504,\n",
       "  0.26073506,\n",
       "  0.2800395,\n",
       "  'NaN',\n",
       "  0.3454162,\n",
       "  0.29100192,\n",
       "  0.30546078,\n",
       "  'NaN',\n",
       "  0.30914515,\n",
       "  0.38268143,\n",
       "  0.2835514,\n",
       "  0.34145874,\n",
       "  0.35196212,\n",
       "  0.21391052,\n",
       "  0.24196362,\n",
       "  0.3405545,\n",
       "  'NaN',\n",
       "  0.3271253,\n",
       "  0.4104663,\n",
       "  0.31324914,\n",
       "  0.2704122,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.3352281,\n",
       "  0.49284625,\n",
       "  0.4385287,\n",
       "  0.3418998,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.28883827,\n",
       "  'NaN',\n",
       "  0.38050342,\n",
       "  0.23515138,\n",
       "  'NaN',\n",
       "  0.28843942,\n",
       "  0.24534321,\n",
       "  0.25332975,\n",
       "  'NaN',\n",
       "  0.2891686,\n",
       "  0.24702573,\n",
       "  0.31888792,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.36993366,\n",
       "  0.31223464,\n",
       "  0.38962403,\n",
       "  0.30925012,\n",
       "  0.40205395,\n",
       "  'NaN',\n",
       "  0.3681618,\n",
       "  0.2797972,\n",
       "  0.4189089,\n",
       "  0.26211423,\n",
       "  0.25512695,\n",
       "  0.34984076,\n",
       "  0.28943956,\n",
       "  0.3729003,\n",
       "  0.31783134,\n",
       "  0.43579257,\n",
       "  0.3622524,\n",
       "  0.3969326,\n",
       "  0.24249975,\n",
       "  0.2664804,\n",
       "  'NaN',\n",
       "  0.26852778,\n",
       "  0.2797212,\n",
       "  0.26597822,\n",
       "  0.275693,\n",
       "  'NaN',\n",
       "  0.22215304,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.26481074,\n",
       "  'NaN',\n",
       "  0.3203466,\n",
       "  'NaN',\n",
       "  0.43676275,\n",
       "  0.24108875,\n",
       "  'NaN',\n",
       "  0.2885541,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.30928254,\n",
       "  0.32565418,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.18723726,\n",
       "  0.24813822,\n",
       "  'NaN',\n",
       "  0.2976213,\n",
       "  0.22202022,\n",
       "  0.3332597,\n",
       "  0.35653526,\n",
       "  0.37254995,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.28295475,\n",
       "  0.40323314,\n",
       "  0.24364507,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.30751655,\n",
       "  0.2533055,\n",
       "  0.34130067,\n",
       "  0.29285762,\n",
       "  0.45772052,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2788481,\n",
       "  0.33763927,\n",
       "  'NaN',\n",
       "  0.27832675,\n",
       "  0.26296675,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.32703114,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.28400815,\n",
       "  0.3189112,\n",
       "  'NaN',\n",
       "  0.34556153,\n",
       "  0.37163988,\n",
       "  0.34694588,\n",
       "  'NaN',\n",
       "  0.023023678,\n",
       "  0.314201,\n",
       "  0.15132983,\n",
       "  0.3055513,\n",
       "  0.26445436,\n",
       "  0.3244292,\n",
       "  'NaN',\n",
       "  0.24082594,\n",
       "  0.3767932,\n",
       "  'NaN',\n",
       "  0.3675872,\n",
       "  'NaN',\n",
       "  0.27146626,\n",
       "  'NaN',\n",
       "  0.4026085,\n",
       "  0.33373296,\n",
       "  0.26590896,\n",
       "  0.36776862,\n",
       "  0.4753304,\n",
       "  'NaN',\n",
       "  0.353274,\n",
       "  0.21691035,\n",
       "  0.31046826,\n",
       "  'NaN',\n",
       "  0.43517226,\n",
       "  0.37252453,\n",
       "  0.47551924,\n",
       "  'NaN',\n",
       "  0.4560753,\n",
       "  0.3156268,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.26974088,\n",
       "  0.38506642,\n",
       "  0.3844924,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.27414036,\n",
       "  0.27470595,\n",
       "  0.4495355,\n",
       "  0.24099085,\n",
       "  0.32856238,\n",
       "  0.39405447,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.39519882,\n",
       "  0.40339115,\n",
       "  'NaN',\n",
       "  0.3214904,\n",
       "  'NaN',\n",
       "  0.2816672,\n",
       "  0.29070768,\n",
       "  'NaN',\n",
       "  0.3730088,\n",
       "  0.31229833,\n",
       "  0.34459168,\n",
       "  0.32061502,\n",
       "  0.35907602,\n",
       "  0.28136745,\n",
       "  0.3035932,\n",
       "  0.30560803,\n",
       "  'NaN',\n",
       "  0.3792914,\n",
       "  'NaN',\n",
       "  0.004028165,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.3045436,\n",
       "  0.34997562,\n",
       "  0.39292654,\n",
       "  0.3656634,\n",
       "  0.41372555,\n",
       "  0.33077163,\n",
       "  'NaN',\n",
       "  0.42305517,\n",
       "  0.32140195,\n",
       "  0.39076746,\n",
       "  0.25409806,\n",
       "  0.41969544,\n",
       "  'NaN',\n",
       "  0.47612485,\n",
       "  0.39493132,\n",
       "  0.37417215,\n",
       "  'NaN',\n",
       "  0.32278275,\n",
       "  'NaN',\n",
       "  0.3628897,\n",
       "  0.29448336,\n",
       "  0.3469607,\n",
       "  0.42520452,\n",
       "  0.32365423,\n",
       "  0.48673156,\n",
       "  0.39985466,\n",
       "  0.31904316,\n",
       "  0.32999346,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.2949974,\n",
       "  0.23312539,\n",
       "  0.32919526,\n",
       "  0.3815177,\n",
       "  0.3707201,\n",
       "  0.2809695,\n",
       "  0.27971047,\n",
       "  'NaN',\n",
       "  0.3480235,\n",
       "  0.37937677,\n",
       "  0.34688526,\n",
       "  0.16647545,\n",
       "  0.2569636,\n",
       "  0.3013792,\n",
       "  0.30041692,\n",
       "  0.33307567,\n",
       "  0.33932492,\n",
       "  0.26120186,\n",
       "  'NaN',\n",
       "  0.36670405,\n",
       "  0.21029814,\n",
       "  0.40169263,\n",
       "  0.347895,\n",
       "  0.2864788,\n",
       "  0.2923531,\n",
       "  0.3406183,\n",
       "  0.2999219,\n",
       "  0.35403934,\n",
       "  0.42691666,\n",
       "  'NaN',\n",
       "  0.20208511,\n",
       "  0.35671377,\n",
       "  0.32692248,\n",
       "  'NaN',\n",
       "  0.24462289,\n",
       "  0.24699149,\n",
       "  0.33843738,\n",
       "  0.20599727,\n",
       "  0.3097539,\n",
       "  0.4019287,\n",
       "  0.3108744,\n",
       "  0.42555058,\n",
       "  0.3196643,\n",
       "  0.35794693,\n",
       "  0.3245256,\n",
       "  0.36668938,\n",
       "  0.35368443,\n",
       "  0.4435967,\n",
       "  0.35976166,\n",
       "  0.3363558,\n",
       "  0.3236797,\n",
       "  0.30893597,\n",
       "  0.360331,\n",
       "  0.3742556,\n",
       "  0.29785624,\n",
       "  0.43818387,\n",
       "  'NaN',\n",
       "  0.47861943,\n",
       "  0.3795607,\n",
       "  0.28581572,\n",
       "  0.30558136,\n",
       "  0.34821945,\n",
       "  0.31397328,\n",
       "  0.30934292,\n",
       "  0.32317492,\n",
       "  0.2978996,\n",
       "  0.22314075,\n",
       "  0.34921807,\n",
       "  0.29474166,\n",
       "  0.25782862,\n",
       "  0.30652684,\n",
       "  0.3444552,\n",
       "  0.35901195,\n",
       "  0.3607096,\n",
       "  0.32291335,\n",
       "  0.34913033,\n",
       "  0.25849247,\n",
       "  'NaN',\n",
       "  0.40799525,\n",
       "  0.24572189,\n",
       "  0.3049789,\n",
       "  0.27664703,\n",
       "  0.23480006,\n",
       "  0.33628124,\n",
       "  0.3382891,\n",
       "  0.30046278,\n",
       "  0.043803472,\n",
       "  0.3131551,\n",
       "  0.20788863,\n",
       "  0.3137556,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.34085816,\n",
       "  0.1265185,\n",
       "  0.4118693,\n",
       "  0.34630513,\n",
       "  0.38718012,\n",
       "  0.2856402,\n",
       "  0.29843402,\n",
       "  0.2939943,\n",
       "  'NaN',\n",
       "  0.34948963,\n",
       "  0.27597928,\n",
       "  0.2640723,\n",
       "  0.3298033,\n",
       "  0.14475948,\n",
       "  0.24787523,\n",
       "  'NaN',\n",
       "  0.29594243,\n",
       "  0.34733546,\n",
       "  0.29012585,\n",
       "  0.2867856,\n",
       "  0.29371095,\n",
       "  0.35911065,\n",
       "  0.25374234,\n",
       "  0.3105453,\n",
       "  0.2877031,\n",
       "  'NaN',\n",
       "  0.269449,\n",
       "  0.06583424,\n",
       "  0.36325026,\n",
       "  0.28591657,\n",
       "  0.51664203,\n",
       "  0.3384047,\n",
       "  0.31795788,\n",
       "  0.4974862,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.38173804,\n",
       "  0.38955012,\n",
       "  'NaN',\n",
       "  0.35819626,\n",
       "  0.28982812,\n",
       "  'NaN',\n",
       "  0.3315196,\n",
       "  0.34526652,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.29506606,\n",
       "  0.37060326,\n",
       "  'NaN',\n",
       "  0.41940522,\n",
       "  0.41131014,\n",
       "  0.3187726,\n",
       "  'NaN',\n",
       "  0.35635126,\n",
       "  0.298338,\n",
       "  0.4005124,\n",
       "  0.16928595,\n",
       "  'NaN',\n",
       "  0.32778668,\n",
       "  0.2507339,\n",
       "  'NaN',\n",
       "  0.23559862,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.245823,\n",
       "  'NaN',\n",
       "  0.35442695,\n",
       "  0.2820776,\n",
       "  0.3457296,\n",
       "  0.3014718,\n",
       "  0.37721902,\n",
       "  0.356511,\n",
       "  0.22142196,\n",
       "  0.36495537,\n",
       "  0.29986578,\n",
       "  0.27676105,\n",
       "  'NaN',\n",
       "  0.29691312,\n",
       "  0.37923795,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.35079646,\n",
       "  0.2928507,\n",
       "  0.32542503,\n",
       "  0.3448186,\n",
       "  0.37272662,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.44559938,\n",
       "  0.2502097,\n",
       "  0.4134968,\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  'NaN',\n",
       "  0.30827615,\n",
       "  0.31966043,\n",
       "  0.3973869,\n",
       "  0.4038099,\n",
       "  0.3851684,\n",
       "  ...]}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dic['sim_soft'] = list(np.array(result_dic['sim_soft']))\n",
    "result_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(result_dic['bad_knn']), len(result_dic['bad_max_max'])\n",
    "import pandas as pd\n",
    "new = pd.DataFrame.from_dict(result_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.to_csv('new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>bad_knn</th>\n",
       "      <th>real_class</th>\n",
       "      <th>pred_knn</th>\n",
       "      <th>sim_real_cl</th>\n",
       "      <th>sim_best_knn</th>\n",
       "      <th>pred_soft</th>\n",
       "      <th>bad_soft</th>\n",
       "      <th>soft_val</th>\n",
       "      <th>sim_soft</th>\n",
       "      <th>pred_max_max</th>\n",
       "      <th>bad_max_max</th>\n",
       "      <th>max_max_val</th>\n",
       "      <th>sim_max_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>220</td>\n",
       "      <td>True</td>\n",
       "      <td>44</td>\n",
       "      <td>7272</td>\n",
       "      <td>0.263647</td>\n",
       "      <td>0.273097</td>\n",
       "      <td>44</td>\n",
       "      <td>False</td>\n",
       "      <td>0.276703</td>\n",
       "      <td>0.2767033</td>\n",
       "      <td>44</td>\n",
       "      <td>False</td>\n",
       "      <td>0.291689</td>\n",
       "      <td>0.273097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>434</td>\n",
       "      <td>True</td>\n",
       "      <td>86</td>\n",
       "      <td>5632</td>\n",
       "      <td>0.379282</td>\n",
       "      <td>0.389031</td>\n",
       "      <td>86</td>\n",
       "      <td>False</td>\n",
       "      <td>0.293346</td>\n",
       "      <td>0.29334617</td>\n",
       "      <td>86</td>\n",
       "      <td>False</td>\n",
       "      <td>0.313597</td>\n",
       "      <td>0.389031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>740</td>\n",
       "      <td>True</td>\n",
       "      <td>148</td>\n",
       "      <td>4400</td>\n",
       "      <td>0.297766</td>\n",
       "      <td>0.311228</td>\n",
       "      <td>148</td>\n",
       "      <td>False</td>\n",
       "      <td>0.403892</td>\n",
       "      <td>0.4038915</td>\n",
       "      <td>148</td>\n",
       "      <td>False</td>\n",
       "      <td>0.377757</td>\n",
       "      <td>0.311228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>899</td>\n",
       "      <td>True</td>\n",
       "      <td>179</td>\n",
       "      <td>754</td>\n",
       "      <td>0.350931</td>\n",
       "      <td>0.363378</td>\n",
       "      <td>179</td>\n",
       "      <td>False</td>\n",
       "      <td>0.340076</td>\n",
       "      <td>0.34007648</td>\n",
       "      <td>179</td>\n",
       "      <td>False</td>\n",
       "      <td>0.346619</td>\n",
       "      <td>0.363378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1149</td>\n",
       "      <td>True</td>\n",
       "      <td>229</td>\n",
       "      <td>1172</td>\n",
       "      <td>0.432248</td>\n",
       "      <td>0.442383</td>\n",
       "      <td>229</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420982</td>\n",
       "      <td>0.42098248</td>\n",
       "      <td>229</td>\n",
       "      <td>False</td>\n",
       "      <td>0.434871</td>\n",
       "      <td>0.442383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>48238</td>\n",
       "      <td>True</td>\n",
       "      <td>9647</td>\n",
       "      <td>7383</td>\n",
       "      <td>0.334514</td>\n",
       "      <td>0.338781</td>\n",
       "      <td>9647</td>\n",
       "      <td>False</td>\n",
       "      <td>0.383441</td>\n",
       "      <td>0.3834414</td>\n",
       "      <td>9647</td>\n",
       "      <td>False</td>\n",
       "      <td>0.354863</td>\n",
       "      <td>0.338781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>48555</td>\n",
       "      <td>True</td>\n",
       "      <td>9711</td>\n",
       "      <td>4302</td>\n",
       "      <td>0.219070</td>\n",
       "      <td>0.256552</td>\n",
       "      <td>9711</td>\n",
       "      <td>False</td>\n",
       "      <td>0.254419</td>\n",
       "      <td>0.25441867</td>\n",
       "      <td>9711</td>\n",
       "      <td>False</td>\n",
       "      <td>0.295279</td>\n",
       "      <td>0.256552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>48795</td>\n",
       "      <td>True</td>\n",
       "      <td>9759</td>\n",
       "      <td>1933</td>\n",
       "      <td>0.412487</td>\n",
       "      <td>0.420583</td>\n",
       "      <td>9759</td>\n",
       "      <td>False</td>\n",
       "      <td>0.394274</td>\n",
       "      <td>0.39427388</td>\n",
       "      <td>9759</td>\n",
       "      <td>False</td>\n",
       "      <td>0.366673</td>\n",
       "      <td>0.420583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>49308</td>\n",
       "      <td>True</td>\n",
       "      <td>9861</td>\n",
       "      <td>7808</td>\n",
       "      <td>0.320856</td>\n",
       "      <td>0.349858</td>\n",
       "      <td>9861</td>\n",
       "      <td>False</td>\n",
       "      <td>0.340926</td>\n",
       "      <td>0.3409263</td>\n",
       "      <td>9861</td>\n",
       "      <td>False</td>\n",
       "      <td>0.344722</td>\n",
       "      <td>0.349858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>49521</td>\n",
       "      <td>True</td>\n",
       "      <td>9904</td>\n",
       "      <td>6904</td>\n",
       "      <td>0.374019</td>\n",
       "      <td>0.382832</td>\n",
       "      <td>9904</td>\n",
       "      <td>False</td>\n",
       "      <td>0.332973</td>\n",
       "      <td>0.33297274</td>\n",
       "      <td>9904</td>\n",
       "      <td>False</td>\n",
       "      <td>0.406226</td>\n",
       "      <td>0.382832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        idx  bad_knn  real_class  pred_knn  sim_real_cl  sim_best_knn  \\\n",
       "8       220     True          44      7272     0.263647      0.273097   \n",
       "18      434     True          86      5632     0.379282      0.389031   \n",
       "27      740     True         148      4400     0.297766      0.311228   \n",
       "34      899     True         179       754     0.350931      0.363378   \n",
       "39     1149     True         229      1172     0.432248      0.442383   \n",
       "...     ...      ...         ...       ...          ...           ...   \n",
       "1525  48238     True        9647      7383     0.334514      0.338781   \n",
       "1540  48555     True        9711      4302     0.219070      0.256552   \n",
       "1550  48795     True        9759      1933     0.412487      0.420583   \n",
       "1572  49308     True        9861      7808     0.320856      0.349858   \n",
       "1584  49521     True        9904      6904     0.374019      0.382832   \n",
       "\n",
       "     pred_soft bad_soft  soft_val    sim_soft pred_max_max bad_max_max  \\\n",
       "8           44    False  0.276703   0.2767033           44       False   \n",
       "18          86    False  0.293346  0.29334617           86       False   \n",
       "27         148    False  0.403892   0.4038915          148       False   \n",
       "34         179    False  0.340076  0.34007648          179       False   \n",
       "39         229    False  0.420982  0.42098248          229       False   \n",
       "...        ...      ...       ...         ...          ...         ...   \n",
       "1525      9647    False  0.383441   0.3834414         9647       False   \n",
       "1540      9711    False  0.254419  0.25441867         9711       False   \n",
       "1550      9759    False  0.394274  0.39427388         9759       False   \n",
       "1572      9861    False  0.340926   0.3409263         9861       False   \n",
       "1584      9904    False  0.332973  0.33297274         9904       False   \n",
       "\n",
       "     max_max_val sim_max_max  \n",
       "8       0.291689    0.273097  \n",
       "18      0.313597    0.389031  \n",
       "27      0.377757    0.311228  \n",
       "34      0.346619    0.363378  \n",
       "39      0.434871    0.442383  \n",
       "...          ...         ...  \n",
       "1525    0.354863    0.338781  \n",
       "1540    0.295279    0.256552  \n",
       "1550    0.366673    0.420583  \n",
       "1572    0.344722    0.349858  \n",
       "1584    0.406226    0.382832  \n",
       "\n",
       "[105 rows x 14 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.where(new.query(\"bad_max_max == False and bad_knn == True\")['bad_soft'].to_numpy())[0].shape\n",
    "new.query(\"bad_max_max == False and bad_soft == False and bad_knn == True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dic, 'new_dic.torch')\n",
    "torch.save(result_dic, 'result_dic.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_clusters = 10\n",
    "# centers, labels = clustering.init_centers(traincenterx, n_clusters)\n",
    "# model = clustering.Fast_KMeans(n_clusters=n_clusters, max_iter=100, tol=0.0001, verbose=0, centroids=centers, mode=\"cosine\", minibatch=None)\n",
    "# lbls = model.fit_predict(torch.Tensor(traincenterx).cuda())\n",
    "# for i in range(n_clusters):\n",
    "#     result[i] = (lbls == i).nonzero().cpu().numpy().squeeze(1)\n",
    "\n",
    "# for r in result:\n",
    "#     unique_result[r] = result[r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr = model.predict(torch.Tensor(testx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trues = 0\n",
    "# falses = 0\n",
    "# for idx, p in enumerate(pr):\n",
    "#     if testl[idx] in result[p.item()]:\n",
    "#         trues += 1\n",
    "#     else:\n",
    "#         falses += 1\n",
    "# trues / (trues + falses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = dict()\n",
    "# lbls = dict()\n",
    "# for idx, l in enumerate(labels):\n",
    "#     lbls[l] = idx\n",
    "# for i in range(n_clusters):\n",
    "#     data[i] = [centers[i]]\n",
    "\n",
    "# for i, train in tqdm(enumerate(traincenterx)):\n",
    "#     max_best = -1\n",
    "#     for j, cntr in enumerate(centers):\n",
    "#         d = utility_functions.cos_sim(torch.Tensor([train]), torch.Tensor(data[j]))\n",
    "#         best = d.max()\n",
    "#         if best > max_best:\n",
    "#             max_best = best\n",
    "#             max_idx = j\n",
    "#     data[max_idx].append(train)\n",
    "#     lbls[i] = max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez_compressed(file=join('..', dataset_name, 'data', str(n_classes), str(n_classes) + '_nearest_clustering.npz'), res = data)\n",
    "# d = np.load(join('..', dataset_name, 'data', str(n_classes), str(n_classes) + '_nearest_clustering.npz'), allow_pickle=True)['res'].item()\n",
    "\n",
    "# res = dict()\n",
    "# for i in range(n_clusters):\n",
    "#     res[i] = []\n",
    "# for l in lbls:\n",
    "#     res[lbls[l]] += [l]\n",
    "\n",
    "# for r in res:\n",
    "#     res[r] = np.array(np.sort(res[r]))\n",
    "\n",
    "# np.savez_compressed(file=join('..', dataset_name, 'data', str(n_classes), str(n_classes) + '_nearest_clustering_result.npz'), res = res)\n",
    "# result = np.load(join('..', dataset_name, 'data', str(n_classes), str(n_classes) + '_nearest_clustering_result.npz'), allow_pickle=True)['res'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 10000\n",
    "# batch_numbers = len(testx) // batch_size + 1\n",
    "# nearests = []\n",
    "# for batch in tqdm(range(batch_numbers)):\n",
    "#     batch_distances = utility_functions.cos_sim(torch.Tensor(testx[batch * batch_size: np.min([len(testx), (batch+1) * batch_size])]), torch.Tensor(traincenterx)).numpy()\n",
    "#     nearests.extend(np.argmax(batch_distances, 1))\n",
    "# nearests = np.array(nearests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trues = 0\n",
    "# falses = 0\n",
    "\n",
    "# for idx, test in (enumerate(testx)):\n",
    "#     pred_cluster = None\n",
    "#     real_cluster = None\n",
    "#     for r in res:\n",
    "#         if testl[idx] in res[r]:\n",
    "#             real_cluster = r\n",
    "    \n",
    "#     for r in res:\n",
    "#         if nearests[idx].item() in res[r]:\n",
    "#             pred_cluster = r\n",
    "\n",
    "#     if pred_cluster == real_cluster:\n",
    "#         trues += 1\n",
    "#     else:\n",
    "#         falses += 1\n",
    "\n",
    "#     if (idx + 1) % 1000 == 0:\n",
    "#         print(idx + 1, trues / (trues + falses))\n",
    "# trues / (trues + falses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trues = 0\n",
    "# falses = 0\n",
    "\n",
    "# for idx, test in (enumerate(testx)):\n",
    "#     pred_cluster = None\n",
    "#     real_cluster = None\n",
    "#     for r in result:\n",
    "#         if testl[idx] in result[r]:\n",
    "#             real_cluster = r\n",
    "    \n",
    "#     for r in result:\n",
    "#         if nearests[idx].item() in result[r]:\n",
    "#             pred_cluster = r\n",
    "\n",
    "#     if pred_cluster == real_cluster:\n",
    "#         trues += 1\n",
    "#     else:\n",
    "#         falses += 1\n",
    "\n",
    "#     if (idx + 1) % 1000 == 0:\n",
    "#         print(idx + 1, trues / (trues + falses))\n",
    "# trues / (trues + falses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # our method\n",
    "# if len(unique_result) == 1:\n",
    "#     trues = 0\n",
    "#     falses = 0\n",
    "#     for test_sample in range(len(testx)):\n",
    "#         real_class = testl[test_sample]\n",
    "#         pred_class = argmax_softmax[test_sample][0]\n",
    "#         if pred_class == real_class:\n",
    "#             trues += weights[test_sample]\n",
    "#         else:\n",
    "#             falses += weights[test_sample]\n",
    "\n",
    "#         # if (test_sample + 1) % 10000 == 0:\n",
    "#         #     print(test_sample + 1, trues / (trues + falses))    \n",
    "\n",
    "#     pprint((test_sample + 1, trues / (trues + falses)))\n",
    "\n",
    "# # res = (sim_values * 1.3 * ((sim_softmax+1)/2)).numpy() > (sofmax_values * ((np.array(softmax_sims)+1)/2))\n",
    "\n",
    "# elif meth != 'sample_kmeans':\n",
    "#     max_max_trues = 0\n",
    "#     knn_trues = 0\n",
    "#     mixed_trues = 0\n",
    "#     max_max_falses = 0\n",
    "#     knn_falses = 0\n",
    "#     mixed_falses = 0\n",
    "\n",
    "#     index = 0\n",
    "#     # cc = None\n",
    "\n",
    "#     for test_sample in range(len(testx)):\n",
    "#         if meth == 'half_random':\n",
    "#             ssw = False\n",
    "#             for t in temp:\n",
    "#                 if testl[test_sample] in temp[t]:\n",
    "#                     ssw = True\n",
    "#                     break\n",
    "#             if ssw:\n",
    "#                 max_max_falses += weights[test_sample]\n",
    "#                 knn_falses += weights[test_sample]\n",
    "#                 mixed_falses += weights[test_sample]\n",
    "#                 continue\n",
    "\n",
    "#         real_class = testl[test_sample]\n",
    "#         # real_cluster = np.where([real_class in unique_result[i] for i in range(n_clusters)])[0][0]\n",
    "#         # class_in_cluster = np.where(unique_result[real_cluster] == real_class)[0][0]\n",
    "#         # # real_class_softmax_value = models[real_cluster].predict(np.array([temp_testx[test_sample]]))[0][class_in_cluster]\n",
    "#         # # real_class_knn_value = cc[test_sample, test_sample].numpy()\n",
    "\n",
    "#         # softmax_softmax_cluster = np.argmax(max_softmax[test_sample])\n",
    "#         # softmax_softmax_class = unique_result[softmax_softmax_cluster][argmax_softmax[test_sample][softmax_softmax_cluster]]\n",
    "#         # softmax_softmax_value = max_softmax[test_sample][softmax_softmax_cluster]\n",
    "#         # # if index != len(points):\n",
    "#         # #     if test_sample == points[index]:\n",
    "#         # #         cc = torch.load(join('..', dataset_name, 'data', str(n_classes), str(n_classes) + '_cos_sim_' + str(index) + '.npz'))\n",
    "#         # #         index += 1\n",
    "\n",
    "#         if sofmax_values[test_sample] > 0.5:\n",
    "#             if softmax_classes[test_sample] == real_class:\n",
    "#                 max_max_trues += weights[test_sample]\n",
    "#                 knn_trues += weights[test_sample]\n",
    "#                 mixed_trues += weights[test_sample]\n",
    "#             else:\n",
    "#                 max_max_falses += weights[test_sample]\n",
    "#                 knn_falses += weights[test_sample]\n",
    "#                 mixed_falses += weights[test_sample]\n",
    "#         else:\n",
    "#             if softmax_classes[test_sample] != real_class:\n",
    "#                 max_max_falses += weights[test_sample]\n",
    "#             elif softmax_classes[test_sample] == real_class:         \n",
    "#                 max_max_trues += weights[test_sample]\n",
    "\n",
    "#         if res[test_sample]:\n",
    "#             if sim_classes[test_sample] == real_class:\n",
    "#                 mixed_trues += weights[test_sample]\n",
    "#                 knn_trues += weights[test_sample]\n",
    "#             else:\n",
    "#                 mixed_falses += weights[test_sample]\n",
    "#                 knn_falses += weights[test_sample]\n",
    "#             if softmax_classes[test_sample] == real_class:\n",
    "#                 max_max_trues += weights[test_sample]\n",
    "#             else:\n",
    "#                 max_max_falses += weights[test_sample]\n",
    "#         else:\n",
    "#             if softmax_classes[test_sample] == real_class:\n",
    "#                 mixed_trues += weights[test_sample]\n",
    "#                 max_max_trues += weights[test_sample]\n",
    "#             else:\n",
    "#                 mixed_falses += weights[test_sample]\n",
    "#                 max_max_falses += weights[test_sample]\n",
    "#             if sim_classes[test_sample] == real_class:\n",
    "#                 knn_trues += weights[test_sample]\n",
    "#             else:\n",
    "#                 knn_falses += weights[test_sample]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             knn_knn_cluster = None\n",
    "#             knn_knn_value = -1\n",
    "#             for f in unique_result:\n",
    "#                 g = utility_functions.cos_sim(torch.Tensor([testx[test_sample]]), torch.Tensor(traincenterx[unique_result[f]]))\n",
    "#                 temp_knn_knn_class = unique_result[f][np.argmax(g).item()]\n",
    "#                 temp_knn_knn_value = np.max(g.numpy())\n",
    "#                 if temp_knn_knn_value > knn_knn_value:\n",
    "#                     knn_knn_cluster = f\n",
    "#                     knn_knn_value = temp_knn_knn_value\n",
    "#                     knn_knn_class = temp_knn_knn_class\n",
    "#             # knn_knn_cluster = np.where([knn_knn_class in unique_result[i] for i in range(n_clusters)])[0][0]\n",
    "#             knn_softmax_class = unique_result[knn_knn_cluster][argmax_softmax[test_sample][knn_knn_cluster]]\n",
    "\n",
    "#             knn_knn_class = np.argmax(cc[np.max([0, test_sample - ((index-1) * batch_size)])]).item()\n",
    "#             try:\n",
    "#                 knn_knn_cluster = np.where([knn_knn_class in unique_result[i] for i in range(n_clusters)])[0][0]\n",
    "#             except:\n",
    "#                 knn_falses += weights[test_sample]\n",
    "#                 mixed_falses += weights[test_sample]\n",
    "#                 continue\n",
    "\n",
    "#             knn_softmax_class = unique_result[knn_knn_cluster][argmax_softmax[test_sample][knn_knn_cluster]]\n",
    "\n",
    "#             # class_in_cluster = np.where(unique_result[knn_knn_cluster] == knn_knn_class)[0][0]\n",
    "#             # knn_softmax_value = models[knn_knn_cluster].predict(np.array([testx[test_sample]]))[0][class_in_cluster]\n",
    "#             # # knn_knn_value = cc[np.max([0, test_sample - ((index-1) * batch_size)])].max().item()\n",
    "#             # # softmax_knn_value = cc[np.max([0, test_sample -((index-1) * batch_size)]), softmax_softmax_class].item()\n",
    "#             # softmax_knn_value = utility_functions.cos_sim(torch.Tensor([testx[test_sample]]), torch.Tensor([traincenterx[softmax_softmax_class]])).item()\n",
    "\n",
    "#             if knn_knn_class == softmax_softmax_class:\n",
    "#                 if knn_knn_class == real_class:\n",
    "#                     mixed_trues += weights[test_sample]\n",
    "#                     knn_trues += weights[test_sample]\n",
    "#                 else:\n",
    "#                     mixed_falses += weights[test_sample]\n",
    "#                     knn_falses += weights[test_sample]\n",
    "#             else:\n",
    "#                 if knn_softmax_class == real_class:\n",
    "#                     knn_trues += weights[test_sample]\n",
    "#                 else:\n",
    "#                     knn_falses += weights[test_sample]        \n",
    "\n",
    "#                 if knn_softmax_value * knn_knn_value > softmax_softmax_value * softmax_knn_value:\n",
    "#                     if knn_knn_class == real_class:\n",
    "#                         mixed_trues += weights[test_sample]\n",
    "#                     else:\n",
    "#                         mixed_falses += weights[test_sample]\n",
    "#                 else:\n",
    "#                     if softmax_softmax_class == real_class:\n",
    "#                         mixed_trues += weights[test_sample]\n",
    "#                     else:\n",
    "#                         mixed_falses += weights[test_sample]\n",
    "\n",
    "#         if (test_sample + 1) % 1000 == 0:\n",
    "#             pprint((test_sample + 1, max_max_trues / (max_max_trues + max_max_falses), knn_trues / (knn_trues + knn_falses), mixed_trues / (mixed_trues + mixed_falses)))    \n",
    "\n",
    "#     pprint((test_sample + 1, max_max_trues / (max_max_trues + max_max_falses), knn_trues / (knn_trues + knn_falses), mixed_trues / (mixed_trues + mixed_falses)))    \n",
    "\n",
    "# # اگر خواستیم از پایینی استفاده کنیم، به شکل اصلاح شده بالایی درآوریم\n",
    "# # # our method for fuzzy_clustering\n",
    "# # elif meth == 'sample_kmeans':\n",
    "# #     max_max_trues = 0\n",
    "# #     knn_trues = 0\n",
    "# #     mixed_trues = 0\n",
    "# #     max_max_falses = 0\n",
    "# #     knn_falses = 0\n",
    "# #     mixed_falses = 0\n",
    "\n",
    "# #     index = 0\n",
    "# #     cc = None\n",
    "# #     for test_sample in range(len(testx)):\n",
    "# #         real_class = testl[test_sample]\n",
    "# #         real_cluster = np.where([real_class in unique_result[i] for i in range(n_clusters)])[0][0]\n",
    "# #         class_in_cluster = np.where(unique_result[real_cluster] == real_class)[0][0]\n",
    "# #         # real_class_softmax_value = models[real_cluster].predict(np.array([temp_testx[test_sample]]))[0][class_in_cluster]\n",
    "# #         # real_class_knn_value = cc[test_sample, test_sample].numpy()\n",
    "\n",
    "# #         softmax_softmax_cluster = np.argmax(max_softmax[test_sample])\n",
    "# #         softmax_softmax_class = unique_result[softmax_softmax_cluster][argmax_softmax[test_sample][softmax_softmax_cluster]]\n",
    "# #         softmax_softmax_value = max_softmax[test_sample][softmax_softmax_cluster]\n",
    "# #         # if index != len(points):\n",
    "# #         #     if test_sample == points[index]:\n",
    "# #         #         cc = torch.load(join('..', dataset_name, 'data', str(n_classes), str(n_classes) + '_cos_sim_' + str(index) + '.npz'))\n",
    "# #         #         index += 1\n",
    "\n",
    "# #         if sofmax_values[test_sample] > 0.5:\n",
    "# #             if softmax_classes[test_sample] == real_class\n",
    "# #                 max_max_trues += weights[test_sample]\n",
    "# #                 knn_trues += weights[test_sample]\n",
    "# #                 mixed_trues += weights[test_sample]\n",
    "# #             else:\n",
    "# #                 max_max_falses += weights[test_sample]\n",
    "# #                 knn_falses += weights[test_sample]\n",
    "# #                 mixed_falses += weights[test_sample]\n",
    "# #         else:\n",
    "# #             if softmax_softmax_class != real_class:\n",
    "# #                 max_max_falses += weights[test_sample]\n",
    "# #             elif softmax_softmax_class == real_class:         \n",
    "# #                 max_max_trues += weights[test_sample]\n",
    "        \n",
    "# #             # knn_knn_class = np.argmax(cc[np.max([0, test_sample - ((index-1) * batch_size)])]).item()\n",
    "# #             selected_cluster_value = -1\n",
    "# #             knn_knn_cluster = None\n",
    "# #             knn_knn_value = -1\n",
    "# #             for f in unique_result:\n",
    "# #                 g = utility_functions.cos_sim(torch.Tensor([testx[test_sample]]), torch.Tensor(traincenterx[unique_result[f]]))\n",
    "# #                 temp_knn_knn_class = unique_result[f][np.argmax(g).item()]\n",
    "# #                 temp_knn_knn_value = np.max(g.numpy())\n",
    "# #                 if temp_knn_knn_value > knn_knn_value:\n",
    "# #                     knn_knn_cluster = f\n",
    "# #                     knn_knn_value = temp_knn_knn_value\n",
    "# #                     knn_knn_class = temp_knn_knn_class\n",
    "# #             # knn_knn_cluster = np.where([knn_knn_class in unique_result[i] for i in range(n_clusters)])[0][0]\n",
    "# #             knn_softmax_class = unique_result[knn_knn_cluster][argmax_softmax[test_sample][knn_knn_cluster]]\n",
    "\n",
    "# #             class_in_cluster = np.where(unique_result[knn_knn_cluster] == knn_knn_class)[0][0]\n",
    "# #             knn_softmax_value = models[knn_knn_cluster].predict(np.array([testx[test_sample]]))[0][class_in_cluster]\n",
    "# #             # knn_knn_value = cc[np.max([0, test_sample - ((index-1) * batch_size)])].max().item()\n",
    "# #             # softmax_knn_value = cc[np.max([0, test_sample -((index-1) * batch_size)]), softmax_softmax_class].item()\n",
    "# #             softmax_knn_value = utility_functions.cos_sim(torch.Tensor([testx[test_sample]]), torch.Tensor([traincenterx[softmax_softmax_class]]))\n",
    "\n",
    "# #             if knn_knn_class == softmax_softmax_class:\n",
    "# #                 if knn_knn_class == real_class:\n",
    "# #                     mixed_trues += weights[test_sample]\n",
    "# #                     knn_trues += weights[test_sample]\n",
    "# #                 else:\n",
    "# #                     mixed_falses += weights[test_sample]\n",
    "# #                     knn_falses += weights[test_sample]\n",
    "# #             else:\n",
    "# #                 if knn_softmax_class == real_class:\n",
    "# #                     knn_trues += weights[test_sample]\n",
    "# #                 else:\n",
    "# #                     knn_falses += weights[test_sample]        \n",
    "\n",
    "# #                 if knn_softmax_value * knn_knn_value > softmax_softmax_value * softmax_knn_value:\n",
    "# #                     if knn_knn_class == real_class:\n",
    "# #                         mixed_trues += weights[test_sample]\n",
    "# #                     else:\n",
    "# #                         mixed_falses += weights[test_sample]\n",
    "# #                 else:\n",
    "# #                     if softmax_softmax_class == real_class:\n",
    "# #                         mixed_trues += weights[test_sample]\n",
    "# #                     else:\n",
    "# #                         mixed_falses += weights[test_sample]\n",
    "\n",
    "\n",
    "# #         if (test_sample + 1) % 1000 == 0:\n",
    "# #             pprint((test_sample + 1, max_max_trues / (max_max_trues + max_max_falses), knn_trues / (knn_trues + knn_falses), mixed_trues / (mixed_trues + mixed_falses)))\n",
    "\n",
    "# #     pprint((test_sample + 1, max_max_trues / (max_max_trues + max_max_falses), knn_trues / (knn_trues + knn_falses), mixed_trues / (mixed_trues + mixed_falses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # very important: for feature extraction balanced test dataset\n",
    "# import onnx\n",
    "# import warnings\n",
    "# from onnx_tf.backend import prepare\n",
    "# import numpy as np\n",
    "# from datetime import datetime\n",
    "# import tensorflow as tf\n",
    "# import os\n",
    "# import cv2\n",
    "# from tqdm import tqdm\n",
    "# import PIL\n",
    "# from PIL import Image\n",
    "\n",
    "\n",
    "# all_ids = np.load('./id_files/glint_all_ids.npz')['res']\n",
    "# warnings.filterwarnings('ignore') # Ignore all the warning messages in this tutorial\n",
    "\n",
    "# onnx_model = onnx.load('F:/test/onnx_tensorflow/model.onnx')\n",
    "# tf_rep = prepare(onnx_model) # Import the ONNX model to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# all_id_files = dict()\n",
    "# with open(os.path.join('..', 'glint360k_224', 'data', 'all_id_files.json')) as jsonfile:\n",
    "#     all_id_files = json.load(jsonfile)\n",
    "\n",
    "# keys = list(all_id_files.keys())[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = 'E:/balanced_glint360k_224/test/'\n",
    "# new_dataset_path = 'F:/test/deepface/glint360k_224/img_data'\n",
    "# os.makedirs(new_dataset_path, exist_ok=True)\n",
    "# os.makedirs(os.path.join(new_dataset_path, 'embeddings'), exist_ok=True)\n",
    "\n",
    "# image_list = []\n",
    "# begin = 99990\n",
    "# idx = begin\n",
    "# for d in tqdm(keys[begin:]):\n",
    "#     if idx % 10 == 0 and idx > begin:\n",
    "#         emb = tf_rep.run(np.array(image_list))._0\n",
    "#         np.savez_compressed(os.path.join(new_dataset_path, 'embeddings', str(idx) + '.npz'), res=emb)\n",
    "#         image_list = []\n",
    "#     for f in os.listdir(os.path.join(dataset_path, d)):\n",
    "#         img_path = os.path.join(dataset_path, d, f)\n",
    "#         # img = cv2.imread(img_path)\n",
    "#         try:\n",
    "#             img = Image.open(img_path)\n",
    "#         except:\n",
    "#             try:\n",
    "#                 img = Image.open(os.path.join('H:/glint360k_224/', d, f))\n",
    "#             except:\n",
    "#                 img = Image.open(os.path.join('D:/glint360k_224/', d, f))\n",
    "#         x_train = tf.image.resize(np.array(img), (112, 112), method=\"nearest\")\n",
    "#         x_train = (tf.cast(x_train, tf.float32) - 127.5) / 128.\n",
    "#         x_train = tf.transpose(x_train, perm=[2, 0, 1])\n",
    "#         x_train = tf.expand_dims(x_train, 0)\n",
    "#         image_list.extend(x_train)\n",
    "#     idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import sys\n",
    "# import numpy as np\n",
    "# import mxnet as mx\n",
    "# import os\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "\n",
    "# from scipy import misc\n",
    "# import random\n",
    "# import sklearn\n",
    "# from sklearn.decomposition import PCA\n",
    "# from time import sleep\n",
    "# from easydict import EasyDict as edict\n",
    "# from mtcnn_detector import MtcnnDetector\n",
    "# from skimage import transform as trans\n",
    "# import matplotlib.pyplot as plt\n",
    "# from mxnet.contrib.onnx.onnx2mx.import_model import import_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import onnx\n",
    "# from onnx_tf.backend import prepare\n",
    "\n",
    "# # model_name = 'F:/test/onnx_tensorflow/model.onnx'\n",
    "# # # model_name = 'C:\\\\Users\\\\Dianat\\\\Downloads\\\\arcfaceresnet100-8.onnx'\n",
    "# # model = get_model(ctx , model_name)\n",
    "\n",
    "# model = onnx.load('F:/test/onnx_tensorflow/model.onnx')\n",
    "# tf_rep = prepare(model) # Import the ONNX model to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# # mx.test_utils.download('https://s3.amazonaws.com/onnx-model-zoo/arcface/player1.jpg')\n",
    "# img = cv2.imread('player1.jpg')\n",
    "# x_train = tf.image.resize(img, (112, 112), method=\"nearest\")\n",
    "# x_train = (tf.cast(x_train, tf.float32) - 127.5) / 128.\n",
    "# x_train = tf.transpose(x_train, perm=[2, 1, 0])\n",
    "# x_train = tf.expand_dims(x_train, 0)\n",
    "# out1 = tf_rep.run(np.array(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 10\n",
    "# batch_numbers = len(all_ids) // batch_size + (1 if (len(all_ids) % batch_size != 0) else 0)\n",
    "# print('started at : ', datetime.now().strftime('%y/%m/%d-%H:%M:%S'))\n",
    "# for batch in tqdm(range(batch_numbers)):\n",
    "# #     with tf.device('/cpu:0'):\n",
    "# #     print(datetime.now().strftime('%y/%m/%d-%H:%M:%S'), batch, ' from ', batch_numbers, ' batchs')\n",
    "#     selected_classes = all_ids[batch*batch_size:min(len(all_classes), (batch+1)*batch_size)]\n",
    "\n",
    "#     data = []\n",
    "#     for id in (selected_classes):\n",
    "#         x = np.load(join('E:/balanced_glint360k_224/test', str(id) + '.npz'), allow_pickle=True)\n",
    "#         x = x[x.files[0]]\n",
    "#         data += list(x)    \n",
    "#     new_data = np.moveaxis(data, -1, 1)#[1,2], [3,2])\n",
    "#     result = tf_rep.run(new_data)[0]\n",
    "\n",
    "#     for idx, id in (enumerate(selected_classes)):\n",
    "#         embs = result[max_sample_size*idx:max_sample_size*(idx+1)]\n",
    "# #     np.savez_compressed('F:\\\\test\\\\deepface\\\\250k_dataset\\\\embeddings\\\\common\\\\' + str(id) + '.npz')\n",
    "#         np.savetxt(join('..', dataset_name, 'embeddings', 'common', str(id) + '.csv'), embs, delimiter=\",\")\n",
    "#     #     for load:\n",
    "#     # data = np.loadtxt('data.csv', delimiter=',')    \n",
    "# print('finished at : ', datetime.now().strftime('%y/%m/%d-%H:%M:%S'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import fetch_lfw_pairs\n",
    "# lfw_pairs_test = fetch_lfw_pairs(subset = 'test')\n",
    "# pairs = lfw_pairs_test.pairs\n",
    "# # target = fetch_lfw_pairs.target\n",
    "# target = lfw_pairs_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_val_pair(path, name):\n",
    "#     carray = bcolz.carray(rootdir=os.path.join(path, name), mode='r')\n",
    "#     issame = np.load('{}/{}_list.npy'.format(path, name))\n",
    "\n",
    "#     return carray, issame\n",
    "\n",
    "\n",
    "# def get_lfw_data(data_path):\n",
    "#     \"\"\"get validation data\"\"\"\n",
    "#     _lfw, _lfw_issame = get_val_pair(data_path, 'lfw_align_112/lfw')\n",
    "\n",
    "#     return _lfw, _lfw_issame\n",
    "\n",
    "\n",
    "# def get_val_data(data_path):\n",
    "#     \"\"\"get validation data\"\"\"\n",
    "#     _lfw, _lfw_issame = get_val_pair(data_path, 'lfw_align_112/lfw')\n",
    "#     _agedb_30, _agedb_30_issame = get_val_pair(data_path, 'AgeDB/agedb_30')\n",
    "#     _cfp_fp, _cfp_fp_issame = get_val_pair(data_path, 'cfp_align_112/cfp_fp')\n",
    "\n",
    "#     return _lfw, _agedb_30, _cfp_fp, _lfw_issame, _agedb_30_issame, _cfp_fp_issame\n",
    "\n",
    "\n",
    "# def test_on_val_data(model, lfw, lfw_issame, is_ccrop: bool = False, step_i: int = 1, alfa_multiplied_ten: int = 1):\n",
    "#     step = int(alfa_multiplied_ten / step_i)\n",
    "\n",
    "#     print(\"-----------------------------------\")\n",
    "#     acc_lfw, best_th = perform_val_arcface(512, 64, model, lfw, lfw_issame, is_ccrop=is_ccrop)\n",
    "#     print(f\"[*] Results on LFW, Accuracy --> {acc_lfw} || Best Threshold --> {best_th}\")\n",
    "#     print(\"-----------------------------------\")\n",
    "\n",
    "# def hflip_batch(imgs):\n",
    "#     return imgs[:, :, ::-1, :]\n",
    "\n",
    "# def l2_norm(x, axis=1):\n",
    "#     \"\"\"l2 norm\"\"\"\n",
    "#     norm = np.linalg.norm(x, axis=axis, keepdims=True)\n",
    "#     output = x / norm\n",
    "\n",
    "#     return output\n",
    "        \n",
    "# def perform_val_arcface(embedding_size, batch_size, model,\n",
    "#                         carray, issame, nrof_folds=10, is_ccrop=False, is_flip=True):\n",
    "#     \"\"\"perform val\"\"\"\n",
    "#     embeddings = np.zeros([len(carray), embedding_size])\n",
    "\n",
    "#     for idx in tqdm.tqdm(range(0, len(carray), batch_size)):\n",
    "#         batch = carray[idx:idx + batch_size]\n",
    "#         batch = np.transpose(batch, [0, 2, 3, 1])\n",
    "#         b, g, r = tf.split(batch, 3, axis=-1)\n",
    "#         batch = tf.concat([r, g, b], -1)\n",
    "#         if is_flip:\n",
    "#             flipped = hflip_batch(batch)\n",
    "#             emb_batch = model([batch, tf.ones((batch.shape[0],), dtype=tf.int64)], training=False)[-1] + model([flipped, tf.ones((batch.shape[0],), dtype=tf.int64)], training=False)[-1]\n",
    "#             embeddings[idx:idx + batch_size] = l2_norm(emb_batch)\n",
    "#         else:\n",
    "#             emb_batch = model([batch, tf.ones((batch.shape[0],), dtype=tf.int64)], training=False)[-1]\n",
    "#             embeddings[idx:idx + batch_size] = l2_norm(emb_batch)\n",
    "\n",
    "#     tpr, fpr, accuracy, best_thresholds = evaluate(\n",
    "#         embeddings, issame, nrof_folds)\n",
    "\n",
    "#     return accuracy.mean(), best_thresholds.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lfw, lfw_issame = get_lfw_data(\"E:/datasets/test_dataset\")\n",
    "\n",
    "# carray = lfw\n",
    "# embedding_size = 512\n",
    "# batch_size = 64\n",
    "# is_flip = False\n",
    "# embeddings = np.zeros([len(carray), embedding_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in tqdm(range(0, len(carray), batch_size)):\n",
    "#     batch = carray[idx:idx + batch_size]\n",
    "#     batch = np.transpose(batch, [0, 2, 3, 1])\n",
    "#     b, g, r = tf.split(batch, 3, axis=-1)\n",
    "#     batch = tf.concat([r, g, b], -1)\n",
    "#     batch = tf.transpose(batch, perm=[0, 3, 1, 2])\n",
    "#     emb = tf_rep.run(np.array(batch))._0\n",
    "#     if is_flip:\n",
    "#         flipped = hflip_batch(batch)\n",
    "#         # emb_batch = model([batch, tf.ones((batch.shape[0],), dtype=tf.int64)], training=False)[-1] + model([flipped, tf.ones((batch.shape[0],), dtype=tf.int64)], training=False)[-1]\n",
    "#         emb_batch = tf_rep.run(np.array(batch))._0 + tf_rep.run(np.array(flipped))._0\n",
    "#         embeddings[idx:idx + batch_size] = l2_norm(emb_batch)\n",
    "#     else:\n",
    "#         # emb_batch = model([batch, tf.ones((batch.shape[0],), dtype=tf.int64)], training=False)[-1]\n",
    "#         emb_batch = tf_rep.run(np.array(batch))\n",
    "#         embeddings[idx:idx + batch_size] = l2_norm(emb_batch)\n",
    "\n",
    "# np.savez_compressed('lfw_no_flip_embeddings.npz', res = embeddings)\n",
    "# # return accuracy.mean(), best_thresholds.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_accuracy(threshold, dist, actual_issame):\n",
    "#     predict_issame = np.less(dist, threshold)\n",
    "#     tp = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "#     fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "#     tn = np.sum(np.logical_and(np.logical_not(predict_issame),\n",
    "#                                np.logical_not(actual_issame)))\n",
    "#     fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n",
    "\n",
    "#     tpr = 0 if (tp + fn == 0) else float(tp) / float(tp + fn)\n",
    "#     fpr = 0 if (fp + tn == 0) else float(fp) / float(fp + tn)\n",
    "#     _acc = float(tp + tn) / dist.size\n",
    "#     return tpr, fpr, _acc\n",
    "\n",
    "\n",
    "# def calculate_roc(thresholds, embeddings1, embeddings2, actual_issame,\n",
    "#                   nrof_folds=10):\n",
    "#     assert (embeddings1.shape[0] == embeddings2.shape[0])\n",
    "#     assert (embeddings1.shape[1] == embeddings2.shape[1])\n",
    "#     nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n",
    "#     nrof_thresholds = len(thresholds)\n",
    "#     k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n",
    "\n",
    "#     tprs = np.zeros((nrof_folds, nrof_thresholds))\n",
    "#     fprs = np.zeros((nrof_folds, nrof_thresholds))\n",
    "#     accuracy = np.zeros((nrof_folds,))\n",
    "#     best_thresholds = np.zeros((nrof_folds,))\n",
    "#     indices = np.arange(nrof_pairs)\n",
    "\n",
    "#     diff = np.subtract(embeddings1, embeddings2)\n",
    "#     dist = np.sum(np.square(diff), 1)\n",
    "\n",
    "#     for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n",
    "#         # Find the best threshold for the fold\n",
    "#         acc_train = np.zeros((nrof_thresholds,))\n",
    "#         for threshold_idx, threshold in enumerate(thresholds):\n",
    "#             _, _, acc_train[threshold_idx] = calculate_accuracy(\n",
    "#                 threshold, dist[train_set], actual_issame[train_set])\n",
    "#         best_threshold_index = np.argmax(acc_train)\n",
    "\n",
    "#         best_thresholds[fold_idx] = thresholds[best_threshold_index]\n",
    "#         for threshold_idx, threshold in enumerate(thresholds):\n",
    "#             tprs[fold_idx, threshold_idx], fprs[fold_idx, threshold_idx], _ = \\\n",
    "#                 calculate_accuracy(threshold,\n",
    "#                                    dist[test_set],\n",
    "#                                    actual_issame[test_set])\n",
    "#         _, _, accuracy[fold_idx] = calculate_accuracy(\n",
    "#             thresholds[best_threshold_index],\n",
    "#             dist[test_set],\n",
    "#             actual_issame[test_set])\n",
    "\n",
    "#     tpr = np.mean(tprs, 0)\n",
    "#     fpr = np.mean(fprs, 0)\n",
    "#     return tpr, fpr, accuracy, best_thresholds\n",
    "\n",
    "\n",
    "# def evaluate(embeddings, actual_issame, nrof_folds=10):\n",
    "#     # Calculate evaluation metrics\n",
    "#     thresholds = np.arange(0, 4, 0.01)\n",
    "#     embeddings1 = embeddings[0::2]\n",
    "#     embeddings2 = embeddings[1::2]\n",
    "#     tpr, fpr, accuracy, best_thresholds = calculate_roc(\n",
    "#         thresholds, embeddings1, embeddings2, np.asarray(actual_issame),\n",
    "#         nrof_folds=nrof_folds)\n",
    "\n",
    "#     return tpr, fpr, accuracy, best_thresholds\n",
    "\n",
    "\n",
    "# def perform_val(embedding_size, batch_size, model,\n",
    "#                 carray, issame, nrof_folds=10, is_ccrop=False, is_flip=True):\n",
    "#     \"\"\"perform val\"\"\"\n",
    "#     embeddings = np.zeros([len(carray), embedding_size])\n",
    "\n",
    "#     for idx in tqdm.tqdm(range(0, len(carray), batch_size)):\n",
    "#         batch = carray[idx:idx + batch_size]\n",
    "#         batch = np.transpose(batch, [0, 2, 3, 1])\n",
    "#         b, g, r = tf.split(batch, 3, axis=-1)\n",
    "#         batch = tf.concat([r, g, b], -1)\n",
    "#         if is_flip:\n",
    "#             flipped = hflip_batch(batch)\n",
    "#             emb_batch = model(batch, training=False) + model(flipped, training=False)\n",
    "#             embeddings[idx:idx + batch_size] = l2_norm(emb_batch)\n",
    "#         else:\n",
    "#             emb_batch = model(batch, training=False)\n",
    "#             embeddings[idx:idx + batch_size] = l2_norm(emb_batch)\n",
    "\n",
    "#     tpr, fpr, accuracy, best_thresholds = evaluate(\n",
    "#         embeddings, issame, nrof_folds)\n",
    "\n",
    "#     return accuracy.mean(), best_thresholds.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# tpr, fpr, accuracy, best_thresholds = evaluate(embeddings, lfw_issame, nrof_folds=10)\n",
    "\n",
    "# embeddings = np.load('lfw_flip_embeddings.npz')['res']\n",
    "# tpr, fpr, accuracy, best_thresholds = evaluate(embeddings, lfw_issame, nrof_folds=10)\n",
    "# accuracy.mean()\n",
    "\n",
    "# preds = model(embeddings)\n",
    "# ids = np.argmax(preds, 1)\n",
    "# zero_ids = ids[0::2]\n",
    "# one_ids = ids[1::2]\n",
    "# res = (zero_ids == one_ids)\n",
    "# f = (res == lfw_issame)\n",
    "# len(np.where(f)[0]) / 6000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('arcface-tf2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7e022da480da49e336235f935b29fd28be2ae9e0f4ebe7bf3c5148e275c03b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
