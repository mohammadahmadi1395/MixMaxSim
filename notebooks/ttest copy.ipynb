{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utility_functions, clustering, training\n",
    "# import importlib\n",
    "# importlib.reload(utility_functions)\n",
    "# import clustering\n",
    "# importlib.reload(clustering)\n",
    "# import training\n",
    "# importlib.reload(training)\n",
    "\n",
    "import shutil\n",
    "import json\n",
    "# import mxnet as mx\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from os.path import join\n",
    "from numpy.random import seed\n",
    "from numpy import unique, where, savez_compressed\n",
    "import random\n",
    "import torch\n",
    "import re\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils.extmath import softmax\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import itertools\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# # very important\n",
    "# # ! git clone https://github.com/yinguobing/arcface.git\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn import metrics\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "# from src.torrent_downloader import download_torrent_or_magnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration file\n",
    "with open(\"./config/config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download glint360k dataset\n",
    "# glint360k_magnet_link = config[\"glint360k\"][\"magnet_link\"] \n",
    "# glint360k_torrent_path = config[\"glint360k\"][\"torrent_path\"] \n",
    "# glint360k_save_path = config[\"glint360k\"][\"orig_dataset_dir\"]\n",
    "\n",
    "# # try:\n",
    "# #     download_torrent_or_magnet(glint360k_torrent_path, glint360k_save_path)\n",
    "# # except:\n",
    "# #     download_torrent_or_magnet(glint360k_magnet_link, glint360k_save_path)\n",
    "\n",
    "# # download ms1m dataset\n",
    "# ms1m_torrent_path = config[\"ms-celeb-1m\"][\"torrent_path\"] \n",
    "# ms1m_save_path = config[\"ms-celeb-1m\"][\"orig_dataset_dir\"]\n",
    "\n",
    "# # download_torrent_or_magnet(ms1m_torrent_path, ms1m_save_path)\n",
    "\n",
    "# # download vggface2 dataset\n",
    "# vggface2_torrent_path = config[\"vggface2\"][\"magnet_link\"]\n",
    "# vggface2_save_path = config[\"vggface2\"][\"orig_dataset_dir\"]\n",
    "\n",
    "# # download_torrent_or_magnet(vggface2_torrent_path, vggface2_save_path)\n",
    "\n",
    "# prepare data such that each id has 20 images for train, 5 for test and 5 for validation (for all datasets)\n",
    "# balanced datasets are balanced_glint360k, balanced_ms1m, balanced_vggface2\n",
    "\n",
    "# extract features of all images using onnx pretrained model and save them in dataset_name/embeddings directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset_name):\n",
    "    train_embeddings_path = join(config[dataset_name][\"features\"], 'train')\n",
    "    val_embeddings_path = join(config[dataset_name][\"features\"], 'val')\n",
    "    test_embeddings_path = join(config[dataset_name][\"features\"], 'test')\n",
    "\n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    trainl = []\n",
    "\n",
    "    traincenterx = []\n",
    "    traincentery = []\n",
    "    traincenterl = []\n",
    "\n",
    "    testx = []\n",
    "    testy = []\n",
    "    testl = []\n",
    "\n",
    "    valx = []\n",
    "    valy = []\n",
    "    vall = []\n",
    "\n",
    "    if overwrite == False and os.path.isfile(join(super_scenario_path, 'testx.npz')):\n",
    "        trainx = np.load(join(super_scenario_path, 'trainx.npz'))['res'][:20]\n",
    "        trainy = np.load(join(super_scenario_path, 'trainy.npz'))['res'][:20]\n",
    "        trainl = np.load(join(super_scenario_path, 'trainl.npz'))['res'][:20]\n",
    "\n",
    "        traincenterx = np.load(join(super_scenario_path, 'traincenterx.npz'))['res']\n",
    "        traincentery = np.load(join(super_scenario_path, 'traincentery.npz'))['res']\n",
    "        traincenterl = np.load(join(super_scenario_path, 'traincenterl.npz'))['res']\n",
    "\n",
    "        testx = np.load(join(super_scenario_path, 'testx.npz'))['res'][:5]\n",
    "        testy = np.load(join(super_scenario_path, 'testy.npz'))['res'][:5]\n",
    "        testl = np.load(join(super_scenario_path, 'testl.npz'))['res'][:5]\n",
    "\n",
    "        valx = np.load(join(super_scenario_path, 'valx.npz'))['res'][:5]\n",
    "        valy = np.load(join(super_scenario_path, 'valy.npz'))['res'][:5]\n",
    "        vall = np.load(join(super_scenario_path, 'vall.npz'))['res'][:5]\n",
    "    else:\n",
    "        all_id_files = dict()\n",
    "        with open(join(config[dataset_name]['orig_dataset_dir'], dataset_name, 'all_id_files.json')) as jsonfile:\n",
    "            all_id_files = json.load(jsonfile)\n",
    "\n",
    "        keys = list(all_id_files.keys())[:n_classes]\n",
    "        \n",
    "        os.makedirs(join('..', dataset_name, 'data', str(n_classes)), exist_ok=True)\n",
    "\n",
    "        idx = 0\n",
    "        for class_name in tqdm(keys):\n",
    "            tr_x = np.load(join(train_embeddings_path, class_name + '.npz'), allow_pickle=True)\n",
    "            tr_f = 20 # len(tr_features)\n",
    "            tr_features = tr_x[tr_x.files[0]][:tr_f]    \n",
    "            trainx.extend(tr_features)\n",
    "            trainy.extend([class_name] * tr_f)\n",
    "            trainl.extend([idx] for t in range(tr_f))\n",
    "\n",
    "            traincenterx.append(np.mean(tr_features, axis=0))\n",
    "            traincenterl.append(idx)\n",
    "            traincentery.append(class_name)\n",
    "\n",
    "            te_x = np.load(join(test_embeddings_path, class_name + '.npz'), allow_pickle=True)\n",
    "            te_f = 5 # len(te_features)\n",
    "            te_features = te_x[te_x.files[0]][:te_f]\n",
    "            testx.extend(te_features)\n",
    "            testl.extend([idx] for t in range(te_f))\n",
    "            testy.extend([class_name] * te_f)\n",
    "\n",
    "            v_x = np.load(join(val_embeddings_path, class_name + '.npz'), allow_pickle=True)\n",
    "            v_f = 5 # len(v_features)\n",
    "            v_features = v_x[v_x.files[0]][:v_f]\n",
    "            valx.extend(v_features)\n",
    "            vall.extend([idx] for t in range(v_f))\n",
    "            valy.extend([class_name] * v_f)\n",
    "\n",
    "            idx+=1\n",
    "\n",
    "        trainx = np.array(trainx)\n",
    "        trainl = np.array(trainl)\n",
    "        trainy = np.array(trainy)\n",
    "\n",
    "        traincenterx = np.array(traincenterx)\n",
    "        traincenterl = np.array(traincenterl)\n",
    "        traincentery = np.array(traincentery)\n",
    "\n",
    "        testx = np.array(testx)\n",
    "        testl = np.array(testl)\n",
    "        testy = np.array(testy)\n",
    "\n",
    "        valx = np.array(valx)\n",
    "        vall = np.array(vall)\n",
    "        valy = np.array(valy)\n",
    "\n",
    "        savez_compressed(join(super_scenario_path, 'trainx.npz'), res=trainx)\n",
    "        savez_compressed(join(super_scenario_path, 'trainy.npz'), res=trainy)\n",
    "        savez_compressed(join(super_scenario_path, 'trainl.npz'), res=trainl)\n",
    "\n",
    "        savez_compressed(join(super_scenario_path, 'traincenterx.npz'), res=traincenterx)\n",
    "        savez_compressed(join(super_scenario_path, 'traincentery.npz'), res=traincentery)\n",
    "        savez_compressed(join(super_scenario_path, 'traincenterl.npz'), res=traincenterl)\n",
    "\n",
    "        savez_compressed(join(super_scenario_path, 'testx.npz'), res=testx)\n",
    "        savez_compressed(join(super_scenario_path, 'testy.npz'), res=testy)\n",
    "        savez_compressed(join(super_scenario_path, 'testl.npz'), res=testl)\n",
    "\n",
    "        savez_compressed(join(super_scenario_path, 'valx.npz'), res=valx)\n",
    "        savez_compressed(join(super_scenario_path, 'valy.npz'), res=valy)\n",
    "        savez_compressed(join(super_scenario_path, 'vall.npz'), res=vall)    \n",
    "\n",
    "    return trainx, trainy, trainl, traincenterx, traincentery, traincenterl, testx, testy, testl, valx, valy, vall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_data(method, trainx, trainy, trainl, traincenterx, traincentery, traincenterl, testx, testy, testl, valx, valy, vall):\n",
    "    distribution_path = join(data_scenario_path, 'parts.npz')\n",
    "    parts = dict()\n",
    "\n",
    "    if method == 'ISM':   \n",
    "        if overwrite == False and os.path.isfile(distribution_path):\n",
    "            parts = (np.load(distribution_path, allow_pickle=True)['res']).item()\n",
    "        else:\n",
    "            # # random \n",
    "            lbls = np.zeros((n_classes), dtype='int')\n",
    "            for i in range(n_classes):\n",
    "                lbls[i] = random.randint(0, n_clusters-1)\n",
    "            for i in range(n_clusters):\n",
    "                # parts[i] = torch.Tensor((lbls == i).nonzero()).squeeze(0).unsqueeze(1).cuda().int().cpu().numpy().squeeze(1)\n",
    "                parts[i] = torch.Tensor((lbls == i).nonzero()).flatten().int().numpy()\n",
    "            savez_compressed(distribution_path, res = parts)\n",
    "    \n",
    "    elif method == \"MMS\":\n",
    "        clustering_model_filename = join(model_scenario_path, 'kmeans.sav')\n",
    "\n",
    "        if overwrite == False and os.path.isfile(distribution_path) and os.path.isfile(clustering_model_filename):\n",
    "            kmeans_model = pickle.load(open(clustering_model_filename, 'rb'))\n",
    "            parts = (np.load(distribution_path, allow_pickle=True)['res']).item()\n",
    "        else:\n",
    "            centers, labels = clustering.init_centers(trainx, n_clusters) # IMPORTANT TODO: dont forget to cite the reference\n",
    "            kmeans_model = clustering.Fast_KMeans(n_clusters=n_clusters, max_iter=100, tol=0.0001, verbose=0, centroids=centers, mode=m, minibatch=None)\n",
    "            lbls = kmeans_model.fit_predict(torch.Tensor(traincenterx).cuda())\n",
    "            pickle.dump(kmeans_model, open(clustering_model_filename, 'wb'))\n",
    "            for i in range(n_clusters):\n",
    "                parts[i] = (lbls == i).nonzero().cpu().numpy().squeeze(1)\n",
    "            savez_compressed(distribution_path, res = parts)\n",
    "\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_submodels(method, parts, trainx, trainy, trainl, traincenterx, traincentery, traincenterl, testx, testy, testl, valx, valy, vall):\n",
    "    dataset_path = config[dataset_name][\"features\"]\n",
    "    for r in (range(n_clusters)):\n",
    "        utility_functions.pprint(('cluster', str(r)), dataset_name)\n",
    "        train_ids = utility_functions.train_calc_ids(parts[r], trainy, trainl, r)\n",
    "        test_ids = utility_functions.test_calc_ids(parts[r], trainy, trainl, r)\n",
    "        train_sample_count, test_sample_count = utility_functions.convert_emb_to_tfrecord(dataset_name, data_scenario_path, dataset_path, train_ids, test_ids, r, overwrite=False, all_samples=True, n_classes=len(parts[r]))\n",
    "        train_dataset, test_dataset = utility_functions.prepare_data_sets(dataset_name, data_scenario_path, train_ids, test_ids, r)    \n",
    "        training.softmax_train(dataset_name, model_scenario_path, train_dataset, parts[r], trainx, trainl, r, epochs=50,train_overwrite=False, freq = train_sample_count // 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(m, t = 'val'):\n",
    "\n",
    "    models = dict()\n",
    "    for idx in (range(n_clusters)):    \n",
    "        model_path = join(model_scenario_path, str(idx), 'exported', 'hrnetv2')\n",
    "        with tf.device('/cpu:0'):\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "            models[idx] = model\n",
    "\n",
    "    model = None\n",
    "    batch_softmax = None\n",
    "    batch_argmax_softmax = None\n",
    "    batch_max_softmax = None\n",
    "\n",
    "    softmax_prediction = dict()\n",
    "    batch_size = 1000\n",
    "\n",
    "    batch_number = len(valx) // batch_size + (1 if (len(valx) % batch_size != 0) else 0)\n",
    "\n",
    "    max_softmax = dict()\n",
    "    argmax_softmax = dict()\n",
    "\n",
    "    for idx in tqdm(range(n_clusters)):\n",
    "        if overwrite == False and os.path.isfile(join(data_scenario_path, str(idx) + '_predicted_max.npz')) and os.path.isfile(join(data_scenario_path, str(idx) + '_predicted_argmax.npz')):\n",
    "            max_softmax[idx] = np.load(join(data_scenario_path, str(idx) + '_' + t + '_predicted_max.npz'))['res']\n",
    "            argmax_softmax[idx] = np.load(join(data_scenario_path, str(idx) + '_' + t + '_predicted_argmax.npz'))['res']\n",
    "        else:\n",
    "            model_path = join(model_scenario_path, str(idx), 'exported', 'hrnetv2')\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "            max_softmax[idx] = []\n",
    "            argmax_softmax[idx] = []\n",
    "\n",
    "            for batch_counter in range(batch_number):\n",
    "                batch_softmax = model(np.array(valx[batch_counter * batch_size : np.min([len(valx), (batch_counter + 1) * batch_size])]))\n",
    "                batch_max_softmax = np.max(batch_softmax, axis=1)\n",
    "                batch_argmax_softmax = np.argmax(batch_softmax, axis=1)\n",
    "                max_softmax[idx] += list(batch_max_softmax)\n",
    "                argmax_softmax[idx] += list(batch_argmax_softmax)\n",
    "            savez_compressed(join(data_scenario_path, str(idx) + '_' + t + '_predicted_max.npz'), res=max_softmax[idx])\n",
    "            savez_compressed(join(data_scenario_path, str(idx) + '_' + t + '_predicted_argmax.npz'), res=argmax_softmax[idx])\n",
    "\n",
    "    max_softmax = np.array(list(max_softmax.values())).transpose()\n",
    "    argmax_softmax = np.array(list(argmax_softmax.values())).transpose()\n",
    "\n",
    "    # ابتدا نزدیکترین دسته به هر نمونه آزمون را پیدا میکنیم\n",
    "    # شماره دسته، شماره خوشه، شماره دسته در خوشه و مقدار شباهت کسینوسی و مقدار سافتمکس نزدیکترین دسته را در لیستهای مجزا ذخیره میکنیم\n",
    "    batch_size = 5000\n",
    "    batch_numbers = len(valx) //batch_size #+ 1\n",
    "\n",
    "    sim_clusters = []\n",
    "    sim_classes = []\n",
    "    sim_classes_in_clusters=[]\n",
    "    sim_values = []\n",
    "\n",
    "    pre_path = data_scenario_path\n",
    "\n",
    "    if overwrite == False and os.path.isfile(join(pre_path, t + '_sim_clusters.pt')):\n",
    "        sim_clusters = np.array(torch.load(join(pre_path, t + '_sim_clusters.pt')))\n",
    "        sim_classes = np.array(torch.load(join(pre_path, t + '_sim_classes.pt')))\n",
    "        sim_classes_in_clusters = np.array(torch.load(join(pre_path, t + '_sim_classes_in_clusters.pt')))\n",
    "        sim_values = np.array(torch.load(join(pre_path, t + '_sim_values.pt')))\n",
    "        sim_softmax = np.array(torch.load(join(pre_path, t + '_sim_softmax.pt')))\n",
    "    else:\n",
    "        for batch in tqdm(range(batch_numbers)):\n",
    "            if batch == batch_numbers - 1 and (len(valx) % batch_size):\n",
    "                batch_clusters = [0] * (len(valx) % batch_size)\n",
    "            else:\n",
    "                batch_clusters = [0] * batch_size\n",
    "            batch_classes_in_clusters = []\n",
    "            if m == 'euclidean':\n",
    "                batch_sim = utility_functions.euc_sim(torch.Tensor(valx[batch*batch_size:np.min([len(valx), (batch+1)*batch_size])]), torch.Tensor(traincenterx)) \n",
    "            else:\n",
    "                batch_sim = utility_functions.cos_sim(torch.Tensor(valx[batch*batch_size:np.min([len(valx), (batch+1)*batch_size])]), torch.Tensor(traincenterx))\n",
    "            \n",
    "            # just for euclidean\n",
    "            if m == 'euclidean':\n",
    "                v = batch_sim\n",
    "                v_min, v_max = v.min(), v.max() #(dim=1)[0], v.max(dim=1)[0]\n",
    "                new_min, new_max = 0, 0.9\n",
    "                v_p = ((v.transpose(0,1) - v_min)/(v_max - v_min)*(new_max - new_min) + new_min).transpose(0,1)\n",
    "                batch_sim = v_p\n",
    "\n",
    "            batch_classes = (batch_sim.max(1)[1]).numpy()\n",
    "            batch_values = (batch_sim.max(1)[0]).numpy()\n",
    "            for r in parts:\n",
    "                batch_clusters += (r * (np.in1d(batch_classes, parts[r])).astype(np.int32))\n",
    "            for idx, b in enumerate(batch_classes):\n",
    "                batch_classes_in_clusters.append(np.where(parts[batch_clusters[idx]] == batch_classes[idx].item())[0][0])\n",
    "            \n",
    "            sim_clusters.extend(list(batch_clusters))\n",
    "            sim_classes.extend(list(batch_classes))\n",
    "            sim_classes_in_clusters.extend(list(batch_classes_in_clusters))\n",
    "            sim_values.extend(list(batch_values))\n",
    "\n",
    "        torch.save(torch.Tensor(sim_clusters), join(pre_path, t + '_sim_clusters.pt'))\n",
    "        torch.save(torch.Tensor(sim_classes), join(pre_path, t + '_sim_classes.pt'))\n",
    "        torch.save(torch.Tensor(sim_classes_in_clusters), join(pre_path, t + '_sim_classes_in_clusters.pt'))\n",
    "        torch.save(torch.Tensor(sim_values), join(pre_path, t + '_sim_values.pt'))\n",
    "\n",
    "        sim_clusters = np.array(torch.load(join(pre_path, t + '_sim_clusters.pt')))\n",
    "        sim_classes = np.array(torch.load(join(pre_path, t + '_sim_classes.pt')))\n",
    "        sim_classes_in_clusters = np.array(torch.load(join(pre_path, t + '_sim_classes_in_clusters.pt')))\n",
    "        sim_values = np.array(torch.load(join(pre_path, t + '_sim_values.pt')))\n",
    "\n",
    "        # نمونه های مربوط به هر خوشه را جدا میکنیم\n",
    "        ids = dict()\n",
    "        for r in parts:\n",
    "            ids[r] = np.where(sim_clusters == r)[0]\n",
    "\n",
    "        # دار سافتمکس دسته ای که بیشترین شباهت کسینوسی به داده آزمون را دارد\n",
    "        sim_softmax = np.zeros(len(valx))\n",
    "        m = dict()\n",
    "        batch_size = 1000\n",
    "        for i in tqdm(ids):\n",
    "            m[i] = []\n",
    "            batch_numbers = len(ids[i]) // batch_size + (1 if len(ids[i]) % batch_size != 0 else 0)\n",
    "            for batch in (range(batch_numbers)):\n",
    "                pr = models[i](valx[ids[i][batch * batch_size : np.min([len(ids[i]), (batch + 1) * batch_size])]])#[0][sim_classes_in_clusters[idx].int().item()]\n",
    "                for pidx, p in enumerate(pr):\n",
    "                    m[i].append(pr[pidx][sim_classes_in_clusters[ids[i][pidx]].astype('int')])\n",
    "            sim_softmax[ids[i]] = m[i]\n",
    "\n",
    "        torch.save(torch.Tensor(sim_softmax), join(pre_path, t + '_sim_softmax.pt'))\n",
    "        sim_softmax = np.array(torch.load(join(pre_path, t + '_sim_softmax.pt')))\n",
    "\n",
    "    \n",
    "    # در مرحله دوم، ابتدا بهترین دسته هایی که بیشترین مقدار سافتمکس را دریافت کرده اند، پیدا میکنیم\n",
    "    # شماره دسته، شماره خوشه، شماره دسته در خوشه، مقدار سافتمکس و مقدار شباهت کسینوسی دسته با داده آزمون\n",
    "    if overwrite == False and os.path.isfile(join(pre_path, t + '_softmax_values.pt')):\n",
    "        softmax_clusters = torch.load(join(pre_path, t + '_softmax_clusters.pt'))\n",
    "        softmax_classes = torch.load(join(pre_path, t + '_softmax_classes.pt'))\n",
    "        softmax_classes_in_clusters = torch.load(join(pre_path, t + '_softmax_classes_in_clusters.pt'))\n",
    "        softmax_values = torch.load(join(pre_path, t + '_softmax_values.pt'))\n",
    "        softmax_sims = torch.load(join(pre_path, t + '_softmax_sims.pt'))\n",
    "    else:\n",
    "        softmax_values = max_softmax.max(1)\n",
    "        softmax_clusters = max_softmax.argmax(1)\n",
    "        softmax_classes = []\n",
    "        softmax_classes_in_clusters = []\n",
    "        for idx, cl in tqdm(enumerate(softmax_clusters)):\n",
    "            softmax_classes.append(parts[cl][argmax_softmax[idx][cl]])\n",
    "            softmax_classes_in_clusters.append(argmax_softmax[idx][cl])\n",
    "\n",
    "        torch.save(torch.Tensor(softmax_clusters), join(pre_path, t + '_softmax_clusters.pt'))\n",
    "        torch.save(torch.Tensor(softmax_classes), join(pre_path, t + '_softmax_classes.pt'))\n",
    "        torch.save(torch.Tensor(softmax_classes_in_clusters), join(pre_path, t + '_softmax_classes_in_clusters.pt'))\n",
    "        torch.save(torch.Tensor(softmax_values), join(pre_path, t + '_softmax_values.pt'))\n",
    "\n",
    "        # مقدار شباهت کسینوسی دسته ای که بیشترین مقدار سافتمکس را دارد\n",
    "        softmax_sims = []\n",
    "        for idx in tqdm(range(len(valx))):\n",
    "            if m == 'euclidean':\n",
    "                softmax_sims.append(utility_functions.euc_sim(torch.Tensor(np.array([valx[idx]])), torch.Tensor(np.array([traincenterx[softmax_classes[idx]]])))[0][0].item())\n",
    "            else:\n",
    "                softmax_sims.append(utility_functions.cos_sim(torch.Tensor(np.array([valx[idx]])), torch.Tensor(np.array([traincenterx[softmax_classes[idx]]])))[0][0].item())\n",
    "\n",
    "        # # just for euclidean\n",
    "        if m == 'euclidean':\n",
    "            v = torch.Tensor(softmax_sims)\n",
    "            v_min, v_max = v.min(), v.max()\n",
    "            new_min, new_max = 0, 0.9\n",
    "            v_p = ((v - v_min)/(v_max - v_min)*(new_max - new_min) + new_min)\n",
    "            softmax_sims = v_p   \n",
    "\n",
    "        torch.save(torch.Tensor(softmax_sims), join(pre_path, t + '_softmax_sims.pt'))\n",
    "    return sim_classes, sim_values, sim_softmax, softmax_values, softmax_sims, softmax_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_weights():\n",
    "    weights = []\n",
    "    for i in range(n_classes):\n",
    "        w = (len(np.where(testl == i)[0]))\n",
    "        for s in range(w):\n",
    "            weights.append(1/w)\n",
    "    weights = np.array(weights)\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4153217608.py, line 68)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Dianat\\AppData\\Local\\Temp\\ipykernel_3360\\4153217608.py\"\u001b[1;36m, line \u001b[1;32m68\u001b[0m\n\u001b[1;33m    utility_functions.pprint(()\"best_thr\", best_thr), dataset_name)\u001b[0m\n\u001b[1;37m                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def find_best_thr(sim_classes, sim_values, sim_softmax, softmax_values, softmax_sims, softmax_classes):\n",
    "    weights = set_weights()\n",
    "    true_prec = np.zeros(n_classes)\n",
    "    false_prec = np.zeros(n_classes)\n",
    "\n",
    "    true_recall = np.zeros(n_classes)\n",
    "    false_recall = np.zeros(n_classes)\n",
    "\n",
    "    best_thr = -1\n",
    "    best_recall = 0\n",
    "\n",
    "    main_preds = np.zeros((len(valx), 20))\n",
    "    for th in range(0, 20):\n",
    "        trues = 0\n",
    "        falses = 0\n",
    "        try:\n",
    "            res = (sim_values * (th / 10) * ((sim_softmax+1)/2)) > (softmax_values * ((np.array(softmax_sims)+1)/2))\n",
    "        except:\n",
    "            res = (sim_values * (th / 10) * ((sim_softmax+1)/2)) > (softmax_values.numpy() * ((np.array(softmax_sims.numpy())+1)/2))\n",
    "        for idx in range(len(valx)):\n",
    "            if softmax_values[idx] > 0.5:\n",
    "                main_preds[idx, th] = softmax_classes[idx]\n",
    "                if softmax_classes[idx] == vall[idx]:\n",
    "                    true_prec[vall[idx]] += weights[idx]\n",
    "                    true_recall[vall[idx]] += weights[idx]\n",
    "                    trues += weights[idx]\n",
    "                else:\n",
    "                    falses += weights[idx]\n",
    "                    try:\n",
    "                        false_prec[softmax_classes[idx].int()] += weights[idx]\n",
    "                    except:\n",
    "                        false_prec[softmax_classes[idx]] += weights[idx]\n",
    "\n",
    "                    false_recall[vall[idx]] += weights[idx]\n",
    "                continue\n",
    "            if res[idx]:\n",
    "                main_preds[idx, th] = sim_classes[idx]\n",
    "                if sim_classes[idx] == vall[idx]:\n",
    "                    trues += weights[idx]\n",
    "                    true_recall[vall[idx]] += weights[idx]\n",
    "                    true_prec[vall[idx]] += weights[idx]\n",
    "                else:\n",
    "                    falses += weights[idx]\n",
    "                    false_recall[vall[idx]] += weights[idx]\n",
    "                    try:\n",
    "                        false_prec[softmax_classes[idx].int()] += weights[idx]\n",
    "                    except:\n",
    "                        false_prec[softmax_classes[idx]] += weights[idx]\n",
    "\n",
    "            else:\n",
    "                main_preds[idx, th] = softmax_classes[idx]\n",
    "                if softmax_classes[idx] == vall[idx]:\n",
    "                    true_prec[vall[idx]] += weights[idx]\n",
    "                    true_recall[vall[idx]] += weights[idx]\n",
    "                    trues += weights[idx]\n",
    "                else:\n",
    "                    falses += weights[idx]\n",
    "                    false_recall[vall[idx]] += weights[idx]\n",
    "                    try:\n",
    "                        false_prec[softmax_classes[idx].int()] += weights[idx]\n",
    "                    except:\n",
    "                        false_prec[softmax_classes[idx]] += weights[idx]                    \n",
    "        main_recall = trues / (trues + falses)\n",
    "        print((th, main_recall))\n",
    "        if main_recall > best_recall:\n",
    "            best_thr = th\n",
    "            best_recall = main_recall\n",
    "    utility_functions.pprint(()\"best_thr\", best_thr), dataset_name)\n",
    "    return best_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_final_acc(thr, sim_classes, sim_values, sim_softmax, softmax_values, softmax_sims, softmax_classes):\n",
    "    best_thr = thr\n",
    "    main_preds = np.zeros(len(testx))\n",
    "\n",
    "    trues = 0\n",
    "    falses = 0\n",
    "    try:\n",
    "        res = (sim_values * (thr / 10) * ((sim_softmax+1)/2)) > (softmax_values * ((np.array(softmax_sims)+1)/2))\n",
    "    except:\n",
    "        res = (sim_values * (thr / 10) * ((sim_softmax+1)/2)) > (softmax_values.numpy() * ((np.array(softmax_sims.numpy())+1)/2))\n",
    "    for idx in range(len(valx)):\n",
    "        if softmax_values[idx] > 0.5:\n",
    "            main_preds[idx] = softmax_classes[idx]\n",
    "            if softmax_classes[idx] == testl[idx]:\n",
    "                trues += weights[idx]\n",
    "            else:\n",
    "                falses += weights[idx]\n",
    "            continue\n",
    "        if res[idx]:\n",
    "            main_preds[idx] = sim_classes[idx]\n",
    "            if sim_classes[idx] == testl[idx]:\n",
    "                trues += weights[idx]\n",
    "            else:\n",
    "                falses += weights[idx]\n",
    "        else:\n",
    "            main_preds[idx] = softmax_classes[idx]\n",
    "            if softmax_classes[idx] == testl[idx]:\n",
    "                trues += weights[idx]\n",
    "            else:\n",
    "                falses += weights[idx]\n",
    "    \n",
    "    main_report = (metrics.classification_report(testl, main_preds, output_dict=True, zero_division=0))\n",
    "    utility_functions.pprint((main_report['accuracy'], main_report['macro avg'], main_report['weighted avg']), dataset_name)\n",
    "    return main_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mms_acc(m):\n",
    "    val_sim_classes, val_sim_values, val_sim_softmax, val_softmax_values, val_softmax_sims, val_softmax_classes = post_process(m, 'val')\n",
    "    thr = find_best_thr(val_sim_classes, val_sim_values, val_sim_softmax, val_softmax_values, val_softmax_sims, val_softmax_classes)\n",
    "\n",
    "    test_sim_classes, test_sim_values, test_sim_softmax, test_softmax_values, test_softmax_sims, test_softmax_classes = post_process(m, 'test')\n",
    "    final_report = calc_final_acc(thr, test_sim_classes, test_sim_values, test_sim_softmax, test_softmax_values, test_softmax_sims, test_softmax_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:03,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:01<00:02,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:02<00:01,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:03<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.20it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.41it/s]\n",
      "100%|██████████| 5/5 [00:58<00:00, 11.78s/it]\n",
      "50000it [00:00, 577311.14it/s]\n",
      "100%|██████████| 50000/50000 [00:04<00:00, 10286.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99392 {'precision': 0.9943230952380953, 'recall': 0.9939199999999999, 'f1-score': 0.9935857686757686, 'support': 50000} {'precision': 0.9943230952380953, 'recall': 0.99392, 'f1-score': 0.9935857686757686, 'support': 50000}\n"
     ]
    }
   ],
   "source": [
    "# only max_max\n",
    "# if method == \"MMS\":\n",
    "def calc_ism_acc(m):\n",
    "    test_sim_classes, test_sim_values, test_sim_softmax, test_softmax_values, test_softmax_sims, test_softmax_classes = post_process(m, 'test')\n",
    "\n",
    "    max_max_trues = 0\n",
    "    max_max_falses = 0\n",
    "\n",
    "    max_max_true_prec = np.zeros(n_classes)\n",
    "    max_max_false_prec = np.zeros(n_classes)\n",
    "    max_max_true_recall = np.zeros(n_classes)\n",
    "    max_max_false_recall = np.zeros(n_classes)\n",
    "\n",
    "    index = 0\n",
    "    for test_sample in range(len(testx)):\n",
    "        real_class = testl[test_sample]\n",
    "\n",
    "        if test_softmax_classes[test_sample] == real_class:\n",
    "            max_max_trues += weights[test_sample]\n",
    "            max_max_true_prec[real_class] += weights[test_sample]\n",
    "            max_max_true_recall[real_class] += weights[test_sample]\n",
    "        else:\n",
    "            max_max_falses += weights[test_sample]\n",
    "            max_max_false_recall[real_class] += weights[test_sample]\n",
    "            try:\n",
    "                max_max_false_prec[test_softmax_classes[test_sample].int()] += weights[test_sample]\n",
    "            except:\n",
    "                max_max_false_prec[test_softmax_classes[test_sample]] += weights[test_sample]\n",
    "\n",
    "    # print(max_max_trues / (max_max_trues + max_max_falses))\n",
    "    max_max_precision_array = np.divide(max_max_true_prec, (max_max_true_prec + max_max_false_prec), out=np.zeros_like(max_max_true_prec), where=(max_max_false_prec + max_max_true_prec)!=0)\n",
    "    max_max_recall_array = np.divide(max_max_true_recall, (max_max_true_recall + max_max_false_recall), out=np.zeros_like(max_max_true_recall), where=(max_max_false_recall + max_max_true_recall)!=0)\n",
    "\n",
    "    max_max_f_score_x = 2 * max_max_precision_array * max_max_recall_array\n",
    "    max_max_f_score_y = max_max_precision_array + max_max_recall_array\n",
    "    max_max_fscore_array =  np.divide(max_max_f_score_x, max_max_f_score_y, out=np.zeros_like(max_max_f_score_x), where=(max_max_f_score_y)!=0)\n",
    "\n",
    "    max_max_precision = np.sum(max_max_precision_array) / n_classes\n",
    "    max_max_recall = np.sum(max_max_recall_array) / n_classes\n",
    "    max_max_fscore = np.sum(max_max_fscore_array) / n_classes\n",
    "\n",
    "    max_max_precision, max_max_recall, max_max_fscore\n",
    "\n",
    "    max_max_report = metrics.classification_report(testl, test_softmax_classes, output_dict=True, zero_division=0)\n",
    "    utility_functions.pprint((max_max_report['accuracy'], max_max_report['macro avg'], max_max_report['weighted avg']), dataset_name)\n",
    "    return max_max_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method = config['method'] \n",
    "# n_classes = config['n_classes']\n",
    "# n_clusters = config['n_clusters']\n",
    "dataset_name = 'ms-celeb-1m'\n",
    "# overwrite = config['overwrite']\n",
    "m = config['distance_measure']\n",
    "\n",
    "for method in [\"ISM\", \"MMS\"]:\n",
    "    for n_classes, n_clusters in zip([1000, 5000, 10000, 20000, 50000], [1,2,3,5,10,20]):\n",
    "        scenario = str(n_classes) + '_' + method + str(n_clusters)\n",
    "        data_scenario_path = join(config[dataset_name][\"scenario_embs\"], scenario)\n",
    "        model_scenario_path = join(config[dataset_name][\"scenario_submodels\"], scenario)\n",
    "        super_scenario_path = join(config[dataset_name][\"scenario_embs\"], str(n_classes))\n",
    "        \n",
    "        utility_functions.pprint((\"dataset_name = \", dataset_name), dataset_name)\n",
    "        utility_functions.pprint((\"meth = \", method), dataset_name)\n",
    "        utility_functions.pprint((\"n_classes = \", n_classes), dataset_name)\n",
    "        utility_functions.pprint((\"n_clusters = \", n_clusters), dataset_name)\n",
    "\n",
    "        os.makedirs(super_scenario_path, exist_ok=True)\n",
    "        os.makedirs(data_scenario_path, exist_ok=True)\n",
    "        os.makedirs(model_scenario_path, exist_ok=True)\n",
    "\n",
    "        trainx, trainy, trainl, traincenterx, traincentery, traincenterl, testx, testy, testl, valx, valy, vall = prepare_data(dataset_name)\n",
    "        parts = cluster_data(method, trainx, trainy, trainl, traincenterx, traincentery, traincenterl, testx, testy, testl, valx, valy, vall)\n",
    "        train_submodels(method, parts, trainx, trainy, trainl, traincenterx, traincentery, traincenterl, testx, testy, testl, valx, valy, vall):\n",
    "\n",
    "        if method == \"MMS\":\n",
    "            calc_mms_acc(m)\n",
    "        else:\n",
    "            calc_ism_acc(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9052159999999999, 0.9214439752469751, 0.9035843264884409, 0.9045679999999999, 0.9217843817936069, 0.9032642261693693)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(\"../results/results.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"select avg(ism_recall), avg(ism_precision), avg(ism_fscore), avg(mms_recall), avg(mms_precision), avg(mms_fscore) from results where n_classes=5000 and n_clusters=3\")\n",
    "rows = cursor.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
